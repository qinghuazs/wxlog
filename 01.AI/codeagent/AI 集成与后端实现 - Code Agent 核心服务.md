---
title: AI é›†æˆä¸åç«¯å®ç° - Code Agent æ ¸å¿ƒæœåŠ¡
date: 2025-01-30
permalink: /ai/codeagent/ai-integration.html
categories:
  - AI
  - CodeAgent
---

# AI é›†æˆä¸åç«¯å®ç°

## ä¸€ã€åç«¯æ¶æ„

### 1.1 FastAPI æœåŠ¡æ­å»º

```python
# backend/main.py
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional, AsyncIterator
import asyncio

from core.ai_engine import AIEngine
from core.code_analyzer import CodeAnalyzer
from core.context_manager import ContextManager

app = FastAPI(title="Code Agent API")

# CORS é…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
ai_engine = AIEngine()
code_analyzer = CodeAnalyzer()
context_manager = ContextManager()

# è¯·æ±‚æ¨¡å‹
class CompletionRequest(BaseModel):
    prefix: str
    suffix: str = ""
    language: str = "python"
    file_path: Optional[str] = None

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[ChatMessage]

class ExplainRequest(BaseModel):
    code: str
    language: str

class RefactorRequest(BaseModel):
    code: str
    instruction: str
    language: str

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy"}

@app.post("/completion")
async def get_completion(request: CompletionRequest):
    """ä»£ç è¡¥å…¨ï¼ˆåŒæ­¥ï¼‰"""
    try:
        # æ„å»ºä¸Šä¸‹æ–‡
        context = await context_manager.build_completion_context(
            prefix=request.prefix,
            suffix=request.suffix,
            language=request.language,
            file_path=request.file_path
        )

        # è°ƒç”¨ AI
        completion = await ai_engine.complete(context)

        return {"completion": completion}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/completion/stream")
async def stream_completion(request: CompletionRequest):
    """ä»£ç è¡¥å…¨ï¼ˆæµå¼ï¼‰"""
    async def generate():
        try:
            context = await context_manager.build_completion_context(
                prefix=request.prefix,
                suffix=request.suffix,
                language=request.language,
                file_path=request.file_path
            )

            async for chunk in ai_engine.stream_complete(context):
                yield f"data: {chunk}\n\n"

            yield "event: done\ndata: \n\n"

        except Exception as e:
            yield f"event: error\ndata: {str(e)}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )

@app.post("/chat")
async def chat(request: ChatRequest):
    """æ™ºèƒ½å¯¹è¯"""
    try:
        messages = [
            {"role": msg.role, "content": msg.content}
            for msg in request.messages
        ]

        response = await ai_engine.chat(messages)

        return {"message": response}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/chat/stream")
async def stream_chat(request: ChatRequest):
    """æ™ºèƒ½å¯¹è¯ï¼ˆæµå¼ï¼‰"""
    async def generate():
        try:
            messages = [
                {"role": msg.role, "content": msg.content}
                for msg in request.messages
            ]

            async for chunk in ai_engine.stream_chat(messages):
                yield f"data: {chunk}\n\n"

            yield "event: done\ndata: \n\n"

        except Exception as e:
            yield f"event: error\ndata: {str(e)}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )

@app.post("/explain")
async def explain_code(request: ExplainRequest):
    """è§£é‡Šä»£ç """
    try:
        # åˆ†æä»£ç ç»“æ„
        analysis = code_analyzer.analyze(request.code, request.language)

        # ç”Ÿæˆè§£é‡Š
        explanation = await ai_engine.explain(
            code=request.code,
            language=request.language,
            analysis=analysis
        )

        return {"explanation": explanation}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/refactor")
async def refactor_code(request: RefactorRequest):
    """é‡æ„ä»£ç """
    try:
        refactored = await ai_engine.refactor(
            code=request.code,
            instruction=request.instruction,
            language=request.language
        )

        return {"refactored": refactored}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/generate-tests")
async def generate_tests(request: ExplainRequest):
    """ç”Ÿæˆæµ‹è¯•"""
    try:
        # åˆ†æä»£ç 
        analysis = code_analyzer.analyze(request.code, request.language)

        # ç”Ÿæˆæµ‹è¯•
        tests = await ai_engine.generate_tests(
            code=request.code,
            language=request.language,
            analysis=analysis
        )

        return {"tests": tests}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

## äºŒã€AI å¼•æ“å®ç°

### 2.1 ç»Ÿä¸€çš„ AI æ¥å£

```python
# backend/core/ai_engine.py
from abc import ABC, abstractmethod
from typing import AsyncIterator, List, Dict
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain.prompts import ChatPromptTemplate
from langchain.schema import HumanMessage, SystemMessage

class AIProvider(ABC):
    """AI æä¾›è€…æŠ½è±¡åŸºç±»"""

    @abstractmethod
    async def complete(self, prompt: str, **kwargs) -> str:
        pass

    @abstractmethod
    async def stream_complete(self, prompt: str, **kwargs) -> AsyncIterator[str]:
        pass

class OpenAIProvider(AIProvider):
    """OpenAI æä¾›è€…"""

    def __init__(self, api_key: str, model: str = "gpt-4"):
        self.llm = ChatOpenAI(
            api_key=api_key,
            model=model,
            temperature=0.2
        )

    async def complete(self, prompt: str, **kwargs) -> str:
        response = await self.llm.ainvoke(prompt)
        return response.content

    async def stream_complete(self, prompt: str, **kwargs) -> AsyncIterator[str]:
        async for chunk in self.llm.astream(prompt):
            if chunk.content:
                yield chunk.content

class ClaudeProvider(AIProvider):
    """Claude æä¾›è€…"""

    def __init__(self, api_key: str, model: str = "claude-3-sonnet-20240229"):
        self.llm = ChatAnthropic(
            api_key=api_key,
            model=model,
            temperature=0.2
        )

    async def complete(self, prompt: str, **kwargs) -> str:
        response = await self.llm.ainvoke(prompt)
        return response.content

    async def stream_complete(self, prompt: str, **kwargs) -> AsyncIterator[str]:
        async for chunk in self.llm.astream(prompt):
            if chunk.content:
                yield chunk.content

class AIEngine:
    """AI å¼•æ“ - ç»Ÿä¸€ç®¡ç†æ‰€æœ‰ AI åŠŸèƒ½"""

    def __init__(self):
        # åˆå§‹åŒ–æä¾›è€…
        self.providers = {
            "openai": OpenAIProvider(api_key="your-openai-key"),
            "claude": ClaudeProvider(api_key="your-claude-key"),
        }
        self.default_provider = "openai"

        # Prompt æ¨¡æ¿
        self.prompts = self._init_prompts()

    def _init_prompts(self) -> Dict[str, ChatPromptTemplate]:
        """åˆå§‹åŒ– Prompt æ¨¡æ¿"""
        return {
            "completion": ChatPromptTemplate.from_messages([
                SystemMessage(content="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç è¡¥å…¨åŠ©æ‰‹ã€‚
è§„åˆ™ï¼š
1. åªè¿”å›è¡¥å…¨çš„ä»£ç ï¼Œä¸è¦è§£é‡Š
2. ä¿æŒä¸ç°æœ‰ä»£ç é£æ ¼ä¸€è‡´
3. è¡¥å…¨åº”è¯¥åˆç†ä¸”ç¬¦åˆä¸Šä¸‹æ–‡
4. å¦‚æœä¸ç¡®å®šï¼Œè¿”å›æœ€å¯èƒ½çš„é€‰é¡¹
"""),
                HumanMessage(content="""
ä¸Šä¸‹æ–‡ï¼š
{context}

å½“å‰ä»£ç ï¼š
```{language}
{prefix}â–ˆ{suffix}
```

è¯·è¡¥å…¨å…‰æ ‡ä½ç½®ï¼ˆâ–ˆï¼‰çš„ä»£ç ï¼š
""")
            ]),

            "explain": ChatPromptTemplate.from_messages([
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä»£ç è§£é‡Šä¸“å®¶ï¼Œç”¨æ¸…æ™°æ˜“æ‡‚çš„è¯­è¨€è§£é‡Šä»£ç ã€‚"),
                HumanMessage(content="""
è¯·è¯¦ç»†è§£é‡Šä»¥ä¸‹ {language} ä»£ç ï¼š

```{language}
{code}
```

åŒ…æ‹¬ï¼š
1. ä»£ç çš„æ•´ä½“åŠŸèƒ½
2. å…³é”®é€»è¾‘çš„è§£é‡Š
3. æ½œåœ¨çš„é—®é¢˜æˆ–æ”¹è¿›å»ºè®®
""")
            ]),

            "refactor": ChatPromptTemplate.from_messages([
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä»£ç é‡æ„ä¸“å®¶ã€‚"),
                HumanMessage(content="""
åŸå§‹ä»£ç ï¼š
```{language}
{code}
```

é‡æ„è¦æ±‚ï¼š{instruction}

è¯·æä¾›é‡æ„åçš„ä»£ç ï¼ˆåªè¿”å›ä»£ç ï¼Œä¸è¦è§£é‡Šï¼‰ï¼š
""")
            ]),

            "generate_tests": ChatPromptTemplate.from_messages([
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæµ‹è¯•ç”¨ä¾‹ç”Ÿæˆä¸“å®¶ã€‚"),
                HumanMessage(content="""
ä¸ºä»¥ä¸‹ {language} ä»£ç ç”Ÿæˆå®Œæ•´çš„å•å…ƒæµ‹è¯•ï¼š

```{language}
{code}
```

è¦æ±‚ï¼š
1. è¦†ç›–æ‰€æœ‰ä¸»è¦åŠŸèƒ½
2. åŒ…å«æ­£å¸¸æƒ…å†µå’Œè¾¹ç•Œæƒ…å†µ
3. ä½¿ç”¨é€‚åˆ {language} çš„æµ‹è¯•æ¡†æ¶
4. ä»£ç è¦å®Œæ•´å¯è¿è¡Œ
""")
            ])
        }

    async def complete(self, context: Dict) -> str:
        """ä»£ç è¡¥å…¨"""
        provider = self.providers[self.default_provider]

        # æ„å»º prompt
        prompt = self.prompts["completion"].format_messages(
            context=context.get("context", ""),
            language=context.get("language", "python"),
            prefix=context.get("prefix", ""),
            suffix=context.get("suffix", "")
        )

        return await provider.complete(str(prompt))

    async def stream_complete(self, context: Dict) -> AsyncIterator[str]:
        """æµå¼ä»£ç è¡¥å…¨"""
        provider = self.providers[self.default_provider]

        prompt = self.prompts["completion"].format_messages(
            context=context.get("context", ""),
            language=context.get("language", "python"),
            prefix=context.get("prefix", ""),
            suffix=context.get("suffix", "")
        )

        async for chunk in provider.stream_complete(str(prompt)):
            yield chunk

    async def chat(self, messages: List[Dict]) -> str:
        """æ™ºèƒ½å¯¹è¯"""
        provider = self.providers[self.default_provider]

        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        prompt = "\n".join([
            f"{msg['role']}: {msg['content']}"
            for msg in messages
        ])

        return await provider.complete(prompt)

    async def stream_chat(self, messages: List[Dict]) -> AsyncIterator[str]:
        """æµå¼æ™ºèƒ½å¯¹è¯"""
        provider = self.providers[self.default_provider]

        prompt = "\n".join([
            f"{msg['role']}: {msg['content']}"
            for msg in messages
        ])

        async for chunk in provider.stream_complete(prompt):
            yield chunk

    async def explain(self, code: str, language: str, analysis: Dict = None) -> str:
        """è§£é‡Šä»£ç """
        provider = self.providers[self.default_provider]

        prompt = self.prompts["explain"].format_messages(
            language=language,
            code=code
        )

        return await provider.complete(str(prompt))

    async def refactor(self, code: str, instruction: str, language: str) -> str:
        """é‡æ„ä»£ç """
        provider = self.providers[self.default_provider]

        prompt = self.prompts["refactor"].format_messages(
            language=language,
            code=code,
            instruction=instruction
        )

        return await provider.complete(str(prompt))

    async def generate_tests(self, code: str, language: str, analysis: Dict = None) -> str:
        """ç”Ÿæˆæµ‹è¯•"""
        provider = self.providers[self.default_provider]

        prompt = self.prompts["generate_tests"].format_messages(
            language=language,
            code=code
        )

        return await provider.complete(str(prompt))
```

## ä¸‰ã€Prompt å·¥ç¨‹ä¼˜åŒ–

### 3.1 Few-Shot ç¤ºä¾‹

```python
# backend/core/prompt_templates.py
class PromptTemplates:
    """Prompt æ¨¡æ¿åº“"""

    COMPLETION_WITH_EXAMPLES = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ {language} ä»£ç è¡¥å…¨åŠ©æ‰‹ã€‚

ç¤ºä¾‹1:
è¾“å…¥: def calculate_
è¾“å‡º: sum(a: int, b: int) -> int:
    return a + b

ç¤ºä¾‹2:
è¾“å…¥: class User
è¾“å‡º: :
    def __init__(self, name: str, email: str):
        self.name = name
        self.email = email

ç¤ºä¾‹3:
è¾“å…¥: # å¿«é€Ÿæ’åºç®—æ³•
è¾“å‡º: def quick_sort(arr):
    if len(arr) <= 1:
        return arr
    pivot = arr[len(arr) // 2]
    left = [x for x in arr if x < pivot]
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x > pivot]
    return quick_sort(left) + middle + quick_sort(right)

ç°åœ¨ï¼Œè¯·è¡¥å…¨ä»¥ä¸‹ä»£ç ï¼š

ä¸Šä¸‹æ–‡:
{context}

ä»£ç :
```{language}
{prefix}â–ˆ
```

åªè¿”å›è¡¥å…¨éƒ¨åˆ†ï¼š
"""

    CODE_REVIEW = """
ä½œä¸ºèµ„æ·±ä»£ç å®¡æŸ¥ä¸“å®¶ï¼Œè¯·å®¡æŸ¥ä»¥ä¸‹ä»£ç ï¼š

```{language}
{code}
```

è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„ä¼°ï¼š

1. ä»£ç è´¨é‡ï¼ˆå¯è¯»æ€§ã€å¯ç»´æŠ¤æ€§ï¼‰
2. æ€§èƒ½é—®é¢˜
3. å®‰å…¨éšæ‚£
4. æœ€ä½³å®è·µ
5. æ”¹è¿›å»ºè®®

ä»¥ Markdown æ ¼å¼è¾“å‡ºè¯¦ç»†æŠ¥å‘Šã€‚
"""

    BUG_FIX = """
ä»¥ä¸‹ä»£ç å­˜åœ¨ Bugï¼š

```{language}
{code}
```

é”™è¯¯ä¿¡æ¯ï¼š
{error}

è¯·ï¼š
1. åˆ†æ Bug åŸå› 
2. æä¾›ä¿®å¤æ–¹æ¡ˆ
3. ç»™å‡ºä¿®å¤åçš„å®Œæ•´ä»£ç 

æ ¼å¼ï¼š
## Bug åˆ†æ
...

## ä¿®å¤æ–¹æ¡ˆ
...

## ä¿®å¤åä»£ç 
```{language}
...
```
"""
```

### 3.2 åŠ¨æ€ Prompt æ„å»º

```python
class DynamicPromptBuilder:
    """åŠ¨æ€ Prompt æ„å»ºå™¨"""

    def __init__(self, code_analyzer):
        self.analyzer = code_analyzer

    async def build_completion_prompt(
        self,
        prefix: str,
        suffix: str,
        language: str,
        context: Dict
    ) -> str:
        """æ„å»ºè¡¥å…¨ Prompt"""

        # 1. åˆ†æä»£ç æ„å›¾
        intent = self._analyze_intent(prefix)

        # 2. é€‰æ‹©åˆé€‚çš„æ¨¡æ¿
        if intent == "function_definition":
            template = self._get_function_template()
        elif intent == "class_definition":
            template = self._get_class_template()
        elif intent == "import_statement":
            template = self._get_import_template()
        else:
            template = self._get_default_template()

        # 3. å¡«å……ä¸Šä¸‹æ–‡
        prompt = template.format(
            language=language,
            prefix=prefix,
            suffix=suffix,
            context=context.get("surrounding_code", ""),
            imports=context.get("imports", ""),
            similar_code=context.get("similar_code", "")
        )

        return prompt

    def _analyze_intent(self, prefix: str) -> str:
        """åˆ†æç”¨æˆ·æ„å›¾"""
        if prefix.strip().startswith("def "):
            return "function_definition"
        elif prefix.strip().startswith("class "):
            return "class_definition"
        elif prefix.strip().startswith(("import ", "from ")):
            return "import_statement"
        else:
            return "general"

    def _get_function_template(self) -> str:
        return """
åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡è¡¥å…¨å‡½æ•°ï¼š

å·²æœ‰å¯¼å…¥ï¼š
{imports}

ç›¸ä¼¼ä»£ç ç¤ºä¾‹ï¼š
{similar_code}

å½“å‰ä»£ç ï¼š
```{language}
{prefix}
```

è¯·è¡¥å…¨å‡½æ•°ä½“ï¼š
"""
```

## å››ã€å‘é‡æ£€ç´¢ç³»ç»Ÿ

### 4.1 ä»£ç ç´¢å¼•

```python
# backend/core/code_indexer.py
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import Language, RecursiveCharacterTextSplitter
import os
from pathlib import Path

class CodeIndexer:
    """ä»£ç ç´¢å¼•å™¨"""

    def __init__(self, persist_directory: str = "./chroma_db"):
        self.embeddings = OpenAIEmbeddings()
        self.vector_store = Chroma(
            persist_directory=persist_directory,
            embedding_function=self.embeddings
        )

    async def index_project(self, project_path: str):
        """ç´¢å¼•æ•´ä¸ªé¡¹ç›®"""
        # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        extensions = {
            ".py": Language.PYTHON,
            ".js": Language.JS,
            ".ts": Language.TS,
            ".java": Language.JAVA,
            ".go": Language.GO,
            ".cpp": Language.CPP,
        }

        documents = []

        for ext, lang in extensions.items():
            # æŸ¥æ‰¾æ‰€æœ‰ç›¸å…³æ–‡ä»¶
            files = Path(project_path).rglob(f"*{ext}")

            for file_path in files:
                # è·³è¿‡æŸäº›ç›®å½•
                if any(skip in str(file_path) for skip in [
                    "node_modules", "venv", ".git", "dist", "build"
                ]):
                    continue

                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        content = f.read()

                    # ä½¿ç”¨è¯­è¨€ç‰¹å®šçš„åˆ†å‰²å™¨
                    splitter = RecursiveCharacterTextSplitter.from_language(
                        language=lang,
                        chunk_size=1000,
                        chunk_overlap=200
                    )

                    chunks = splitter.split_text(content)

                    for i, chunk in enumerate(chunks):
                        documents.append({
                            "content": chunk,
                            "metadata": {
                                "file": str(file_path),
                                "language": ext[1:],
                                "chunk_index": i
                            }
                        })

                except Exception as e:
                    print(f"Error indexing {file_path}: {e}")

        # æ‰¹é‡æ·»åŠ åˆ°å‘é‡åº“
        if documents:
            self.vector_store.add_texts(
                texts=[doc["content"] for doc in documents],
                metadatas=[doc["metadata"] for doc in documents]
            )

        print(f"Indexed {len(documents)} code chunks")

    async def search(self, query: str, k: int = 5) -> List[Dict]:
        """æœç´¢ç›¸å…³ä»£ç """
        results = self.vector_store.similarity_search_with_score(query, k=k)

        return [
            {
                "content": doc.page_content,
                "metadata": doc.metadata,
                "score": score
            }
            for doc, score in results
        ]
```

## äº”ã€å®Œæ•´å·¥ä½œæµç¤ºä¾‹

```python
# backend/core/completion_workflow.py
from langgraph.graph import StateGraph, END
from typing import TypedDict

class CompletionState(TypedDict):
    prefix: str
    suffix: str
    language: str
    context: Dict
    similar_code: List[str]
    prompt: str
    completion: str

def create_completion_workflow():
    """åˆ›å»ºè¡¥å…¨å·¥ä½œæµ"""

    # å®šä¹‰å„ä¸ªæ­¥éª¤
    def gather_context(state: CompletionState) -> Dict:
        """æ”¶é›†ä¸Šä¸‹æ–‡"""
        # å®ç°ä¸Šä¸‹æ–‡æ”¶é›†é€»è¾‘
        return {"context": {...}}

    def retrieve_similar(state: CompletionState) -> Dict:
        """æ£€ç´¢ç›¸ä¼¼ä»£ç """
        indexer = CodeIndexer()
        results = await indexer.search(state["prefix"])
        return {"similar_code": [r["content"] for r in results]}

    def build_prompt(state: CompletionState) -> Dict:
        """æ„å»º Prompt"""
        builder = DynamicPromptBuilder()
        prompt = await builder.build_completion_prompt(
            prefix=state["prefix"],
            suffix=state["suffix"],
            language=state["language"],
            context=state["context"]
        )
        return {"prompt": prompt}

    def call_ai(state: CompletionState) -> Dict:
        """è°ƒç”¨ AI"""
        engine = AIEngine()
        completion = await engine.complete({"prompt": state["prompt"]})
        return {"completion": completion}

    # æ„å»ºå›¾
    graph = StateGraph(CompletionState)

    graph.add_node("gather_context", gather_context)
    graph.add_node("retrieve_similar", retrieve_similar)
    graph.add_node("build_prompt", build_prompt)
    graph.add_node("call_ai", call_ai)

    graph.set_entry_point("gather_context")
    graph.add_edge("gather_context", "retrieve_similar")
    graph.add_edge("retrieve_similar", "build_prompt")
    graph.add_edge("build_prompt", "call_ai")
    graph.add_edge("call_ai", END)

    return graph.compile()

# ä½¿ç”¨
workflow = create_completion_workflow()
result = workflow.invoke({
    "prefix": "def quick_sort",
    "suffix": "",
    "language": "python"
})
print(result["completion"])
```

---

**æ­å–œï¼** ä½ å·²ç»æŒæ¡äº†å¼€å‘ Code Agent çš„æ ¸å¿ƒæŠ€æœ¯ï¼

ç°åœ¨ä½ å¯ä»¥ï¼š
1. ğŸ”§ å¯åŠ¨åç«¯æœåŠ¡ï¼š`python backend/main.py`
2. ğŸ”Œ å¼€å‘ VSCode æ’ä»¶
3. ğŸ¤– é›†æˆå¤šç§ AI æ¨¡å‹
4. ğŸš€ ä¼˜åŒ–æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒ

ç»§ç»­æ¢ç´¢æ›´å¤šé«˜çº§åŠŸèƒ½ï¼Œæ‰“é€ å±äºä½ è‡ªå·±çš„ AI ç¼–ç¨‹åŠ©æ‰‹ï¼
