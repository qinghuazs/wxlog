---
title: Code Agent å¼€å‘æŒ‡å— - æ„å»ºç±»ä¼¼ Cursor çš„ AI ç¼–ç¨‹åŠ©æ‰‹
date: 2025-01-30
permalink: /ai/codeagent/overview.html
categories:
  - AI
  - CodeAgent
---

# Code Agent å¼€å‘æŒ‡å—

## ä¸€ã€é¡¹ç›®æ¦‚è¿°

### 1.1 ä»€ä¹ˆæ˜¯ Code Agent

Code Agent æ˜¯ä¸€ä¸ª AI é©±åŠ¨çš„æ™ºèƒ½ç¼–ç¨‹åŠ©æ‰‹ï¼Œç±»ä¼¼äº Cursorã€GitHub Copilot ç­‰å·¥å…·ï¼Œèƒ½å¤Ÿï¼š

- ğŸ’¬ **æ™ºèƒ½å¯¹è¯**: ç†è§£è‡ªç„¶è¯­è¨€éœ€æ±‚ï¼Œç»™å‡ºä»£ç å»ºè®®
- âœï¸ **ä»£ç è¡¥å…¨**: å®æ—¶é¢„æµ‹å¹¶è¡¥å…¨ä»£ç 
- ğŸ”„ **ä»£ç é‡æ„**: è‡ªåŠ¨ä¼˜åŒ–å’Œé‡æ„ä»£ç 
- ğŸ› **Bug ä¿®å¤**: è¯†åˆ«å¹¶ä¿®å¤ä»£ç é—®é¢˜
- ğŸ“ **ä»£ç è§£é‡Š**: è§£é‡Šå¤æ‚ä»£ç çš„åŠŸèƒ½
- ğŸ§ª **æµ‹è¯•ç”Ÿæˆ**: è‡ªåŠ¨ç”Ÿæˆå•å…ƒæµ‹è¯•
- ğŸ“Š **ä»£ç å®¡æŸ¥**: æä¾›ä»£ç è´¨é‡å»ºè®®

### 1.2 ç³»ç»Ÿæ¶æ„æ€»è§ˆ

```mermaid
graph TB
    A[IDE æ’ä»¶] --> B[ä»£ç åˆ†æå¼•æ“]
    A --> C[AI æœåŠ¡å±‚]
    A --> D[ä¸Šä¸‹æ–‡ç®¡ç†]

    B --> B1[è¯­æ³•åˆ†æ]
    B --> B2[è¯­ä¹‰åˆ†æ]
    B --> B3[ä¾èµ–åˆ†æ]

    C --> C1[LLM é›†æˆ]
    C --> C2[æç¤ºå·¥ç¨‹]
    C --> C3[ç»“æœåå¤„ç†]

    D --> D1[æ–‡ä»¶ç´¢å¼•]
    D --> D2[ä»£ç åµŒå…¥]
    D --> D3[å‘é‡æœç´¢]

    E[ç”¨æˆ·äº¤äº’] --> A
    F[ä»£ç åº“] --> B
    G[AI æ¨¡å‹] --> C

    style A fill:#e1f5fe
    style B fill:#b3e5fc
    style C fill:#81d4fa
    style D fill:#4fc3f7
```

### 1.3 æ ¸å¿ƒæŠ€æœ¯æ ˆ

#### å‰ç«¯ (IDE æ’ä»¶)
- **VSCode Extension API**: æ’ä»¶å¼€å‘æ¡†æ¶
- **TypeScript**: ä¸»è¦å¼€å‘è¯­è¨€
- **React**: UI ç•Œé¢ï¼ˆä¾§è¾¹æ é¢æ¿ï¼‰
- **Monaco Editor**: ä»£ç ç¼–è¾‘å™¨é›†æˆ

#### åç«¯æœåŠ¡
- **FastAPI**: Python Web æ¡†æ¶
- **LangChain/LangGraph**: AI å·¥ä½œæµç¼–æ’
- **LlamaIndex**: ä»£ç ç´¢å¼•å’Œæ£€ç´¢
- **ChromaDB/Qdrant**: å‘é‡æ•°æ®åº“

#### AI æ¨¡å‹
- **OpenAI GPT-4**: ä¸»åŠ›æ¨¡å‹
- **Claude**: å¤‡é€‰æ¨¡å‹
- **Codex/CodeLlama**: ä»£ç ä¸“ç”¨æ¨¡å‹
- **æœ¬åœ°æ¨¡å‹**: Ollama æ”¯æŒç¦»çº¿ä½¿ç”¨

#### ä»£ç åˆ†æ
- **Tree-sitter**: è¯­æ³•è§£æ
- **Language Server Protocol (LSP)**: ä»£ç æ™ºèƒ½
- **AST**: æŠ½è±¡è¯­æ³•æ ‘åˆ†æ

## äºŒã€åŠŸèƒ½æ¨¡å—åˆ’åˆ†

### 2.1 æ ¸å¿ƒåŠŸèƒ½

```mermaid
graph LR
    A[Code Agent] --> B[ä»£ç è¡¥å…¨]
    A --> C[æ™ºèƒ½å¯¹è¯]
    A --> D[ä»£ç ç”Ÿæˆ]
    A --> E[ä»£ç å®¡æŸ¥]
    A --> F[é‡æ„ä¼˜åŒ–]

    B --> B1[è¡Œå†…è¡¥å…¨]
    B --> B2[å¤šè¡Œè¡¥å…¨]
    B --> B3[å‡½æ•°è¡¥å…¨]

    C --> C1[éœ€æ±‚ç†è§£]
    C --> C2[ä»£ç è§£é‡Š]
    C --> C3[é—®é¢˜è§£ç­”]

    D --> D1[ä»æ³¨é‡Šç”Ÿæˆ]
    D --> D2[ä»éœ€æ±‚ç”Ÿæˆ]
    D --> D3[æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ]

    E --> E1[ä»£ç è´¨é‡]
    E --> E2[å®‰å…¨æ£€æŸ¥]
    E --> E3[æ€§èƒ½åˆ†æ]

    F --> F1[ä»£ç ä¼˜åŒ–]
    F --> F2[é‡æ„å»ºè®®]
    F --> F3[æœ€ä½³å®è·µ]

    style A fill:#e1f5fe
```

### 2.2 è¾…åŠ©åŠŸèƒ½

- **ä»£ç æœç´¢**: è¯­ä¹‰åŒ–ä»£ç æœç´¢
- **æ–‡æ¡£ç”Ÿæˆ**: è‡ªåŠ¨ç”Ÿæˆä»£ç æ–‡æ¡£
- **é”™è¯¯è¯Šæ–­**: æ™ºèƒ½é”™è¯¯æç¤ºå’Œä¿®å¤å»ºè®®
- **Git é›†æˆ**: Commit æ¶ˆæ¯ç”Ÿæˆã€ä»£ç å·®å¼‚åˆ†æ
- **ç»ˆç«¯é›†æˆ**: å‘½ä»¤å»ºè®®å’Œé”™è¯¯è§£é‡Š

## ä¸‰ã€å¼€å‘è·¯çº¿å›¾

### ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€è®¾æ–½ï¼ˆ1-2ä¸ªæœˆï¼‰

```mermaid
gantt
    title Code Agent å¼€å‘è®¡åˆ’
    dateFormat  YYYY-MM-DD
    section åŸºç¡€è®¾æ–½
    VSCodeæ’ä»¶æ¡†æ¶       :a1, 2025-02-01, 14d
    åç«¯APIæœåŠ¡         :a2, 2025-02-08, 14d
    ä»£ç åˆ†æå¼•æ“        :a3, 2025-02-15, 21d
    å‘é‡æ•°æ®åº“é›†æˆ      :a4, 2025-02-22, 14d

    section æ ¸å¿ƒåŠŸèƒ½
    ä»£ç è¡¥å…¨           :b1, 2025-03-08, 21d
    æ™ºèƒ½å¯¹è¯           :b2, 2025-03-15, 21d
    ä»£ç ç”Ÿæˆ           :b3, 2025-03-29, 21d

    section ä¼˜åŒ–è¿­ä»£
    æ€§èƒ½ä¼˜åŒ–           :c1, 2025-04-15, 14d
    ç”¨æˆ·ä½“éªŒä¼˜åŒ–        :c2, 2025-04-22, 14d
    æµ‹è¯•ä¸å‘å¸ƒ         :c3, 2025-05-01, 14d
```

**æ ¸å¿ƒä»»åŠ¡ï¼š**
1. âœ… æ­å»º VSCode æ’ä»¶åŸºç¡€æ¡†æ¶
2. âœ… å®ç°åŸºç¡€çš„ AI æœåŠ¡è°ƒç”¨
3. âœ… å»ºç«‹ä»£ç åˆ†æèƒ½åŠ›
4. âœ… å®Œæˆå‘é‡æ•°æ®åº“é›†æˆ

### ç¬¬äºŒé˜¶æ®µï¼šæ ¸å¿ƒåŠŸèƒ½ï¼ˆ2-3ä¸ªæœˆï¼‰

**æ ¸å¿ƒä»»åŠ¡ï¼š**
1. âœ… å®ç°ä»£ç è¡¥å…¨åŠŸèƒ½
2. âœ… å¼€å‘æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ
3. âœ… æ„å»ºä»£ç ç”Ÿæˆèƒ½åŠ›
4. âœ… é›†æˆå¤šç§ AI æ¨¡å‹

### ç¬¬ä¸‰é˜¶æ®µï¼šä¼˜åŒ–è¿­ä»£ï¼ˆ1-2ä¸ªæœˆï¼‰

**æ ¸å¿ƒä»»åŠ¡ï¼š**
1. âœ… æ€§èƒ½ä¼˜åŒ–ï¼ˆå“åº”é€Ÿåº¦ã€èµ„æºå ç”¨ï¼‰
2. âœ… ç”¨æˆ·ä½“éªŒæ”¹è¿›
3. âœ… å®‰å…¨æ€§åŠ å›º
4. âœ… æµ‹è¯•å’Œ Bug ä¿®å¤

## å››ã€æŠ€æœ¯éš¾ç‚¹ä¸è§£å†³æ–¹æ¡ˆ

### 4.1 ä¸Šä¸‹æ–‡ç†è§£

**éš¾ç‚¹**: å¦‚ä½•è®© AI ç†è§£å®Œæ•´çš„ä»£ç ä¸Šä¸‹æ–‡

```mermaid
graph TD
    A[ç”¨æˆ·ç¼–è¾‘ä½ç½®] --> B[æ”¶é›†ä¸Šä¸‹æ–‡]
    B --> C[å½“å‰æ–‡ä»¶]
    B --> D[ç›¸å…³æ–‡ä»¶]
    B --> E[é¡¹ç›®ä¾èµ–]
    B --> F[Gitå†å²]

    C --> G[æ™ºèƒ½é€‰æ‹©]
    D --> G
    E --> G
    F --> G

    G --> H[æ„å»ºPrompt]
    H --> I[LLMæ¨ç†]
    I --> J[ä»£ç å»ºè®®]

    style A fill:#ffcdd2
    style G fill:#c5e1a5
    style I fill:#81d4fa
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
class ContextBuilder:
    """ä¸Šä¸‹æ–‡æ„å»ºå™¨"""

    def __init__(self):
        self.embeddings = OpenAIEmbeddings()
        self.vector_store = ChromaDB()
        self.ast_parser = TreeSitterParser()

    def build_context(self, cursor_position: Position) -> Context:
        """æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡"""
        context = {
            "current_file": self._get_current_file_context(cursor_position),
            "related_files": self._get_related_files(cursor_position),
            "dependencies": self._get_dependencies(),
            "recent_changes": self._get_git_history(),
        }

        # æ™ºèƒ½å‹ç¼©ä¸Šä¸‹æ–‡ï¼ˆæ§åˆ¶ token æ•°é‡ï¼‰
        compressed = self._compress_context(context)

        return compressed

    def _get_current_file_context(self, position: Position) -> dict:
        """è·å–å½“å‰æ–‡ä»¶ä¸Šä¸‹æ–‡"""
        # 1. å½“å‰å‡½æ•°/ç±»
        current_scope = self.ast_parser.get_enclosing_scope(position)

        # 2. å¯¼å…¥è¯­å¥
        imports = self.ast_parser.get_imports()

        # 3. å…‰æ ‡å‰åçš„ä»£ç 
        surrounding_code = self._get_surrounding_code(position, lines=50)

        return {
            "scope": current_scope,
            "imports": imports,
            "code": surrounding_code
        }

    def _get_related_files(self, position: Position) -> list:
        """è·å–ç›¸å…³æ–‡ä»¶ï¼ˆé€šè¿‡å‘é‡ç›¸ä¼¼åº¦ï¼‰"""
        # å½“å‰ä»£ç ç‰‡æ®µ
        current_code = self._get_current_function(position)

        # å‘é‡æœç´¢ç›¸å…³ä»£ç 
        similar_chunks = self.vector_store.similarity_search(
            current_code,
            k=5
        )

        return similar_chunks
```

### 4.2 ä»£ç è¡¥å…¨æ€§èƒ½

**éš¾ç‚¹**: å®æ—¶è¡¥å…¨è¦æ±‚æä½å»¶è¿Ÿï¼ˆ<100msï¼‰

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. **æœ¬åœ°ç¼“å­˜**: ç¼“å­˜å¸¸ç”¨è¡¥å…¨ç»“æœ
2. **æµå¼è¾“å‡º**: é€å­—è¾“å‡ºï¼Œæå‡æ„ŸçŸ¥é€Ÿåº¦
3. **é¢„æµ‹å¼è¯·æ±‚**: æå‰å‘é€è¯·æ±‚
4. **æœ¬åœ°æ¨¡å‹**: ä½¿ç”¨å°å‹æœ¬åœ°æ¨¡å‹åšåˆæ­¥è¡¥å…¨

```typescript
// VSCode æ’ä»¶ç«¯
class CompletionProvider implements vscode.InlineCompletionItemProvider {
    private cache = new LRUCache<string, string>(100);
    private debouncer = new Debouncer(150); // 150ms é˜²æŠ–

    async provideInlineCompletionItems(
        document: vscode.TextDocument,
        position: vscode.Position,
        context: vscode.InlineCompletionContext
    ): Promise<vscode.InlineCompletionItem[]> {
        const cacheKey = this.getCacheKey(document, position);

        // æ£€æŸ¥ç¼“å­˜
        const cached = this.cache.get(cacheKey);
        if (cached) {
            return [new vscode.InlineCompletionItem(cached)];
        }

        // é˜²æŠ–
        return this.debouncer.run(async () => {
            // è°ƒç”¨ API
            const completion = await this.fetchCompletion(document, position);

            // ç¼“å­˜ç»“æœ
            this.cache.set(cacheKey, completion);

            return [new vscode.InlineCompletionItem(completion)];
        });
    }

    private async fetchCompletion(
        document: vscode.TextDocument,
        position: vscode.Position
    ): Promise<string> {
        const context = await this.buildContext(document, position);

        // æµå¼è¯·æ±‚
        const stream = await fetch('/api/completion/stream', {
            method: 'POST',
            body: JSON.stringify(context),
        });

        let completion = '';
        const reader = stream.body?.getReader();

        while (true) {
            const { done, value } = await reader!.read();
            if (done) break;

            const chunk = new TextDecoder().decode(value);
            completion += chunk;

            // å¢é‡æ›´æ–° UI
            this.updateInlineCompletion(completion);
        }

        return completion;
    }
}
```

### 4.3 å¤šæ¨¡å‹æ”¯æŒ

**éš¾ç‚¹**: æ”¯æŒå¤šç§ AI æ¨¡å‹ï¼Œç»Ÿä¸€æ¥å£

```python
from abc import ABC, abstractmethod
from typing import AsyncIterator

class LLMProvider(ABC):
    """LLM æä¾›è€…æŠ½è±¡åŸºç±»"""

    @abstractmethod
    async def complete(self, prompt: str, **kwargs) -> str:
        """åŒæ­¥è¡¥å…¨"""
        pass

    @abstractmethod
    async def stream_complete(self, prompt: str, **kwargs) -> AsyncIterator[str]:
        """æµå¼è¡¥å…¨"""
        pass

class OpenAIProvider(LLMProvider):
    """OpenAI å®ç°"""

    def __init__(self, api_key: str, model: str = "gpt-4"):
        self.client = AsyncOpenAI(api_key=api_key)
        self.model = model

    async def complete(self, prompt: str, **kwargs) -> str:
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            **kwargs
        )
        return response.choices[0].message.content

    async def stream_complete(self, prompt: str, **kwargs) -> AsyncIterator[str]:
        stream = await self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            stream=True,
            **kwargs
        )

        async for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

class OllamaProvider(LLMProvider):
    """Ollama æœ¬åœ°æ¨¡å‹å®ç°"""

    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.client = httpx.AsyncClient()

    async def complete(self, prompt: str, **kwargs) -> str:
        response = await self.client.post(
            f"{self.base_url}/api/generate",
            json={
                "model": kwargs.get("model", "codellama"),
                "prompt": prompt,
                "stream": False
            }
        )
        return response.json()["response"]

    async def stream_complete(self, prompt: str, **kwargs) -> AsyncIterator[str]:
        async with self.client.stream(
            "POST",
            f"{self.base_url}/api/generate",
            json={
                "model": kwargs.get("model", "codellama"),
                "prompt": prompt,
                "stream": True
            }
        ) as response:
            async for line in response.aiter_lines():
                if line:
                    data = json.loads(line)
                    yield data.get("response", "")

# ç»Ÿä¸€çš„æ¨¡å‹ç®¡ç†å™¨
class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨"""

    def __init__(self):
        self.providers: dict[str, LLMProvider] = {}
        self.default_provider = "openai"

    def register(self, name: str, provider: LLMProvider):
        """æ³¨å†Œæ¨¡å‹æä¾›è€…"""
        self.providers[name] = provider

    async def complete(
        self,
        prompt: str,
        provider: str = None,
        **kwargs
    ) -> str:
        """ä½¿ç”¨æŒ‡å®šæä¾›è€…è¡¥å…¨"""
        provider_name = provider or self.default_provider
        llm = self.providers[provider_name]
        return await llm.complete(prompt, **kwargs)

    async def stream_complete(
        self,
        prompt: str,
        provider: str = None,
        **kwargs
    ) -> AsyncIterator[str]:
        """æµå¼è¡¥å…¨"""
        provider_name = provider or self.default_provider
        llm = self.providers[provider_name]

        async for chunk in llm.stream_complete(prompt, **kwargs):
            yield chunk

# ä½¿ç”¨ç¤ºä¾‹
manager = ModelManager()
manager.register("openai", OpenAIProvider(api_key="..."))
manager.register("ollama", OllamaProvider())

# æ ¹æ®é…ç½®é€‰æ‹©æ¨¡å‹
result = await manager.complete("å†™ä¸€ä¸ªå¿«æ’", provider="openai")
```

## äº”ã€å¿«é€Ÿå¼€å§‹

### 5.1 ç¯å¢ƒå‡†å¤‡

```bash
# 1. å®‰è£…ä¾èµ–
npm install -g yo generator-code
pip install fastapi uvicorn langchain openai chromadb tree-sitter

# 2. åˆ›å»ºé¡¹ç›®ç»“æ„
mkdir code-agent
cd code-agent
mkdir -p {extension,backend,shared}

# 3. åˆå§‹åŒ– VSCode æ’ä»¶
cd extension
yo code

# 4. åˆå§‹åŒ–åç«¯
cd ../backend
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 5.2 æœ€å°å¯è¡Œäº§å“ï¼ˆMVPï¼‰

```typescript
// extension/src/extension.ts - VSCode æ’ä»¶å…¥å£
import * as vscode from 'vscode';

export function activate(context: vscode.ExtensionContext) {
    console.log('Code Agent å·²æ¿€æ´»');

    // æ³¨å†Œè¡¥å…¨æä¾›è€…
    const completionProvider = vscode.languages.registerInlineCompletionItemProvider(
        { pattern: '**' },
        {
            async provideInlineCompletionItems(document, position, context) {
                // è·å–å½“å‰è¡Œ
                const line = document.lineAt(position.line).text;
                const prefix = line.substring(0, position.character);

                // è°ƒç”¨åç«¯ API
                const response = await fetch('http://localhost:8000/complete', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ prefix })
                });

                const { completion } = await response.json();

                return [new vscode.InlineCompletionItem(completion)];
            }
        }
    );

    context.subscriptions.push(completionProvider);
}
```

```python
# backend/main.py - åç«¯ API
from fastapi import FastAPI
from pydantic import BaseModel
from openai import AsyncOpenAI

app = FastAPI()
client = AsyncOpenAI(api_key="your-api-key")

class CompletionRequest(BaseModel):
    prefix: str

@app.post("/complete")
async def complete(request: CompletionRequest):
    """ä»£ç è¡¥å…¨ API"""
    prompt = f"è¡¥å…¨ä»¥ä¸‹ä»£ç ï¼š\n{request.prefix}"

    response = await client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=100
    )

    completion = response.choices[0].message.content

    return {"completion": completion}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

## å…­ã€å­¦ä¹ èµ„æº

### 6.1 å®˜æ–¹æ–‡æ¡£
- [VSCode Extension API](https://code.visualstudio.com/api)
- [Language Server Protocol](https://microsoft.github.io/language-server-protocol/)
- [Tree-sitter](https://tree-sitter.github.io/tree-sitter/)
- [LangChain](https://python.langchain.com/)

### 6.2 å¼€æºé¡¹ç›®å‚è€ƒ
- **Continue**: å¼€æºçš„ AI ç¼–ç¨‹åŠ©æ‰‹
- **Tabby**: è‡ªæ‰˜ç®¡çš„ä»£ç è¡¥å…¨å·¥å…·
- **Cody**: Sourcegraph çš„ AI åŠ©æ‰‹
- **Aider**: å‘½ä»¤è¡Œ AI ç¼–ç¨‹å·¥å…·

### 6.3 ç›¸å…³æŠ€æœ¯
- **ç¼–è¯‘åŸç†**: ç†è§£ä»£ç è§£æ
- **å‘é‡æ•°æ®åº“**: ä»£ç æ£€ç´¢
- **Prompt Engineering**: ä¼˜åŒ– AI è¾“å‡º
- **IDE æ’ä»¶å¼€å‘**: VSCode/JetBrains æ’ä»¶

## ä¸ƒã€æ–‡æ¡£å¯¼èˆª

1. [01.æ¶æ„è®¾è®¡è¯¦è§£](./01.æ¶æ„è®¾è®¡è¯¦è§£.md)
2. [02.VSCodeæ’ä»¶å¼€å‘](./02.VSCodeæ’ä»¶å¼€å‘.md)
3. [03.ä»£ç åˆ†æå¼•æ“](./03.ä»£ç åˆ†æå¼•æ“.md)
4. [04.AIæœåŠ¡é›†æˆ](./04.AIæœåŠ¡é›†æˆ.md)
5. [05.ä»£ç è¡¥å…¨å®ç°](./05.ä»£ç è¡¥å…¨å®ç°.md)
6. [06.æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ](./06.æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ.md)
7. [07.æ€§èƒ½ä¼˜åŒ–æŒ‡å—](./07.æ€§èƒ½ä¼˜åŒ–æŒ‡å—.md)
8. [08.éƒ¨ç½²ä¸å‘å¸ƒ](./08.éƒ¨ç½²ä¸å‘å¸ƒ.md)

---

**ä¸‹ä¸€æ­¥**: æŸ¥çœ‹ [01.æ¶æ„è®¾è®¡è¯¦è§£](./01.æ¶æ„è®¾è®¡è¯¦è§£.md) æ·±å…¥äº†è§£ç³»ç»Ÿæ¶æ„ï¼
