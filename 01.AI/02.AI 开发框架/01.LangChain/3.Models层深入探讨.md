---
title: LangChainå®æˆ˜æ•™ç¨‹(ç¬¬ä¸‰å‘¨):Modelså±‚æ·±å…¥æ¢è®¨
date: 2025-01-12
permalink: /ai/langchain/week3-models-deep-dive.html
tags:
  - LangChain
categories:
  - LangChain
---

# ç¬¬3å‘¨ï¼šModels è¯¦è§£

::: tip æœ¬å‘¨å­¦ä¹ ç›®æ ‡
- ğŸ¤– ç†è§£ä¸åŒç±»å‹çš„è¯­è¨€æ¨¡å‹ï¼ˆLLM vs Chat Modelï¼‰
- ğŸ”§ æŒæ¡æ¨¡å‹å‚æ•°è°ƒä¼˜æŠ€å·§
- ğŸš€ å­¦ä¼šä½¿ç”¨æµå¼è¾“å‡ºï¼ˆStreamingï¼‰
- ğŸ’° ä¼˜åŒ– Token ä½¿ç”¨å’Œæˆæœ¬æ§åˆ¶
- ğŸŒ å¯¹æ¥å¤šä¸ª LLM æä¾›å•†
:::

## ä¸€ã€è¯­è¨€æ¨¡å‹åŸºç¡€

### 1.1 LLM vs Chat Models

LangChain ä¸­æœ‰ä¸¤ç§ä¸»è¦çš„æ¨¡å‹ç±»å‹ï¼š

```mermaid
graph TB
    A[è¯­è¨€æ¨¡å‹] --> B[LLM<br/>æ–‡æœ¬è¡¥å…¨æ¨¡å‹]
    A --> C[Chat Model<br/>å¯¹è¯æ¨¡å‹]

    B --> B1[è¾“å…¥: å­—ç¬¦ä¸²]
    B --> B2[è¾“å‡º: å­—ç¬¦ä¸²]
    B --> B3[ç¤ºä¾‹: GPT-3 text-davinci-003]

    C --> C1[è¾“å…¥: æ¶ˆæ¯åˆ—è¡¨]
    C --> C2[è¾“å‡º: æ¶ˆæ¯å¯¹è±¡]
    C --> C3[ç¤ºä¾‹: GPT-3.5-turbo, GPT-4]

    style A fill:#E3F2FD
    style B fill:#BBDEFB
    style C fill:#90CAF9
```

#### 1.1.1 LLMï¼ˆæ–‡æœ¬è¡¥å…¨æ¨¡å‹ï¼‰

```python
"""
LLM æ¨¡å‹ç¤ºä¾‹
ç‰¹ç‚¹ï¼šç®€å•çš„æ–‡æœ¬è¡¥å…¨ï¼Œè¾“å…¥å­—ç¬¦ä¸²ï¼Œè¾“å‡ºå­—ç¬¦ä¸²
é€‚ç”¨åœºæ™¯ï¼šç®€å•çš„æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
"""
from langchain_openai import OpenAI

# åˆå§‹åŒ– LLM
llm = OpenAI(
    model="gpt-3.5-turbo-instruct",  # æ–‡æœ¬è¡¥å…¨æ¨¡å‹
    temperature=0.7,
    max_tokens=100
)

# ç›´æ¥ä¼ å…¥å­—ç¬¦ä¸²
prompt = "è¯·ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼š"
response = llm.invoke(prompt)

print(f"è¾“å…¥ç±»å‹ï¼š{type(prompt)}")  # <class 'str'>
print(f"è¾“å‡ºç±»å‹ï¼š{type(response)}")  # <class 'str'>
print(f"å›ç­”ï¼š{response}")
```

#### 1.1.2 Chat Modelï¼ˆå¯¹è¯æ¨¡å‹ï¼‰

```python
"""
Chat Model ç¤ºä¾‹
ç‰¹ç‚¹ï¼šä¸“ä¸ºå¯¹è¯è®¾è®¡ï¼Œæ”¯æŒè§’è‰²ï¼ˆsystem/user/assistantï¼‰
é€‚ç”¨åœºæ™¯ï¼šéœ€è¦ä¸Šä¸‹æ–‡çš„å¯¹è¯ã€å¤šè½®äº¤äº’
"""
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage, AIMessage

# åˆå§‹åŒ– Chat Model
chat = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.7
)

# ä½¿ç”¨æ¶ˆæ¯åˆ—è¡¨
messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªPythonä¸“å®¶"),
    HumanMessage(content="ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ"),
    AIMessage(content="è£…é¥°å™¨æ˜¯ä¸€ç§ä¿®æ”¹å‡½æ•°è¡Œä¸ºçš„è¯­æ³•ç³–"),
    HumanMessage(content="èƒ½ä¸¾ä¸ªä¾‹å­å—ï¼Ÿ")
]

response = chat.invoke(messages)

print(f"è¾“å…¥ç±»å‹ï¼š{type(messages)}")  # <class 'list'>
print(f"è¾“å‡ºç±»å‹ï¼š{type(response)}")  # <class 'AIMessage'>
print(f"å›ç­”ï¼š{response.content}")
```

#### 1.1.3 å¦‚ä½•é€‰æ‹©ï¼Ÿ

| å¯¹æ¯”é¡¹ | LLM | Chat Model |
|--------|-----|-----------|
| **è¾“å…¥æ ¼å¼** | çº¯æ–‡æœ¬å­—ç¬¦ä¸² | æ¶ˆæ¯åˆ—è¡¨ï¼ˆæ”¯æŒè§’è‰²ï¼‰ |
| **è¾“å‡ºæ ¼å¼** | çº¯æ–‡æœ¬å­—ç¬¦ä¸² | æ¶ˆæ¯å¯¹è±¡ |
| **ä¸Šä¸‹æ–‡ç®¡ç†** | éœ€è¦æ‰‹åŠ¨æ‹¼æ¥ | å†…ç½®æ”¯æŒå¤šè½®å¯¹è¯ |
| **é€‚ç”¨åœºæ™¯** | ç®€å•æ–‡æœ¬ç”Ÿæˆ | å¯¹è¯ã€å®¢æœã€åŠ©æ‰‹ |
| **æˆæœ¬** | è¾ƒä½ï¼ˆè€æ¨¡å‹ï¼‰ | ç•¥é«˜ï¼ˆæ–°æ¨¡å‹ï¼‰ |
| **æ¨èç¨‹åº¦** | â­â­ | â­â­â­â­â­ |

**æ¨èï¼šä¼˜å…ˆä½¿ç”¨ Chat Model**ï¼Œå®ƒæ˜¯ä¸»æµè¶‹åŠ¿ï¼ŒåŠŸèƒ½æ›´å¼ºå¤§ã€‚

### 1.2 æ”¯æŒçš„æ¨¡å‹æä¾›å•†

LangChain æ”¯æŒå¤šä¸ª LLM æä¾›å•†ï¼š

```mermaid
graph LR
    A[LangChain] --> B[OpenAI]
    A --> C[Anthropic]
    A --> D[Google]
    A --> E[å¼€æºæ¨¡å‹]

    B --> B1[GPT-3.5/GPT-4]
    C --> C1[Claude 3]
    D --> D1[Gemini]
    E --> E1[Llama 2/Mistral]

    style A fill:#4CAF50
    style B fill:#00BCD4
    style C fill:#FF9800
    style D fill:#F44336
    style E fill:#9C27B0
```

#### 1.2.1 OpenAI é›†æˆ

```python
"""
OpenAI æ¨¡å‹é›†æˆ
"""
from langchain_openai import ChatOpenAI

# GPT-3.5 Turboï¼ˆæ¨èï¼Œæ€§ä»·æ¯”é«˜ï¼‰
gpt35 = ChatOpenAI(
    model="gpt-3.5-turbo",
    api_key="your-api-key",          # å¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–
    base_url="https://api.openai.com/v1",  # å¯é€‰ï¼Œè‡ªå®šä¹‰ API ç«¯ç‚¹
    temperature=0.7,
    max_tokens=1000
)

# GPT-4ï¼ˆæ€§èƒ½æœ€å¼ºï¼Œæˆæœ¬è¾ƒé«˜ï¼‰
gpt4 = ChatOpenAI(
    model="gpt-4",
    temperature=0.5
)

# GPT-4 Turboï¼ˆæ›´å¿«ã€æ›´ä¾¿å®œï¼‰
gpt4_turbo = ChatOpenAI(
    model="gpt-4-turbo-preview",
    temperature=0.7
)
```

#### 1.2.2 Anthropic Claude é›†æˆ

```python
"""
Anthropic Claude é›†æˆ
ç‰¹ç‚¹ï¼šé•¿ä¸Šä¸‹æ–‡ï¼ˆ100K tokensï¼‰ã€æ›´å®‰å…¨
"""
from langchain_anthropic import ChatAnthropic

claude = ChatAnthropic(
    model="claude-3-opus-20240229",  # Claude 3 Opus
    anthropic_api_key="your-api-key",
    temperature=0.7,
    max_tokens=1024
)

# Claude æ¨¡å‹ç³»åˆ—
models = {
    "claude-3-opus-20240229": "æœ€å¼ºæ€§èƒ½",
    "claude-3-sonnet-20240229": "å¹³è¡¡æ€§èƒ½å’Œæˆæœ¬",
    "claude-3-haiku-20240307": "æœ€å¿«æœ€ä¾¿å®œ"
}
```

#### 1.2.3 å¼€æºæ¨¡å‹é›†æˆï¼ˆOllamaï¼‰

```python
"""
ä½¿ç”¨æœ¬åœ°å¼€æºæ¨¡å‹ï¼ˆOllamaï¼‰
ä¼˜ç‚¹ï¼šå…è´¹ã€éšç§ã€æ— éœ€ç½‘ç»œ
å‰æï¼šéœ€è¦æœ¬åœ°å®‰è£… Ollama
"""
from langchain_community.llms import Ollama

# ä½¿ç”¨ Llama 2
llama = Ollama(
    model="llama2",      # æ¨¡å‹åç§°
    base_url="http://localhost:11434"  # Ollama æœåŠ¡åœ°å€
)

response = llama.invoke("è§£é‡Šä»€ä¹ˆæ˜¯ Docker")
print(response)

# å…¶ä»–å¯ç”¨æ¨¡å‹ï¼šmistral, codellama, phi, etc.
```

#### 1.2.4 å¤šæä¾›å•†ç»Ÿä¸€æ¥å£

```python
"""
ä½¿ç”¨å·¥å‚æ¨¡å¼æ”¯æŒå¤šä¸ªæä¾›å•†
ä¼˜ç‚¹ï¼šè½»æ¾åˆ‡æ¢æ¨¡å‹ï¼Œä¾¿äº A/B æµ‹è¯•
"""
from typing import Literal
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_community.llms import Ollama

def create_model(
    provider: Literal["openai", "anthropic", "ollama"],
    model_name: str = None,
    **kwargs
):
    """
    æ¨¡å‹å·¥å‚å‡½æ•°

    å‚æ•°:
        provider: æä¾›å•†åç§°
        model_name: æ¨¡å‹åç§°
        **kwargs: å…¶ä»–å‚æ•°
    """
    if provider == "openai":
        return ChatOpenAI(model=model_name or "gpt-3.5-turbo", **kwargs)
    elif provider == "anthropic":
        return ChatAnthropic(model=model_name or "claude-3-sonnet-20240229", **kwargs)
    elif provider == "ollama":
        return Ollama(model=model_name or "llama2", **kwargs)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æä¾›å•†: {provider}")

# ä½¿ç”¨ç¤ºä¾‹
model = create_model("openai", temperature=0.7)
# è½»æ¾åˆ‡æ¢åˆ°å…¶ä»–æä¾›å•†
# model = create_model("anthropic", temperature=0.7)
```


## ä¸‰ã€æµå¼è¾“å‡ºï¼ˆStreamingï¼‰

### 3.1 ä»€ä¹ˆæ˜¯æµå¼è¾“å‡ºï¼Ÿ

**æµå¼è¾“å‡º**æ˜¯æŒ‡ AI é€å­—é€å¥ç”Ÿæˆå†…å®¹ï¼Œè€Œéç­‰å¾…å…¨éƒ¨å†…å®¹ç”Ÿæˆå®Œæ¯•åä¸€æ¬¡æ€§è¿”å›ã€‚

```mermaid
sequenceDiagram
    participant C as Client
    participant M as Model

    Note over C,M: ä¼ ç»Ÿæ–¹å¼ï¼ˆé˜»å¡ï¼‰
    C->>M: å‘é€è¯·æ±‚
    M->>M: ç”Ÿæˆå®Œæ•´å›ç­”
    M-->>C: è¿”å›å®Œæ•´ç»“æœ
    Note over C: ç”¨æˆ·ç­‰å¾…æ—¶é—´é•¿

    Note over C,M: æµå¼è¾“å‡ºï¼ˆéé˜»å¡ï¼‰
    C->>M: å‘é€è¯·æ±‚
    M-->>C: é€å—è¿”å›
    M-->>C: é€å—è¿”å›
    M-->>C: é€å—è¿”å›
    Note over C: ç”¨æˆ·ç«‹å³çœ‹åˆ°å†…å®¹
```

**ä¼˜åŠ¿ï¼š**
- âœ… **æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ**ï¼šç«‹å³çœ‹åˆ°è¾“å‡ºï¼Œå‡å°‘ç­‰å¾…æ„Ÿ
- âœ… **é€‚åˆé•¿æ–‡æœ¬**ï¼šç”Ÿæˆæ–‡ç« ã€æŠ¥å‘Šç­‰
- âœ… **å®æ—¶åé¦ˆ**ï¼šå¯ä»¥æå‰åˆ¤æ–­è¾“å‡ºè´¨é‡

### 3.2 å®ç°æµå¼è¾“å‡º

#### æ–¹æ³•1ï¼šä½¿ç”¨ stream() æ–¹æ³•

```python
"""
åŸºç¡€æµå¼è¾“å‡º
"""
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

llm = ChatOpenAI(model="gpt-3.5-turbo", streaming=True)

prompt = "è¯·è¯¦ç»†ä»‹ç» Python è¯­è¨€çš„å†å²å’Œå‘å±•"

# æµå¼è¾“å‡º
print("AI å›ç­”ï¼š", end="", flush=True)
for chunk in llm.stream([HumanMessage(content=prompt)]):
    print(chunk.content, end="", flush=True)
print()  # æ¢è¡Œ
```

#### æ–¹æ³•2ï¼šä½¿ç”¨ Callbacks

```python
"""
ä½¿ç”¨ Streaming Callback Handler
æ›´çµæ´»ï¼Œå¯ä»¥è‡ªå®šä¹‰å¤„ç†é€»è¾‘
"""
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# åˆ›å»ºå¸¦å›è°ƒçš„æ¨¡å‹
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    streaming=True,
    callbacks=[StreamingStdOutCallbackHandler()]  # è‡ªåŠ¨æ‰“å°åˆ°æ ‡å‡†è¾“å‡º
)

prompt = "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—"
response = llm.invoke([HumanMessage(content=prompt)])
# å†…å®¹ä¼šå®æ—¶æ‰“å°ï¼Œæ— éœ€æ‰‹åŠ¨å¤„ç†
```

#### æ–¹æ³•3ï¼šè‡ªå®šä¹‰ Callback

```python
"""
è‡ªå®šä¹‰ Streaming Callback
å¯ä»¥å®ç°æ›´å¤æ‚çš„é€»è¾‘ï¼Œå¦‚ï¼š
- å®æ—¶ä¿å­˜åˆ°æ–‡ä»¶
- å‘é€åˆ° WebSocket
- å®æ—¶ç»Ÿè®¡ Token
"""
from langchain.callbacks.base import BaseCallbackHandler
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage
from typing import Any, Dict

class CustomStreamHandler(BaseCallbackHandler):
    """è‡ªå®šä¹‰æµå¼å¤„ç†å™¨"""

    def __init__(self):
        self.tokens = []
        self.token_count = 0

    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        """æ¯ç”Ÿæˆä¸€ä¸ªæ–° token æ—¶è°ƒç”¨"""
        self.tokens.append(token)
        self.token_count += 1

        # å®æ—¶æ‰“å°ï¼ˆå¯ä»¥æ”¹ä¸ºå…¶ä»–æ“ä½œï¼‰
        print(f"[Token {self.token_count}] {token}", end="", flush=True)

    def on_llm_end(self, response, **kwargs: Any) -> None:
        """ç”Ÿæˆç»“æŸæ—¶è°ƒç”¨"""
        print(f"\n\næ€»å…±ç”Ÿæˆ {self.token_count} ä¸ª token")

# ä½¿ç”¨è‡ªå®šä¹‰å¤„ç†å™¨
handler = CustomStreamHandler()
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    streaming=True,
    callbacks=[handler]
)

response = llm.invoke([HumanMessage(content="è§£é‡Šä»€ä¹ˆæ˜¯åŒºå—é“¾")])
print(f"\nå®Œæ•´å†…å®¹ï¼š\n{''.join(handler.tokens)}")
```

### 3.3 å®æˆ˜ï¼šæ„å»ºå®æ—¶èŠå¤©ç•Œé¢

```python
"""
å®æˆ˜é¡¹ç›®ï¼šå¸¦è¿›åº¦æ˜¾ç¤ºçš„æµå¼èŠå¤©
åŠŸèƒ½ï¼š
1. å®æ—¶æ˜¾ç¤º AI å›ç­”
2. æ˜¾ç¤ºç”Ÿæˆè¿›åº¦
3. ç»Ÿè®¡ Token ä½¿ç”¨
"""
import sys
import time
from langchain.callbacks.base import BaseCallbackHandler
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage

class ProgressStreamHandler(BaseCallbackHandler):
    """å¸¦è¿›åº¦æ˜¾ç¤ºçš„æµå¼å¤„ç†å™¨"""

    def __init__(self):
        self.tokens = []
        self.start_time = None

    def on_llm_start(self, serialized: Dict, prompts, **kwargs) -> None:
        """å¼€å§‹ç”Ÿæˆæ—¶è°ƒç”¨"""
        self.start_time = time.time()
        print("\nğŸ¤– AI æ­£åœ¨æ€è€ƒ...\n")

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        """æ–° token ç”Ÿæˆ"""
        self.tokens.append(token)
        sys.stdout.write(token)
        sys.stdout.flush()

    def on_llm_end(self, response, **kwargs) -> None:
        """ç”Ÿæˆç»“æŸ"""
        elapsed = time.time() - self.start_time
        token_count = len(self.tokens)
        speed = token_count / elapsed if elapsed > 0 else 0

        print(f"\n\n{'='*60}")
        print(f"âœ… ç”Ÿæˆå®Œæˆ")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯ï¼š")
        print(f"   - Token æ•°é‡ï¼š{token_count}")
        print(f"   - è€—æ—¶ï¼š{elapsed:.2f}ç§’")
        print(f"   - é€Ÿåº¦ï¼š{speed:.1f} tokens/ç§’")
        print(f"{'='*60}\n")

def main():
    """ä¸»å‡½æ•°"""
    handler = ProgressStreamHandler()
    llm = ChatOpenAI(
        model="gpt-3.5-turbo",
        temperature=0.7,
        streaming=True,
        callbacks=[handler]
    )

    # å¯¹è¯å†å²
    messages = [
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„ AI åŠ©æ‰‹")
    ]

    print("=" * 60)
    print("å®æ—¶æµå¼èŠå¤©ç³»ç»Ÿ")
    print("è¾“å…¥ 'exit' é€€å‡º")
    print("=" * 60)

    while True:
        # è·å–ç”¨æˆ·è¾“å…¥
        user_input = input("\nä½ : ").strip()

        if user_input.lower() == 'exit':
            print("å†è§ï¼ğŸ‘‹")
            break

        if not user_input:
            continue

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        messages.append(HumanMessage(content=user_input))

        # é‡ç½® handler
        handler.tokens = []

        # æµå¼ç”Ÿæˆå›ç­”
        print("\nAI: ", end="")
        response = llm.invoke(messages)

        # æ·»åŠ  AI å›å¤åˆ°å†å²
        messages.append(response)

if __name__ == "__main__":
    main()
```

### 3.4 æµå¼è¾“å‡ºçš„æ³¨æ„äº‹é¡¹

::: warning æ€§èƒ½è€ƒè™‘
1. **ç½‘ç»œå¼€é”€**ï¼šæµå¼è¾“å‡ºä¼šå¢åŠ ç½‘ç»œè¯·æ±‚æ¬¡æ•°
2. **å»¶è¿Ÿ**ï¼šæ¯ä¸ª token éƒ½æœ‰ç½‘ç»œå¾€è¿”æ—¶é—´
3. **é€‚ç”¨åœºæ™¯**ï¼šä¸»è¦ç”¨äºç”¨æˆ·ç•Œé¢ï¼Œåå°å¤„ç†ä¸æ¨è
:::

**ä½•æ—¶ä½¿ç”¨æµå¼è¾“å‡ºï¼Ÿ**

| åœºæ™¯ | æ˜¯å¦ä½¿ç”¨ | åŸå›  |
|------|---------|------|
| Web èŠå¤©ç•Œé¢ | âœ… æ¨è | æå‡ç”¨æˆ·ä½“éªŒ |
| å‘½ä»¤è¡Œäº¤äº’ | âœ… æ¨è | å®æ—¶åé¦ˆ |
| é•¿æ–‡æœ¬ç”Ÿæˆ | âœ… æ¨è | å‡å°‘ç­‰å¾…æ„Ÿ |
| æ‰¹é‡å¤„ç† | âŒ ä¸æ¨è | å¢åŠ å¼€é”€ |
| API è°ƒç”¨ | âŒ ä¸æ¨è | å¤æ‚åº¦é«˜ |
| æ•°æ®åˆ†æ | âŒ ä¸æ¨è | ä¸éœ€è¦å®æ—¶æ€§ |


## äº”ã€æœ¬å‘¨ç»ƒä¹ é¢˜

### ç»ƒä¹ 1ï¼šå‚æ•°å®éªŒï¼ˆéš¾åº¦ï¼šâ­â­ï¼‰

**ä»»åŠ¡**ï¼šå¯¹æ¯”ä¸åŒ temperature å’Œ top_p ç»„åˆçš„è¾“å‡ºæ•ˆæœã€‚

**è¦æ±‚**ï¼š
1. ä½¿ç”¨åŒä¸€ä¸ª Prompt
2. æµ‹è¯•è‡³å°‘ 6 ç§å‚æ•°ç»„åˆ
3. åˆ†æè¾“å‡ºçš„å·®å¼‚

<details>
<summary>æŸ¥çœ‹å‚è€ƒç­”æ¡ˆ</summary>

```python
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

prompt = "å†™ä¸€é¦–å››è¡Œå°è¯—ï¼Œä¸»é¢˜æ˜¯ç§‹å¤©"

configs = [
    {"temperature": 0.0, "top_p": 1.0, "name": "ç¡®å®šæ€§"},
    {"temperature": 0.5, "top_p": 1.0, "name": "ä½åˆ›é€ æ€§"},
    {"temperature": 1.0, "top_p": 1.0, "name": "é«˜åˆ›é€ æ€§"},
    {"temperature": 1.0, "top_p": 0.1, "name": "ç¨³å®šéšæœº"},
    {"temperature": 1.0, "top_p": 0.5, "name": "ä¸­ç­‰éšæœº"},
    {"temperature": 1.5, "top_p": 1.0, "name": "æé«˜åˆ›é€ æ€§"},
]

for config in configs:
    llm = ChatOpenAI(
        model="gpt-3.5-turbo",
        temperature=config["temperature"],
        top_p=config["top_p"]
    )

    print(f"\n{'='*60}")
    print(f"{config['name']} (temp={config['temperature']}, top_p={config['top_p']})")
    print('='*60)

    for i in range(2):  # æ¯ä¸ªé…ç½®ç”Ÿæˆ2æ¬¡
        response = llm.invoke([HumanMessage(content=prompt)])
        print(f"\nç¬¬{i+1}æ¬¡ï¼š\n{response.content}")
```
</details>

### ç»ƒä¹ 2ï¼šæˆæœ¬ä¼˜åŒ–ï¼ˆéš¾åº¦ï¼šâ­â­ï¼‰

**ä»»åŠ¡**ï¼šå®ç°ä¸€ä¸ªæ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼Œæ ¹æ®é—®é¢˜å¤æ‚åº¦è‡ªåŠ¨é€‰æ‹©æ¨¡å‹ï¼Œæœ€å°åŒ–æˆæœ¬ã€‚

**è¦æ±‚**ï¼š
1. ç®€å•é—®é¢˜ç”¨ GPT-3.5
2. å¤æ‚é—®é¢˜ç”¨ GPT-4
3. ç»Ÿè®¡æ€»æˆæœ¬

<details>
<summary>æŸ¥çœ‹å‚è€ƒç­”æ¡ˆ</summary>

```python
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage
from langchain.callbacks import get_openai_callback

class CostOptimizedQA:
    def __init__(self):
        self.gpt35 = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
        self.gpt4 = ChatOpenAI(model="gpt-4", temperature=0)
        self.total_cost = 0

    def judge_complexity(self, question: str) -> str:
        """åˆ¤æ–­é—®é¢˜å¤æ‚åº¦"""
        judge_prompt = f"""åˆ¤æ–­ä»¥ä¸‹é—®é¢˜æ˜¯"ç®€å•"è¿˜æ˜¯"å¤æ‚"ï¼Ÿ
ç®€å•ï¼šäº‹å®æŸ¥è¯¢ã€åŸºç¡€çŸ¥è¯†
å¤æ‚ï¼šéœ€è¦æ¨ç†ã€åˆ›é€ æ€§ã€å¤šæ­¥éª¤

é—®é¢˜ï¼š{question}
åªå›ç­”"ç®€å•"æˆ–"å¤æ‚"ï¼š"""

        with get_openai_callback() as cb:
            response = self.gpt35.invoke([HumanMessage(content=judge_prompt)])
            self.total_cost += cb.total_cost

        return response.content.strip()

    def answer(self, question: str):
        """å›ç­”é—®é¢˜"""
        complexity = self.judge_complexity(question)

        if "å¤æ‚" in complexity:
            model = self.gpt4
            model_name = "GPT-4"
        else:
            model = self.gpt35
            model_name = "GPT-3.5"

        print(f"[ä½¿ç”¨ {model_name}]")

        with get_openai_callback() as cb:
            response = model.invoke([HumanMessage(content=question)])
            self.total_cost += cb.total_cost

        return response.content

# æµ‹è¯•
qa = CostOptimizedQA()

questions = [
    "Python ä¹‹çˆ¶æ˜¯è°ï¼Ÿ",
    "è®¾è®¡ä¸€ä¸ªé«˜å¹¶å‘çš„ç”µå•†ç³»ç»Ÿæ¶æ„",
    "1+1ç­‰äºå‡ ï¼Ÿ"
]

for q in questions:
    print(f"\né—®é¢˜ï¼š{q}")
    answer = qa.answer(q)
    print(f"å›ç­”ï¼š{answer}\n")

print(f"\næ€»æˆæœ¬ï¼š${qa.total_cost:.6f}")
```
</details>

### ç»ƒä¹ 3ï¼šæµå¼èŠå¤©æœºå™¨äººï¼ˆéš¾åº¦ï¼šâ­â­â­ï¼‰

**ä»»åŠ¡**ï¼šæ„å»ºä¸€ä¸ªå¸¦æµå¼è¾“å‡ºçš„å¤šè½®å¯¹è¯æœºå™¨äººã€‚

**è¦æ±‚**ï¼š
1. æ”¯æŒæµå¼æ˜¾ç¤º AI å›ç­”
2. è®°å½•å¯¹è¯å†å²
3. æ˜¾ç¤º Token ä½¿ç”¨ç»Ÿè®¡
4. æ”¯æŒå¯¼å‡ºå¯¹è¯

<details>
<summary>æŸ¥çœ‹æç¤º</summary>

å‚è€ƒæœ¬å‘¨ 3.3 èŠ‚çš„"å®æˆ˜ï¼šæ„å»ºå®æ—¶èŠå¤©ç•Œé¢"ï¼Œå¹¶æ·»åŠ ï¼š
- å¯¹è¯å†å²ç®¡ç†
- Token ç»Ÿè®¡
- å¯¼å‡ºåŠŸèƒ½
</details>


::: tip å­¦ä¹ å»ºè®®
1. **åŠ¨æ‰‹å®éªŒ**ï¼šå°è¯•ä¸åŒå‚æ•°ç»„åˆï¼Œè§‚å¯Ÿæ•ˆæœ
2. **æˆæœ¬æ„è¯†**ï¼šå§‹ç»ˆå…³æ³¨ Token ä½¿ç”¨å’Œæˆæœ¬
3. **æ€§èƒ½ä¼˜åŒ–**ï¼šåˆç†ä½¿ç”¨ç¼“å­˜å’Œæ‰¹é‡å¤„ç†
4. **ç”¨æˆ·ä½“éªŒ**ï¼šåœ¨éœ€è¦çš„åœºæ™¯ä½¿ç”¨æµå¼è¾“å‡º
:::

**æœ¬å‘¨è¾›è‹¦äº†ï¼ä¸‹å‘¨ç»§ç»­ï¼ğŸš€**
