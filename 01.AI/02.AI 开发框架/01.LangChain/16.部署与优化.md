---
title: LangChainå®æˆ˜æ•™ç¨‹(ç¬¬åå…­å‘¨):éƒ¨ç½²ä¸ä¼˜åŒ–
date: 2025-01-21
permalink: /ai/langchain/week16-deployment-optimization.html
tags:
  - LangChain
categories:
  - LangChain
---

# éƒ¨ç½²ä¸ä¼˜åŒ–

## æœ¬å‘¨å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬å‘¨çš„å­¦ä¹ ,ä½ å°†æŒæ¡:

1. ç†è§£ç”Ÿäº§ç¯å¢ƒçš„æ ¸å¿ƒéœ€æ±‚
2. æŒæ¡æ€§èƒ½ä¼˜åŒ–æŠ€å·§
3. å®ç°ç¼“å­˜ç­–ç•¥
4. æŒæ¡å¼‚æ­¥ç¼–ç¨‹
5. å®ç°é”™è¯¯å¤„ç†å’Œé‡è¯•
6. æŒæ¡æˆæœ¬æ§åˆ¶æ–¹æ³•
7. å®ç°å®‰å…¨é˜²æŠ¤
8. æŒæ¡å®Œæ•´çš„éƒ¨ç½²æµç¨‹

## ä¸€ã€æ€§èƒ½ä¼˜åŒ–

### 1.1 æ€§èƒ½ä¼˜åŒ–å…¨æ™¯å›¾

```mermaid
graph TB
    A[æ€§èƒ½ä¼˜åŒ–] --> B[ç¼“å­˜ä¼˜åŒ–]
    A --> C[å¹¶å‘ä¼˜åŒ–]
    A --> D[æ‰¹é‡å¤„ç†]
    A --> E[æ¨¡å‹ä¼˜åŒ–]
    A --> F[ç½‘ç»œä¼˜åŒ–]

    B --> B1[Embeddingç¼“å­˜]
    B --> B2[ç»“æœç¼“å­˜]
    B --> B3[æ–‡æ¡£ç¼“å­˜]

    C --> C1[å¼‚æ­¥è°ƒç”¨]
    C --> C2[å¹¶è¡Œå¤„ç†]
    C --> C3[è¿æ¥æ± ]

    D --> D1[æ‰¹é‡Embedding]
    D --> D2[æ‰¹é‡æ£€ç´¢]

    E --> E1[æ¨¡å‹é€‰æ‹©]
    E --> E2[å‚æ•°ä¼˜åŒ–]

    F --> F1[CDNåŠ é€Ÿ]
    F --> F2[è´Ÿè½½å‡è¡¡]

    style A fill:#f96,stroke:#333,stroke-width:4px
```

### 1.2 Embedding ç¼“å­˜ä¼˜åŒ–

```python
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore, RedisStore
from langchain_openai import OpenAIEmbeddings
from typing import List
import hashlib
import time
import redis

# ===== æ–¹æ³•1: æ–‡ä»¶ç³»ç»Ÿç¼“å­˜ =====
class FileBasedEmbeddingCache:
    """åŸºäºæ–‡ä»¶ç³»ç»Ÿçš„ Embedding ç¼“å­˜"""

    def __init__(self, cache_dir: str = "./embedding_cache"):
        """
        åˆå§‹åŒ–ç¼“å­˜

        å‚æ•°:
            cache_dir: ç¼“å­˜ç›®å½•
        """
        # åˆ›å»ºæ–‡ä»¶å­˜å‚¨
        self.store = LocalFileStore(cache_dir)

        # åˆ›å»ºåº•å±‚ Embeddings
        underlying_embeddings = OpenAIEmbeddings()

        # åŒ…è£…ä¸ºç¼“å­˜ Embeddings
        self.embeddings = CacheBackedEmbeddings.from_bytes_store(
            underlying_embeddings=underlying_embeddings,
            document_embedding_cache=self.store,
            namespace=underlying_embeddings.model
        )

        print(f"âœ“ æ–‡ä»¶ç¼“å­˜å·²åˆå§‹åŒ–: {cache_dir}")

    def embed_query(self, text: str) -> List[float]:
        """åµŒå…¥æŸ¥è¯¢(å¸¦ç¼“å­˜)"""
        return self.embeddings.embed_query(text)

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """åµŒå…¥æ–‡æ¡£(å¸¦ç¼“å­˜)"""
        return self.embeddings.embed_documents(texts)

# ===== æ–¹æ³•2: Redis ç¼“å­˜ =====
class RedisEmbeddingCache:
    """åŸºäº Redis çš„ Embedding ç¼“å­˜"""

    def __init__(
        self,
        redis_url: str = "redis://localhost:6379",
        ttl: int = 3600
    ):
        """
        åˆå§‹åŒ– Redis ç¼“å­˜

        å‚æ•°:
            redis_url: Redis è¿æ¥ URL
            ttl: ç¼“å­˜è¿‡æœŸæ—¶é—´(ç§’)
        """
        # åˆ›å»º Redis å­˜å‚¨
        self.store = RedisStore(redis_url=redis_url, ttl=ttl)

        # åˆ›å»ºåº•å±‚ Embeddings
        underlying_embeddings = OpenAIEmbeddings()

        # åŒ…è£…ä¸ºç¼“å­˜ Embeddings
        self.embeddings = CacheBackedEmbeddings.from_bytes_store(
            underlying_embeddings=underlying_embeddings,
            document_embedding_cache=self.store,
            namespace=underlying_embeddings.model
        )

        print(f"âœ“ Redis ç¼“å­˜å·²åˆå§‹åŒ–: {redis_url}")

    def embed_query(self, text: str) -> List[float]:
        """åµŒå…¥æŸ¥è¯¢(å¸¦ç¼“å­˜)"""
        return self.embeddings.embed_query(text)

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """åµŒå…¥æ–‡æ¡£(å¸¦ç¼“å­˜)"""
        return self.embeddings.embed_documents(texts)

# ===== æ–¹æ³•3: è‡ªå®šä¹‰æ™ºèƒ½ç¼“å­˜ =====
class SmartEmbeddingCache:
    """æ™ºèƒ½ Embedding ç¼“å­˜ç³»ç»Ÿ"""

    def __init__(
        self,
        redis_url: str = "redis://localhost:6379",
        local_cache_size: int = 1000,
        ttl: int = 3600
    ):
        """
        åˆå§‹åŒ–æ™ºèƒ½ç¼“å­˜

        å‚æ•°:
            redis_url: Redis URL
            local_cache_size: æœ¬åœ°ç¼“å­˜å¤§å°
            ttl: Redis ç¼“å­˜è¿‡æœŸæ—¶é—´
        """
        from collections import OrderedDict

        self.redis_client = redis.from_url(redis_url)
        self.local_cache = OrderedDict()
        self.local_cache_size = local_cache_size
        self.ttl = ttl

        self.embeddings = OpenAIEmbeddings()

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "local_hits": 0,
            "redis_hits": 0,
            "misses": 0,
            "total_requests": 0
        }

    def _get_cache_key(self, text: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return f"embed:{hashlib.md5(text.encode()).hexdigest()}"

    def embed_query(self, text: str) -> List[float]:
        """
        åµŒå…¥æŸ¥è¯¢(æ™ºèƒ½ç¼“å­˜)

        ç¼“å­˜å±‚æ¬¡:
        1. æœ¬åœ°å†…å­˜ç¼“å­˜(æœ€å¿«)
        2. Redis ç¼“å­˜(å¿«)
        3. API è°ƒç”¨(æ…¢)
        """
        self.stats["total_requests"] += 1
        cache_key = self._get_cache_key(text)

        # 1. æ£€æŸ¥æœ¬åœ°ç¼“å­˜
        if cache_key in self.local_cache:
            self.stats["local_hits"] += 1
            # ç§»åˆ°æœ«å°¾(LRU)
            self.local_cache.move_to_end(cache_key)
            return self.local_cache[cache_key]

        # 2. æ£€æŸ¥ Redis ç¼“å­˜
        cached = self.redis_client.get(cache_key)
        if cached:
            self.stats["redis_hits"] += 1
            import json
            vector = json.loads(cached)

            # æ›´æ–°æœ¬åœ°ç¼“å­˜
            self._update_local_cache(cache_key, vector)

            return vector

        # 3. è°ƒç”¨ API
        self.stats["misses"] += 1
        vector = self.embeddings.embed_query(text)

        # 4. æ›´æ–°ç¼“å­˜
        self._update_local_cache(cache_key, vector)
        self._update_redis_cache(cache_key, vector)

        return vector

    def _update_local_cache(self, key: str, value: List[float]):
        """æ›´æ–°æœ¬åœ°ç¼“å­˜(LRU)"""
        self.local_cache[key] = value
        self.local_cache.move_to_end(key)

        # ä¿æŒç¼“å­˜å¤§å°
        if len(self.local_cache) > self.local_cache_size:
            self.local_cache.popitem(last=False)

    def _update_redis_cache(self, key: str, value: List[float]):
        """æ›´æ–° Redis ç¼“å­˜"""
        import json
        self.redis_client.setex(
            key,
            self.ttl,
            json.dumps(value)
        )

    def get_stats(self) -> dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total = self.stats["total_requests"]
        if total == 0:
            return self.stats

        return {
            **self.stats,
            "local_hit_rate": f"{self.stats['local_hits']/total*100:.2f}%",
            "redis_hit_rate": f"{self.stats['redis_hits']/total*100:.2f}%",
            "miss_rate": f"{self.stats['misses']/total*100:.2f}%",
            "total_hit_rate": f"{(self.stats['local_hits']+self.stats['redis_hits'])/total*100:.2f}%"
        }

# ===== æ€§èƒ½æµ‹è¯• =====
def benchmark_caching():
    """æµ‹è¯•ç¼“å­˜æ€§èƒ½"""
    import time

    texts = [
        "What is machine learning?",
        "Explain deep learning",
        "What is NLP?",
        "What is machine learning?",  # é‡å¤
        "Explain deep learning",       # é‡å¤
    ]

    # æµ‹è¯•æ— ç¼“å­˜
    print("æµ‹è¯•1: æ— ç¼“å­˜")
    embeddings_no_cache = OpenAIEmbeddings()

    start = time.time()
    for text in texts:
        embeddings_no_cache.embed_query(text)
    time_no_cache = time.time() - start

    print(f"è€—æ—¶: {time_no_cache:.2f}ç§’\n")

    # æµ‹è¯•æ–‡ä»¶ç¼“å­˜
    print("æµ‹è¯•2: æ–‡ä»¶ç¼“å­˜")
    cache1 = FileBasedEmbeddingCache()

    start = time.time()
    for text in texts:
        cache1.embed_query(text)
    time_file_cache = time.time() - start

    print(f"è€—æ—¶: {time_file_cache:.2f}ç§’\n")

    # æµ‹è¯•æ™ºèƒ½ç¼“å­˜
    print("æµ‹è¯•3: æ™ºèƒ½ç¼“å­˜")
    cache2 = SmartEmbeddingCache()

    start = time.time()
    for text in texts:
        cache2.embed_query(text)
    time_smart_cache = time.time() - start

    print(f"è€—æ—¶: {time_smart_cache:.2f}ç§’")
    print(f"ç»Ÿè®¡: {cache2.get_stats()}\n")

    # æ€§èƒ½å¯¹æ¯”
    print("="*60)
    print("æ€§èƒ½å¯¹æ¯”:")
    print(f"æ— ç¼“å­˜:   {time_no_cache:.2f}ç§’ (åŸºå‡†)")
    print(f"æ–‡ä»¶ç¼“å­˜: {time_file_cache:.2f}ç§’ (æå‡ {time_no_cache/time_file_cache:.1f}x)")
    print(f"æ™ºèƒ½ç¼“å­˜: {time_smart_cache:.2f}ç§’ (æå‡ {time_no_cache/time_smart_cache:.1f}x)")

if __name__ == "__main__":
    benchmark_caching()
```

### 1.3 ç»“æœç¼“å­˜

```python
from functools import lru_cache
from typing import List, Optional
import hashlib
import json
import redis
import time

class ResultCache:
    """RAG ç»“æœç¼“å­˜ç³»ç»Ÿ"""

    def __init__(
        self,
        redis_url: str = "redis://localhost:6379",
        ttl: int = 3600
    ):
        """
        åˆå§‹åŒ–ç»“æœç¼“å­˜

        å‚æ•°:
            redis_url: Redis URL
            ttl: ç¼“å­˜è¿‡æœŸæ—¶é—´(ç§’)
        """
        self.redis_client = redis.from_url(redis_url)
        self.ttl = ttl

        # ç»Ÿè®¡
        self.hits = 0
        self.misses = 0

    def _get_cache_key(self, query: str, metadata: Optional[dict] = None) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        cache_input = f"{query}:{json.dumps(metadata, sort_keys=True) if metadata else ''}"
        return f"result:{hashlib.md5(cache_input.encode()).hexdigest()}"

    def get(self, query: str, metadata: Optional[dict] = None) -> Optional[dict]:
        """
        è·å–ç¼“å­˜çš„ç»“æœ

        å‚æ•°:
            query: æŸ¥è¯¢
            metadata: å…ƒæ•°æ®

        è¿”å›:
            ç¼“å­˜çš„ç»“æœæˆ– None
        """
        cache_key = self._get_cache_key(query, metadata)

        cached = self.redis_client.get(cache_key)

        if cached:
            self.hits += 1
            return json.loads(cached)
        else:
            self.misses += 1
            return None

    def set(
        self,
        query: str,
        result: dict,
        metadata: Optional[dict] = None,
        ttl: Optional[int] = None
    ):
        """
        è®¾ç½®ç¼“å­˜

        å‚æ•°:
            query: æŸ¥è¯¢
            result: ç»“æœ
            metadata: å…ƒæ•°æ®
            ttl: è¿‡æœŸæ—¶é—´(å¯é€‰)
        """
        cache_key = self._get_cache_key(query, metadata)
        ttl = ttl or self.ttl

        self.redis_client.setex(
            cache_key,
            ttl,
            json.dumps(result, ensure_ascii=False)
        )

    def invalidate(self, query: str, metadata: Optional[dict] = None):
        """
        ä½¿ç¼“å­˜å¤±æ•ˆ

        å‚æ•°:
            query: æŸ¥è¯¢
            metadata: å…ƒæ•°æ®
        """
        cache_key = self._get_cache_key(query, metadata)
        self.redis_client.delete(cache_key)

    def clear_all(self):
        """æ¸…ç©ºæ‰€æœ‰ç»“æœç¼“å­˜"""
        keys = self.redis_client.keys("result:*")
        if keys:
            self.redis_client.delete(*keys)

    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        total = self.hits + self.misses
        hit_rate = (self.hits / total * 100) if total > 0 else 0

        return {
            "hits": self.hits,
            "misses": self.misses,
            "total": total,
            "hit_rate": f"{hit_rate:.2f}%"
        }

# ===== å¸¦ç¼“å­˜çš„ RAG ç³»ç»Ÿ =====
class CachedRAGSystem:
    """å¸¦ç¼“å­˜çš„ RAG ç³»ç»Ÿ"""

    def __init__(self, rag_system, cache: ResultCache):
        """
        åˆå§‹åŒ–

        å‚æ•°:
            rag_system: åº•å±‚ RAG ç³»ç»Ÿ
            cache: ç»“æœç¼“å­˜
        """
        self.rag_system = rag_system
        self.cache = cache

    def query(self, question: str, use_cache: bool = True) -> dict:
        """
        æŸ¥è¯¢(å¸¦ç¼“å­˜)

        å‚æ•°:
            question: é—®é¢˜
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜

        è¿”å›:
            å›ç­”ç»“æœ
        """
        # 1. å°è¯•ä»ç¼“å­˜è·å–
        if use_cache:
            cached_result = self.cache.get(question)
            if cached_result:
                print("âœ“ ç¼“å­˜å‘½ä¸­")
                cached_result["from_cache"] = True
                return cached_result

        # 2. æ‰§è¡Œå®é™…æŸ¥è¯¢
        print("âœ— ç¼“å­˜æœªå‘½ä¸­,æ‰§è¡ŒæŸ¥è¯¢...")

        start = time.time()

        # æ£€ç´¢
        documents = self.rag_system.retrieval_system.search(question)

        # ç”Ÿæˆ
        result = self.rag_system.generation_system.generate(question, documents)

        result["query_time"] = time.time() - start
        result["from_cache"] = False

        # 3. ç¼“å­˜ç»“æœ
        if use_cache:
            self.cache.set(question, result)

        return result

# ===== ä½¿ç”¨ç¤ºä¾‹ =====
if __name__ == "__main__":
    # å‡è®¾å·²æœ‰ RAG ç³»ç»Ÿ
    # rag_system = ...

    # åˆ›å»ºç¼“å­˜
    result_cache = ResultCache(ttl=3600)

    # åˆ›å»ºå¸¦ç¼“å­˜çš„ RAG ç³»ç»Ÿ
    # cached_rag = CachedRAGSystem(rag_system, result_cache)

    # æµ‹è¯•
    questions = [
        "What is machine learning?",
        "Explain deep learning",
        "What is machine learning?",  # é‡å¤,åº”è¯¥å‘½ä¸­ç¼“å­˜
    ]

    for question in questions:
        print(f"\né—®é¢˜: {question}")
        # result = cached_rag.query(question)
        # print(f"å›ç­”: {result['answer'][:100]}...")
        # print(f"æ¥æºç¼“å­˜: {result['from_cache']}")

    # ç»Ÿè®¡
    print(f"\nç¼“å­˜ç»Ÿè®¡: {result_cache.get_stats()}")
```

### 1.4 å¼‚æ­¥ä¼˜åŒ–

```python
import asyncio
from typing import List
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# ===== åŸºç¡€å¼‚æ­¥è°ƒç”¨ =====
async def async_llm_call():
    """å¼‚æ­¥ LLM è°ƒç”¨"""
    llm = ChatOpenAI(model="gpt-3.5-turbo")

    # å¼‚æ­¥è°ƒç”¨
    response = await llm.ainvoke([
        HumanMessage(content="What is Python?")
    ])

    return response.content

# ===== å¹¶å‘è°ƒç”¨å¤šä¸ª LLM =====
async def concurrent_llm_calls(questions: List[str]):
    """å¹¶å‘è°ƒç”¨å¤šä¸ª LLM"""
    llm = ChatOpenAI(model="gpt-3.5-turbo")

    # åˆ›å»ºä»»åŠ¡
    tasks = [
        llm.ainvoke([HumanMessage(content=q)])
        for q in questions
    ]

    # å¹¶å‘æ‰§è¡Œ
    responses = await asyncio.gather(*tasks)

    return [r.content for r in responses]

# ===== å¼‚æ­¥ RAG ç³»ç»Ÿ =====
class AsyncRAGSystem:
    """å¼‚æ­¥ RAG ç³»ç»Ÿ"""

    def __init__(self, retrieval_system, generation_system):
        self.retrieval_system = retrieval_system
        self.generation_system = generation_system

    async def aquery(self, question: str) -> dict:
        """
        å¼‚æ­¥æŸ¥è¯¢

        å‚æ•°:
            question: é—®é¢˜

        è¿”å›:
            å›ç­”ç»“æœ
        """
        import time

        start = time.time()

        # 1. å¼‚æ­¥æ£€ç´¢
        # æ³¨æ„: å¦‚æœæ£€ç´¢ç³»ç»Ÿæ”¯æŒå¼‚æ­¥,ä½¿ç”¨ await
        documents = self.retrieval_system.search(question)

        # 2. å¼‚æ­¥ç”Ÿæˆ
        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
        llm = ChatOpenAI(model="gpt-3.5-turbo")

        context = "\n\n".join([doc.page_content for doc in documents])
        prompt = f"åŸºäºä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜:\n\n{context}\n\né—®é¢˜: {question}\n\nå›ç­”:"

        response = await llm.ainvoke([HumanMessage(content=prompt)])

        return {
            "answer": response.content,
            "sources": [doc.metadata.get("source") for doc in documents],
            "query_time": time.time() - start
        }

    async def abatch_query(self, questions: List[str]) -> List[dict]:
        """
        æ‰¹é‡å¼‚æ­¥æŸ¥è¯¢

        å‚æ•°:
            questions: é—®é¢˜åˆ—è¡¨

        è¿”å›:
            ç»“æœåˆ—è¡¨
        """
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æŸ¥è¯¢
        tasks = [self.aquery(q) for q in questions]
        results = await asyncio.gather(*tasks)

        return results

# ===== æ€§èƒ½å¯¹æ¯” =====
async def benchmark_async():
    """å¯¹æ¯”åŒæ­¥å’Œå¼‚æ­¥æ€§èƒ½"""
    import time

    questions = [
        "What is Python?",
        "What is JavaScript?",
        "What is Java?",
        "What is Go?",
        "What is Rust?"
    ]

    llm = ChatOpenAI(model="gpt-3.5-turbo")

    # æµ‹è¯•1: åŒæ­¥è°ƒç”¨
    print("æµ‹è¯•1: åŒæ­¥è°ƒç”¨")
    start = time.time()

    for q in questions:
        response = llm.invoke([HumanMessage(content=q)])

    sync_time = time.time() - start
    print(f"è€—æ—¶: {sync_time:.2f}ç§’\n")

    # æµ‹è¯•2: å¼‚æ­¥å¹¶å‘
    print("æµ‹è¯•2: å¼‚æ­¥å¹¶å‘")
    start = time.time()

    results = await concurrent_llm_calls(questions)

    async_time = time.time() - start
    print(f"è€—æ—¶: {async_time:.2f}ç§’\n")

    # æ€§èƒ½æå‡
    print("="*60)
    print(f"æ€§èƒ½æå‡: {sync_time/async_time:.1f}x")
    print("="*60)

# ===== è¿è¡Œç¤ºä¾‹ =====
if __name__ == "__main__":
    # è¿è¡Œå¼‚æ­¥å‡½æ•°
    asyncio.run(benchmark_async())
```

## äºŒã€æˆæœ¬æ§åˆ¶

### 2.1 Token ä¼˜åŒ–

```python
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from typing import List
import tiktoken

class TokenOptimizer:
    """Token ä¼˜åŒ–å™¨"""

    def __init__(self, model: str = "gpt-3.5-turbo"):
        """
        åˆå§‹åŒ–

        å‚æ•°:
            model: æ¨¡å‹åç§°
        """
        self.model = model
        self.encoding = tiktoken.encoding_for_model(model)

        # Token é™åˆ¶
        self.model_limits = {
            "gpt-3.5-turbo": 4096,
            "gpt-3.5-turbo-16k": 16384,
            "gpt-4": 8192,
            "gpt-4-32k": 32768,
            "gpt-4-turbo": 128000
        }

    def count_tokens(self, text: str) -> int:
        """
        è®¡ç®— token æ•°é‡

        å‚æ•°:
            text: æ–‡æœ¬

        è¿”å›:
            token æ•°é‡
        """
        return len(self.encoding.encode(text))

    def truncate_text(
        self,
        text: str,
        max_tokens: int,
        from_end: bool = False
    ) -> str:
        """
        æˆªæ–­æ–‡æœ¬åˆ°æŒ‡å®š token æ•°é‡

        å‚æ•°:
            text: æ–‡æœ¬
            max_tokens: æœ€å¤§ token æ•°
            from_end: æ˜¯å¦ä»æœ«å°¾æˆªæ–­

        è¿”å›:
            æˆªæ–­åçš„æ–‡æœ¬
        """
        tokens = self.encoding.encode(text)

        if len(tokens) <= max_tokens:
            return text

        if from_end:
            # ä¿ç•™å¼€å¤´
            truncated_tokens = tokens[:max_tokens]
        else:
            # ä¿ç•™æœ«å°¾
            truncated_tokens = tokens[-max_tokens:]

        return self.encoding.decode(truncated_tokens)

    def optimize_context(
        self,
        query: str,
        documents: List[str],
        max_context_tokens: int = 2000
    ) -> str:
        """
        ä¼˜åŒ–ä¸Šä¸‹æ–‡é•¿åº¦

        å‚æ•°:
            query: æŸ¥è¯¢
            documents: æ–‡æ¡£åˆ—è¡¨
            max_context_tokens: æœ€å¤§ä¸Šä¸‹æ–‡ token æ•°

        è¿”å›:
            ä¼˜åŒ–åçš„ä¸Šä¸‹æ–‡
        """
        # è®¡ç®—æŸ¥è¯¢çš„ token
        query_tokens = self.count_tokens(query)

        # å‰©ä½™ token é¢„ç®—
        remaining_tokens = max_context_tokens - query_tokens

        # é€ä¸ªæ·»åŠ æ–‡æ¡£,ç›´åˆ°è¾¾åˆ°é™åˆ¶
        context_parts = []
        current_tokens = 0

        for doc in documents:
            doc_tokens = self.count_tokens(doc)

            if current_tokens + doc_tokens <= remaining_tokens:
                context_parts.append(doc)
                current_tokens += doc_tokens
            else:
                # éƒ¨åˆ†æ·»åŠ æœ€åä¸€ä¸ªæ–‡æ¡£
                remaining = remaining_tokens - current_tokens
                if remaining > 100:  # è‡³å°‘ä¿ç•™100 token
                    truncated = self.truncate_text(doc, remaining)
                    context_parts.append(truncated)
                break

        return "\n\n".join(context_parts)

    def estimate_cost(
        self,
        prompt_tokens: int,
        completion_tokens: int
    ) -> float:
        """
        ä¼°ç®—æˆæœ¬

        å‚æ•°:
            prompt_tokens: æç¤ºè¯ token æ•°
            completion_tokens: å®Œæˆ token æ•°

        è¿”å›:
            æˆæœ¬(ç¾å…ƒ)
        """
        # å®šä»·(2024å¹´ä»·æ ¼,éœ€è¦æ›´æ–°)
        pricing = {
            "gpt-3.5-turbo": {
                "prompt": 0.0015 / 1000,
                "completion": 0.002 / 1000
            },
            "gpt-4": {
                "prompt": 0.03 / 1000,
                "completion": 0.06 / 1000
            },
            "gpt-4-turbo": {
                "prompt": 0.01 / 1000,
                "completion": 0.03 / 1000
            }
        }

        model_pricing = pricing.get(self.model, pricing["gpt-3.5-turbo"])

        cost = (
            prompt_tokens * model_pricing["prompt"] +
            completion_tokens * model_pricing["completion"]
        )

        return cost

# ===== ä½¿ç”¨ç¤ºä¾‹ =====
if __name__ == "__main__":
    optimizer = TokenOptimizer(model="gpt-3.5-turbo")

    # è®¡ç®— token
    text = "LangChain is a framework for developing LLM applications."
    token_count = optimizer.count_tokens(text)
    print(f"Text: {text}")
    print(f"Token count: {token_count}\n")

    # æˆªæ–­æ–‡æœ¬
    long_text = "This is a very long text " * 100
    truncated = optimizer.truncate_text(long_text, max_tokens=50)
    print(f"Original tokens: {optimizer.count_tokens(long_text)}")
    print(f"Truncated tokens: {optimizer.count_tokens(truncated)}\n")

    # ä¼˜åŒ–ä¸Šä¸‹æ–‡
    query = "What is machine learning?"
    documents = [
        "Machine learning is a subset of AI " * 50,
        "Deep learning uses neural networks " * 50,
        "NLP processes human language " * 50
    ]

    optimized_context = optimizer.optimize_context(query, documents, max_context_tokens=500)
    print(f"Optimized context tokens: {optimizer.count_tokens(optimized_context)}\n")

    # ä¼°ç®—æˆæœ¬
    cost = optimizer.estimate_cost(prompt_tokens=1000, completion_tokens=500)
    print(f"Estimated cost: ${cost:.4f}")
```

### 2.2 æ¨¡å‹é€‰æ‹©ç­–ç•¥

```python
from langchain_openai import ChatOpenAI
from typing import Optional, Dict
from enum import Enum

class TaskComplexity(Enum):
    """ä»»åŠ¡å¤æ‚åº¦"""
    SIMPLE = "simple"        # ç®€å•ä»»åŠ¡
    MODERATE = "moderate"    # ä¸­ç­‰ä»»åŠ¡
    COMPLEX = "complex"      # å¤æ‚ä»»åŠ¡

class ModelSelector:
    """æ™ºèƒ½æ¨¡å‹é€‰æ‹©å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–æ¨¡å‹é€‰æ‹©å™¨"""
        # æ¨¡å‹é…ç½®
        self.models = {
            "gpt-3.5-turbo": {
                "cost_per_1k_tokens": 0.002,
                "speed": "fast",
                "quality": "good",
                "complexity": [TaskComplexity.SIMPLE, TaskComplexity.MODERATE]
            },
            "gpt-4": {
                "cost_per_1k_tokens": 0.06,
                "speed": "slow",
                "quality": "excellent",
                "complexity": [TaskComplexity.COMPLEX]
            },
            "gpt-4-turbo": {
                "cost_per_1k_tokens": 0.03,
                "speed": "moderate",
                "quality": "excellent",
                "complexity": [TaskComplexity.MODERATE, TaskComplexity.COMPLEX]
            }
        }

    def select_model(
        self,
        task_complexity: TaskComplexity,
        priority: str = "cost"  # cost, speed, quality
    ) -> str:
        """
        æ ¹æ®ä»»åŠ¡é€‰æ‹©æ¨¡å‹

        å‚æ•°:
            task_complexity: ä»»åŠ¡å¤æ‚åº¦
            priority: ä¼˜å…ˆçº§(cost/speed/quality)

        è¿”å›:
            æ¨¡å‹åç§°
        """
        # ç­›é€‰é€‚åˆçš„æ¨¡å‹
        suitable_models = [
            name for name, config in self.models.items()
            if task_complexity in config["complexity"]
        ]

        if not suitable_models:
            return "gpt-3.5-turbo"  # é»˜è®¤æ¨¡å‹

        # æ ¹æ®ä¼˜å…ˆçº§é€‰æ‹©
        if priority == "cost":
            # é€‰æ‹©æœ€ä¾¿å®œçš„
            return min(
                suitable_models,
                key=lambda m: self.models[m]["cost_per_1k_tokens"]
            )
        elif priority == "speed":
            # é€‰æ‹©æœ€å¿«çš„
            speed_order = {"fast": 0, "moderate": 1, "slow": 2}
            return min(
                suitable_models,
                key=lambda m: speed_order[self.models[m]["speed"]]
            )
        elif priority == "quality":
            # é€‰æ‹©è´¨é‡æœ€å¥½çš„
            quality_order = {"good": 0, "excellent": 1}
            return max(
                suitable_models,
                key=lambda m: quality_order[self.models[m]["quality"]]
            )
        else:
            return suitable_models[0]

    def get_model_recommendation(self, query: str) -> Dict:
        """
        æ ¹æ®æŸ¥è¯¢æ¨èæ¨¡å‹

        å‚æ•°:
            query: ç”¨æˆ·æŸ¥è¯¢

        è¿”å›:
            æ¨èä¿¡æ¯
        """
        # ç®€å•çš„å¤æ‚åº¦åˆ¤æ–­(å®é™…å¯ä»¥æ›´å¤æ‚)
        query_length = len(query.split())

        if query_length < 10:
            complexity = TaskComplexity.SIMPLE
        elif query_length < 30:
            complexity = TaskComplexity.MODERATE
        else:
            complexity = TaskComplexity.COMPLEX

        # æ¨èæ¨¡å‹
        recommended_model = self.select_model(complexity, priority="cost")

        return {
            "query_length": query_length,
            "complexity": complexity.value,
            "recommended_model": recommended_model,
            "alternatives": {
                "cost_optimized": self.select_model(complexity, "cost"),
                "speed_optimized": self.select_model(complexity, "speed"),
                "quality_optimized": self.select_model(complexity, "quality")
            }
        }

# ===== ä½¿ç”¨ç¤ºä¾‹ =====
if __name__ == "__main__":
    selector = ModelSelector()

    # æµ‹è¯•ä¸åŒå¤æ‚åº¦çš„æŸ¥è¯¢
    queries = [
        "Hello",  # ç®€å•
        "Explain the difference between Python and JavaScript",  # ä¸­ç­‰
        "Provide a detailed analysis of the economic impacts of artificial intelligence on global labor markets, including both positive and negative effects, with specific examples from different industries and countries"  # å¤æ‚
    ]

    for query in queries:
        print(f"\næŸ¥è¯¢: {query}")
        recommendation = selector.get_model_recommendation(query)
        print(f"å¤æ‚åº¦: {recommendation['complexity']}")
        print(f"æ¨èæ¨¡å‹: {recommendation['recommended_model']}")
        print(f"å¤‡é€‰æ–¹æ¡ˆ:")
        for key, model in recommendation['alternatives'].items():
            print(f"  {key}: {model}")
```

## ä¸‰ã€é”™è¯¯å¤„ç†

### 3.1 é‡è¯•æœºåˆ¶

```python
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
    before_sleep_log
)
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage
import logging
import time

logger = logging.getLogger(__name__)

# ===== åŸºç¡€é‡è¯• =====
@retry(
    stop=stop_after_attempt(3),  # æœ€å¤šé‡è¯•3æ¬¡
    wait=wait_exponential(multiplier=1, min=2, max=10),  # æŒ‡æ•°é€€é¿
    before_sleep=before_sleep_log(logger, logging.WARNING)  # è®°å½•æ—¥å¿—
)
def call_llm_with_retry(prompt: str) -> str:
    """
    å¸¦é‡è¯•çš„ LLM è°ƒç”¨

    å‚æ•°:
        prompt: æç¤ºè¯

    è¿”å›:
        LLM å“åº”
    """
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    response = llm.invoke([HumanMessage(content=prompt)])
    return response.content

# ===== é’ˆå¯¹ç‰¹å®šå¼‚å¸¸é‡è¯• =====
@retry(
    stop=stop_after_attempt(5),
    wait=wait_exponential(multiplier=2, min=4, max=60),
    retry=retry_if_exception_type((ConnectionError, TimeoutError)),  # åªå¯¹ç‰¹å®šå¼‚å¸¸é‡è¯•
    before_sleep=before_sleep_log(logger, logging.ERROR)
)
def call_llm_specific_retry(prompt: str) -> str:
    """é’ˆå¯¹ç‰¹å®šå¼‚å¸¸é‡è¯•"""
    llm = ChatOpenAI(model="gpt-3.5-turbo", request_timeout=30)
    response = llm.invoke([HumanMessage(content=prompt)])
    return response.content

# ===== è‡ªå®šä¹‰é‡è¯•é€»è¾‘ =====
class SmartRetryHandler:
    """æ™ºèƒ½é‡è¯•å¤„ç†å™¨"""

    def __init__(
        self,
        max_retries: int = 3,
        base_delay: float = 1.0,
        max_delay: float = 60.0
    ):
        """
        åˆå§‹åŒ–

        å‚æ•°:
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            base_delay: åŸºç¡€å»¶è¿Ÿ(ç§’)
            max_delay: æœ€å¤§å»¶è¿Ÿ(ç§’)
        """
        self.max_retries = max_retries
        self.base_delay = base_delay
        self.max_delay = max_delay

        # ç»Ÿè®¡
        self.retry_count = 0
        self.success_count = 0
        self.failure_count = 0

    def call_with_retry(self, func, *args, **kwargs):
        """
        å¸¦é‡è¯•çš„å‡½æ•°è°ƒç”¨

        å‚æ•°:
            func: è¦è°ƒç”¨çš„å‡½æ•°
            *args, **kwargs: å‡½æ•°å‚æ•°

        è¿”å›:
            å‡½æ•°è¿”å›å€¼
        """
        last_exception = None

        for attempt in range(self.max_retries + 1):
            try:
                # å°è¯•è°ƒç”¨
                result = func(*args, **kwargs)

                # æˆåŠŸ
                if attempt > 0:
                    logger.info(f"âœ“ é‡è¯•æˆåŠŸ (å°è¯• {attempt + 1}/{self.max_retries + 1})")
                    self.retry_count += attempt

                self.success_count += 1
                return result

            except Exception as e:
                last_exception = e

                # æœ€åä¸€æ¬¡å°è¯•,ç›´æ¥æŠ›å‡ºå¼‚å¸¸
                if attempt == self.max_retries:
                    logger.error(f"âœ— é‡è¯•å¤±è´¥,å·²è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•° ({self.max_retries + 1})")
                    self.failure_count += 1
                    raise

                # è®¡ç®—å»¶è¿Ÿ(æŒ‡æ•°é€€é¿)
                delay = min(
                    self.base_delay * (2 ** attempt),
                    self.max_delay
                )

                logger.warning(
                    f"âœ— å°è¯• {attempt + 1} å¤±è´¥: {str(e)}, "
                    f"{delay:.1f}ç§’åé‡è¯•..."
                )

                time.sleep(delay)

        # ç†è®ºä¸Šä¸åº”è¯¥åˆ°è¿™é‡Œ
        raise last_exception

    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        total = self.success_count + self.failure_count
        success_rate = (self.success_count / total * 100) if total > 0 else 0

        return {
            "total_calls": total,
            "success_count": self.success_count,
            "failure_count": self.failure_count,
            "retry_count": self.retry_count,
            "success_rate": f"{success_rate:.2f}%",
            "average_retries": self.retry_count / self.success_count if self.success_count > 0 else 0
        }

# ===== ä½¿ç”¨ç¤ºä¾‹ =====
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    # æµ‹è¯•åŸºç¡€é‡è¯•
    print("æµ‹è¯•1: åŸºç¡€é‡è¯•")
    try:
        result = call_llm_with_retry("What is Python?")
        print(f"æˆåŠŸ: {result[:100]}...\n")
    except Exception as e:
        print(f"å¤±è´¥: {e}\n")

    # æµ‹è¯•æ™ºèƒ½é‡è¯•
    print("æµ‹è¯•2: æ™ºèƒ½é‡è¯•")
    handler = SmartRetryHandler(max_retries=3)

    llm = ChatOpenAI(model="gpt-3.5-turbo")

    def llm_call(prompt):
        return llm.invoke([HumanMessage(content=prompt)]).content

    try:
        result = handler.call_with_retry(llm_call, "Explain machine learning")
        print(f"æˆåŠŸ: {result[:100]}...")
    except Exception as e:
        print(f"å¤±è´¥: {e}")

    # ç»Ÿè®¡
    print(f"\nç»Ÿè®¡: {handler.get_stats()}")
```

### 3.2 é™çº§ç­–ç•¥

```python
from typing import Optional, Callable
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage
import logging

logger = logging.getLogger(__name__)

class GracefulDegradation:
    """ä¼˜é›…é™çº§ç³»ç»Ÿ"""

    def __init__(self):
        """åˆå§‹åŒ–é™çº§ç³»ç»Ÿ"""
        # æ¨¡å‹å±‚æ¬¡(ä»é«˜åˆ°ä½)
        self.model_hierarchy = [
            "gpt-4-turbo",
            "gpt-4",
            "gpt-3.5-turbo-16k",
            "gpt-3.5-turbo"
        ]

        # é™çº§è®¡æ•°
        self.degradation_count = {model: 0 for model in self.model_hierarchy}

    def call_with_fallback(
        self,
        prompt: str,
        preferred_model: str = "gpt-4",
        fallback_response: Optional[str] = None
    ) -> dict:
        """
        å¸¦é™çº§çš„è°ƒç”¨

        å‚æ•°:
            prompt: æç¤ºè¯
            preferred_model: é¦–é€‰æ¨¡å‹
            fallback_response: å…œåº•å“åº”

        è¿”å›:
            å“åº”å­—å…¸
        """
        # æ‰¾åˆ°é¦–é€‰æ¨¡å‹åœ¨å±‚æ¬¡ä¸­çš„ä½ç½®
        try:
            start_index = self.model_hierarchy.index(preferred_model)
        except ValueError:
            start_index = 0

        # ä¾æ¬¡å°è¯•æ¨¡å‹
        for model in self.model_hierarchy[start_index:]:
            try:
                logger.info(f"å°è¯•æ¨¡å‹: {model}")

                llm = ChatOpenAI(model=model, request_timeout=30)
                response = llm.invoke([HumanMessage(content=prompt)])

                return {
                    "answer": response.content,
                    "model": model,
                    "degraded": model != preferred_model,
                    "success": True
                }

            except Exception as e:
                logger.warning(f"æ¨¡å‹ {model} å¤±è´¥: {str(e)}")
                self.degradation_count[model] += 1
                continue

        # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥,è¿”å›å…œåº•å“åº”
        logger.error("æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥,ä½¿ç”¨å…œåº•å“åº”")

        return {
            "answer": fallback_response or "æŠ±æ­‰,æœåŠ¡æš‚æ—¶ä¸å¯ç”¨,è¯·ç¨åå†è¯•ã€‚",
            "model": None,
            "degraded": True,
            "success": False
        }

    def get_degradation_stats(self) -> dict:
        """è·å–é™çº§ç»Ÿè®¡"""
        return {
            "degradation_count": self.degradation_count,
            "total_degradations": sum(self.degradation_count.values())
        }

# ===== ä½¿ç”¨ç¤ºä¾‹ =====
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    degradation = GracefulDegradation()

    # æµ‹è¯•
    prompt = "What is machine learning?"

    result = degradation.call_with_fallback(
        prompt,
        preferred_model="gpt-4",
        fallback_response="è¯·è®¿é—®æˆ‘ä»¬çš„æ–‡æ¡£è·å–æ›´å¤šä¿¡æ¯ã€‚"
    )

    print(f"\nå›ç­”: {result['answer'][:100]}...")
    print(f"ä½¿ç”¨æ¨¡å‹: {result['model']}")
    print(f"æ˜¯å¦é™çº§: {result['degraded']}")
    print(f"æ˜¯å¦æˆåŠŸ: {result['success']}")

    # ç»Ÿè®¡
    print(f"\né™çº§ç»Ÿè®¡: {degradation.get_degradation_stats()}")
```

## å››ã€å®‰å…¨é˜²æŠ¤

### 4.1 è¾“å…¥éªŒè¯

```python
import re
from typing import Optional, List
from pydantic import BaseModel, validator, Field

class InputValidator:
    """è¾“å…¥éªŒè¯å™¨"""

    def __init__(
        self,
        max_length: int = 5000,
        min_length: int = 1,
        blocked_patterns: Optional[List[str]] = None
    ):
        """
        åˆå§‹åŒ–éªŒè¯å™¨

        å‚æ•°:
            max_length: æœ€å¤§é•¿åº¦
            min_length: æœ€å°é•¿åº¦
            blocked_patterns: é˜»æ­¢çš„æ¨¡å¼åˆ—è¡¨
        """
        self.max_length = max_length
        self.min_length = min_length

        # é»˜è®¤é˜»æ­¢çš„æ¨¡å¼
        self.blocked_patterns = blocked_patterns or [
            r"<script>",  # XSS
            r"javascript:",  # XSS
            r"on\w+\s*=",  # äº‹ä»¶å¤„ç†
            r"DROP\s+TABLE",  # SQLæ³¨å…¥
            r"DELETE\s+FROM",  # SQLæ³¨å…¥
        ]

    def validate(self, text: str) -> tuple:
        """
        éªŒè¯è¾“å…¥

        å‚æ•°:
            text: è¾“å…¥æ–‡æœ¬

        è¿”å›:
            (is_valid, error_message)
        """
        # 1. æ£€æŸ¥é•¿åº¦
        if len(text) < self.min_length:
            return False, f"è¾“å…¥å¤ªçŸ­(æœ€å°‘ {self.min_length} å­—ç¬¦)"

        if len(text) > self.max_length:
            return False, f"è¾“å…¥å¤ªé•¿(æœ€å¤š {self.max_length} å­—ç¬¦)"

        # 2. æ£€æŸ¥å±é™©æ¨¡å¼
        for pattern in self.blocked_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                return False, f"è¾“å…¥åŒ…å«éæ³•å†…å®¹"

        # 3. æ£€æŸ¥æ˜¯å¦ä¸ºç©ºç™½
        if not text.strip():
            return False, "è¾“å…¥ä¸èƒ½ä¸ºç©ºç™½"

        return True, None

    def sanitize(self, text: str) -> str:
        """
        æ¸…ç†è¾“å…¥

        å‚æ•°:
            text: è¾“å…¥æ–‡æœ¬

        è¿”å›:
            æ¸…ç†åçš„æ–‡æœ¬
        """
        # ç§»é™¤ HTML æ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)

        # ç§»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text).strip()

        # æˆªæ–­åˆ°æœ€å¤§é•¿åº¦
        if len(text) > self.max_length:
            text = text[:self.max_length]

        return text

class SafeQueryRequest(BaseModel):
    """å®‰å…¨çš„æŸ¥è¯¢è¯·æ±‚æ¨¡å‹"""

    query: str = Field(..., description="ç”¨æˆ·æŸ¥è¯¢")
    session_id: Optional[str] = Field(None, description="ä¼šè¯ID")
    k: int = Field(5, ge=1, le=20, description="æ£€ç´¢æ•°é‡")

    @validator('query')
    def validate_query(cls, v):
        """éªŒè¯æŸ¥è¯¢"""
        validator = InputValidator()
        is_valid, error = validator.validate(v)

        if not is_valid:
            raise ValueError(error)

        # æ¸…ç†è¾“å…¥
        return validator.sanitize(v)

    @validator('session_id')
    def validate_session_id(cls, v):
        """éªŒè¯ä¼šè¯ID"""
        if v is None:
            return v

        # ä¼šè¯IDåº”è¯¥æ˜¯ UUID æ ¼å¼
        if not re.match(r'^[a-f0-9\-]{36}$', v, re.IGNORECASE):
            raise ValueError("æ— æ•ˆçš„ä¼šè¯IDæ ¼å¼")

        return v

# ===== ä½¿ç”¨ç¤ºä¾‹ =====
if __name__ == "__main__":
    # æµ‹è¯•éªŒè¯å™¨
    validator = InputValidator()

    test_inputs = [
        "What is Python?",  # æ­£å¸¸
        "",  # ç©ºç™½
        "x",  # å¤ªçŸ­
        "<script>alert('xss')</script>",  # XSS
        "DROP TABLE users",  # SQLæ³¨å…¥
    ]

    print("è¾“å…¥éªŒè¯æµ‹è¯•:")
    for text in test_inputs:
        is_valid, error = validator.validate(text)
        print(f"\nè¾“å…¥: {text[:50]}...")
        print(f"æœ‰æ•ˆ: {is_valid}")
        if not is_valid:
            print(f"é”™è¯¯: {error}")

    # æµ‹è¯• Pydantic æ¨¡å‹
    print("\n\nPydantic æ¨¡å‹æµ‹è¯•:")

    try:
        request = SafeQueryRequest(
            query="What is machine learning?",
            k=5
        )
        print(f"âœ“ æœ‰æ•ˆè¯·æ±‚: {request.query}")
    except ValueError as e:
        print(f"âœ— æ— æ•ˆè¯·æ±‚: {e}")

    try:
        request = SafeQueryRequest(
            query="<script>alert('xss')</script>",
            k=5
        )
        print(f"âœ“ æœ‰æ•ˆè¯·æ±‚: {request.query}")
    except ValueError as e:
        print(f"âœ— æ— æ•ˆè¯·æ±‚: {e}")
```

### 4.2 é€Ÿç‡é™åˆ¶

```python
from collections import defaultdict
from datetime import datetime, timedelta
from typing import Optional
import time

class RateLimiter:
    """é€Ÿç‡é™åˆ¶å™¨"""

    def __init__(
        self,
        max_requests: int = 100,
        window_seconds: int = 60
    ):
        """
        åˆå§‹åŒ–é€Ÿç‡é™åˆ¶å™¨

        å‚æ•°:
            max_requests: æ—¶é—´çª—å£å†…æœ€å¤§è¯·æ±‚æ•°
            window_seconds: æ—¶é—´çª—å£(ç§’)
        """
        self.max_requests = max_requests
        self.window_seconds = window_seconds

        # å­˜å‚¨æ¯ä¸ªç”¨æˆ·çš„è¯·æ±‚æ—¶é—´
        self.requests = defaultdict(list)

    def is_allowed(self, user_id: str) -> tuple:
        """
        æ£€æŸ¥æ˜¯å¦å…è®¸è¯·æ±‚

        å‚æ•°:
            user_id: ç”¨æˆ·ID

        è¿”å›:
            (is_allowed, retry_after_seconds)
        """
        now = datetime.now()

        # è·å–ç”¨æˆ·çš„è¯·æ±‚å†å²
        user_requests = self.requests[user_id]

        # ç§»é™¤è¿‡æœŸçš„è¯·æ±‚
        cutoff = now - timedelta(seconds=self.window_seconds)
        user_requests[:] = [req_time for req_time in user_requests if req_time > cutoff]

        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
        if len(user_requests) >= self.max_requests:
            # è®¡ç®—éœ€è¦ç­‰å¾…çš„æ—¶é—´
            oldest_request = min(user_requests)
            retry_after = (oldest_request + timedelta(seconds=self.window_seconds) - now).total_seconds()

            return False, retry_after

        # è®°å½•å½“å‰è¯·æ±‚
        user_requests.append(now)

        return True, 0

    def get_remaining(self, user_id: str) -> int:
        """è·å–å‰©ä½™è¯·æ±‚æ•°"""
        now = datetime.now()
        cutoff = now - timedelta(seconds=self.window_seconds)

        user_requests = self.requests[user_id]
        user_requests[:] = [req_time for req_time in user_requests if req_time > cutoff]

        return max(0, self.max_requests - len(user_requests))

# ===== è£…é¥°å™¨ç‰ˆæœ¬ =====
def rate_limit(max_requests: int = 100, window_seconds: int = 60):
    """é€Ÿç‡é™åˆ¶è£…é¥°å™¨"""
    limiter = RateLimiter(max_requests, window_seconds)

    def decorator(func):
        def wrapper(user_id: str, *args, **kwargs):
            is_allowed, retry_after = limiter.is_allowed(user_id)

            if not is_allowed:
                raise Exception(f"é€Ÿç‡é™åˆ¶: è¯·åœ¨ {retry_after:.1f}ç§’ åé‡è¯•")

            return func(user_id, *args, **kwargs)

        return wrapper

    return decorator

# ===== FastAPI ä¸­é—´ä»¶ç‰ˆæœ¬ =====
from fastapi import Request, HTTPException
from starlette.middleware.base import BaseHTTPMiddleware

class RateLimitMiddleware(BaseHTTPMiddleware):
    """é€Ÿç‡é™åˆ¶ä¸­é—´ä»¶"""

    def __init__(self, app, max_requests: int = 100, window_seconds: int = 60):
        super().__init__(app)
        self.limiter = RateLimiter(max_requests, window_seconds)

    async def dispatch(self, request: Request, call_next):
        # è·å–ç”¨æˆ·ID(å¯ä»¥ä» headerã€token ç­‰è·å–)
        user_id = request.headers.get("X-User-ID", request.client.host)

        # æ£€æŸ¥é€Ÿç‡é™åˆ¶
        is_allowed, retry_after = self.limiter.is_allowed(user_id)

        if not is_allowed:
            raise HTTPException(
                status_code=429,
                detail=f"Too Many Requests. Retry after {retry_after:.1f}s",
                headers={"Retry-After": str(int(retry_after))}
            )

        # æ·»åŠ é€Ÿç‡é™åˆ¶ä¿¡æ¯åˆ°å“åº”å¤´
        response = await call_next(request)
        response.headers["X-RateLimit-Limit"] = str(self.limiter.max_requests)
        response.headers["X-RateLimit-Remaining"] = str(self.limiter.get_remaining(user_id))
        response.headers["X-RateLimit-Reset"] = str(int(time.time()) + self.limiter.window_seconds)

        return response

# ===== ä½¿ç”¨ç¤ºä¾‹ =====
if __name__ == "__main__":
    # æµ‹è¯•é€Ÿç‡é™åˆ¶å™¨
    limiter = RateLimiter(max_requests=5, window_seconds=10)

    user_id = "user_123"

    print("é€Ÿç‡é™åˆ¶æµ‹è¯•:")
    for i in range(10):
        is_allowed, retry_after = limiter.is_allowed(user_id)

        if is_allowed:
            print(f"è¯·æ±‚ {i+1}: âœ“ å…è®¸ (å‰©ä½™: {limiter.get_remaining(user_id)})")
        else:
            print(f"è¯·æ±‚ {i+1}: âœ— é™åˆ¶ ({retry_after:.1f}ç§’åé‡è¯•)")

        time.sleep(1)

    # æµ‹è¯•è£…é¥°å™¨
    @rate_limit(max_requests=3, window_seconds=5)
    def api_call(user_id: str, message: str):
        return f"å¤„ç†: {message}"

    print("\n\nè£…é¥°å™¨æµ‹è¯•:")
    for i in range(5):
        try:
            result = api_call("user_456", f"æ¶ˆæ¯ {i+1}")
            print(f"âœ“ {result}")
        except Exception as e:
            print(f"âœ— {e}")

        time.sleep(1)
```

## ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. **ç¼“å­˜å®ç°**: å®ç°ä¸€ä¸ªç®€å•çš„ LRU ç¼“å­˜
2. **å¼‚æ­¥è°ƒç”¨**: å®ç°å¼‚æ­¥æ‰¹é‡æŸ¥è¯¢
3. **Token ä¼˜åŒ–**: å®ç° token è®¡æ•°å’Œæˆªæ–­

### è¿›é˜¶ç»ƒä¹ 

4. **é‡è¯•æœºåˆ¶**: å®ç°å¸¦æŒ‡æ•°é€€é¿çš„é‡è¯•
5. **é™çº§ç­–ç•¥**: å®ç°å¤šæ¨¡å‹é™çº§ç³»ç»Ÿ
6. **é€Ÿç‡é™åˆ¶**: å®ç°åŸºäº Redis çš„åˆ†å¸ƒå¼é€Ÿç‡é™åˆ¶

### å®æˆ˜ç»ƒä¹ 

7. **å®Œæ•´ä¼˜åŒ–**: ä¸º RAG ç³»ç»Ÿæ·»åŠ æ‰€æœ‰ä¼˜åŒ–
8. **ç›‘æ§ç³»ç»Ÿ**: é›†æˆ Prometheus ç›‘æ§
9. **å‹åŠ›æµ‹è¯•**: è¿›è¡Œæ€§èƒ½æµ‹è¯•å’Œä¼˜åŒ–

## æœ¬å‘¨æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **æ€§èƒ½ä¼˜åŒ–**
   - ç¼“å­˜(Embeddingã€ç»“æœã€æ–‡æ¡£)
   - å¼‚æ­¥å¹¶å‘
   - æ‰¹é‡å¤„ç†
   - Token ä¼˜åŒ–

2. **æˆæœ¬æ§åˆ¶**
   - é€‰æ‹©åˆé€‚çš„æ¨¡å‹
   - ä¼˜åŒ– token ä½¿ç”¨
   - å®æ–½ç¼“å­˜ç­–ç•¥

3. **å¯é æ€§**
   - é‡è¯•æœºåˆ¶
   - é™çº§ç­–ç•¥
   - é”™è¯¯å¤„ç†

4. **å®‰å…¨æ€§**
   - è¾“å…¥éªŒè¯
   - é€Ÿç‡é™åˆ¶
   - æ•°æ®åŠ å¯†

### æœ€ä½³å®è·µ

1. **ç¼“å­˜ä¼˜å…ˆ**: èƒ½ç¼“å­˜çš„ä¸€å®šè¦ç¼“å­˜
2. **å¼‚æ­¥ä¼˜åŒ–**: èƒ½å¹¶å‘çš„ä¸è¦ä¸²è¡Œ
3. **é™çº§ä¿æŠ¤**: ä¿è¯æœåŠ¡å¯ç”¨æ€§
4. **ç›‘æ§å‘Šè­¦**: å®æ—¶ç›‘æ§ç³»ç»ŸçŠ¶æ€


**ç¥ä½ åœ¨ AI åº”ç”¨å¼€å‘çš„é“è·¯ä¸Šè¶Šèµ°è¶Šè¿œ!** ğŸš€
