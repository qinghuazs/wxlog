---
title: LangChainå®æˆ˜:ä¼ä¸šçº§RAGç³»ç»Ÿ
date: 2025-01-22
permalink: /ai/langchain/enterprise-rag-system.html
tags:
  - LangChain
  - RAG
categories:
  - LangChain
---

# ä¼ä¸šçº§RAGç³»ç»Ÿ

## é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®å°†æ•´åˆå‰é¢æ‰€æœ‰å­¦è¿‡çš„çŸ¥è¯†,æ„å»ºä¸€ä¸ªåŠŸèƒ½å®Œæ•´ã€å¯æŠ•å…¥ç”Ÿäº§ä½¿ç”¨çš„ä¼ä¸šçº§ RAG(æ£€ç´¢å¢å¼ºç”Ÿæˆ)ç³»ç»Ÿã€‚

### é¡¹ç›®ç›®æ ‡

æ„å»ºä¸€ä¸ªæ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ,æ”¯æŒ:
- ğŸ“„ å¤šæ ¼å¼æ–‡æ¡£(PDFã€Wordã€Markdownã€TXTç­‰)
- ğŸ” æ··åˆæœç´¢(è¯­ä¹‰ + å…³é”®è¯)
- ğŸ¯ æ™ºèƒ½é‡æ’åº
- ğŸ’¬ å¤šè½®å¯¹è¯
- ğŸ“Š ç›‘æ§å’Œæˆæœ¬è¿½è¸ª
- ğŸ”’ ç”¨æˆ·æƒé™ç®¡ç†
- ğŸš€ é«˜æ€§èƒ½å’Œå¯æ‰©å±•

### æŠ€æœ¯æ¶æ„

```mermaid
graph TB
    subgraph å‰ç«¯å±‚
        A[Webç•Œé¢] --> B[FastAPIåç«¯]
    end

    subgraph åº”ç”¨å±‚
        B --> C[RAGç³»ç»Ÿ]
        C --> D[æ–‡æ¡£å¤„ç†]
        C --> E[æ£€ç´¢ç³»ç»Ÿ]
        C --> F[ç”Ÿæˆç³»ç»Ÿ]
        C --> G[å¯¹è¯ç®¡ç†]
    end

    subgraph æ•°æ®å±‚
        D --> H[å‘é‡æ•°æ®åº“<br/>Chroma]
        D --> I[å…³ç³»æ•°æ®åº“<br/>PostgreSQL]
        E --> H
        G --> I
    end

    subgraph ç›‘æ§å±‚
        C --> J[Callbacks]
        J --> K[LangSmith]
        J --> L[Prometheus]
    end

    style C fill:#f96,stroke:#333,stroke-width:4px
    style H fill:#9f9,stroke:#333,stroke-width:2px
    style K fill:#bbf,stroke:#333,stroke-width:2px
```

## ä¸€ã€é¡¹ç›®ç»“æ„è®¾è®¡

### 1.1 ç›®å½•ç»“æ„

```
enterprise-rag-system/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # FastAPI åº”ç”¨å…¥å£
â”‚   â”œâ”€â”€ config.py               # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ dependencies.py         # ä¾èµ–æ³¨å…¥
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                   # æ ¸å¿ƒåŠŸèƒ½
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ document_processor.py   # æ–‡æ¡£å¤„ç†
â”‚   â”‚   â”œâ”€â”€ retrieval_system.py     # æ£€ç´¢ç³»ç»Ÿ
â”‚   â”‚   â”œâ”€â”€ generation_system.py    # ç”Ÿæˆç³»ç»Ÿ
â”‚   â”‚   â”œâ”€â”€ conversation_manager.py # å¯¹è¯ç®¡ç†
â”‚   â”‚   â””â”€â”€ monitoring.py           # ç›‘æ§ç³»ç»Ÿ
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                    # API è·¯ç”±
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ documents.py        # æ–‡æ¡£ç®¡ç† API
â”‚   â”‚   â”œâ”€â”€ chat.py             # å¯¹è¯ API
â”‚   â”‚   â”œâ”€â”€ admin.py            # ç®¡ç† API
â”‚   â”‚   â””â”€â”€ monitoring.py       # ç›‘æ§ API
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                 # æ•°æ®æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ document.py
â”‚   â”‚   â”œâ”€â”€ conversation.py
â”‚   â”‚   â””â”€â”€ user.py
â”‚   â”‚
â”‚   â”œâ”€â”€ services/               # ä¸šåŠ¡æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ document_service.py
â”‚   â”‚   â”œâ”€â”€ chat_service.py
â”‚   â”‚   â””â”€â”€ user_service.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/                  # å·¥å…·å‡½æ•°
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py
â”‚       â”œâ”€â”€ cache.py
â”‚       â””â”€â”€ helpers.py
â”‚
â”œâ”€â”€ tests/                      # æµ‹è¯•
â”‚   â”œâ”€â”€ test_retrieval.py
â”‚   â”œâ”€â”€ test_generation.py
â”‚   â””â”€â”€ test_api.py
â”‚
â”œâ”€â”€ data/                       # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ documents/              # åŸå§‹æ–‡æ¡£
â”‚   â”œâ”€â”€ processed/              # å¤„ç†åçš„æ–‡æ¡£
â”‚   â””â”€â”€ vectorstore/            # å‘é‡æ•°æ®åº“
â”‚
â”œâ”€â”€ logs/                       # æ—¥å¿—
â”œâ”€â”€ requirements.txt            # ä¾èµ–
â”œâ”€â”€ .env                        # ç¯å¢ƒå˜é‡
â”œâ”€â”€ Dockerfile                  # Docker é…ç½®
â””â”€â”€ README.md                   # é¡¹ç›®æ–‡æ¡£
```

### 1.2 ä¾èµ–å®‰è£…

```python
"""
requirements.txt

# LangChain æ ¸å¿ƒ
langchain==0.1.0
langchain-openai==0.0.5
langchain-community==0.0.20

# å‘é‡æ•°æ®åº“
chromadb==0.4.22
faiss-cpu==1.7.4

# æ–‡æ¡£å¤„ç†
pypdf==4.0.0
python-docx==1.1.0
python-pptx==0.6.23
unstructured==0.12.0
markdown==3.5.2

# Web æ¡†æ¶
fastapi==0.109.0
uvicorn[standard]==0.27.0
python-multipart==0.0.6

# æ•°æ®åº“
sqlalchemy==2.0.25
psycopg2-binary==2.9.9
alembic==1.13.1

# ç¼“å­˜
redis==5.0.1

# ç›‘æ§
prometheus-client==0.19.0
langsmith==0.0.87

# å·¥å…·
python-dotenv==1.0.0
pydantic==2.5.3
pydantic-settings==2.1.0
"""
```

### 1.3 é…ç½®ç®¡ç†

```python
# app/config.py

from pydantic_settings import BaseSettings
from typing import Optional
from functools import lru_cache

class Settings(BaseSettings):
    """åº”ç”¨é…ç½®"""

    # åº”ç”¨åŸºç¡€é…ç½®
    APP_NAME: str = "Enterprise RAG System"
    APP_VERSION: str = "1.0.0"
    DEBUG: bool = False
    HOST: str = "0.0.0.0"
    PORT: int = 8000

    # LLM é…ç½®
    OPENAI_API_KEY: str
    OPENAI_MODEL: str = "gpt-3.5-turbo"
    OPENAI_TEMPERATURE: float = 0.0
    OPENAI_MAX_TOKENS: Optional[int] = None

    # Embedding é…ç½®
    EMBEDDING_MODEL: str = "text-embedding-3-small"
    EMBEDDING_DIMENSION: int = 1536

    # å‘é‡æ•°æ®åº“é…ç½®
    VECTOR_STORE_TYPE: str = "chroma"  # chroma, faiss
    VECTOR_STORE_PATH: str = "./data/vectorstore"
    COLLECTION_NAME: str = "documents"

    # æ•°æ®åº“é…ç½®
    DATABASE_URL: str = "postgresql://user:password@localhost:5432/rag_db"
    DATABASE_ECHO: bool = False

    # Redis é…ç½®
    REDIS_URL: str = "redis://localhost:6379/0"
    CACHE_TTL: int = 3600  # ç¼“å­˜è¿‡æœŸæ—¶é—´(ç§’)

    # æ–‡æ¡£å¤„ç†é…ç½®
    CHUNK_SIZE: int = 500
    CHUNK_OVERLAP: int = 50
    MAX_FILE_SIZE: int = 50 * 1024 * 1024  # 50MB
    ALLOWED_EXTENSIONS: list = [".pdf", ".docx", ".txt", ".md"]

    # æ£€ç´¢é…ç½®
    TOP_K: int = 5
    RERANK_K: int = 3
    USE_HYBRID_SEARCH: bool = True
    BM25_WEIGHT: float = 0.5
    VECTOR_WEIGHT: float = 0.5

    # ç›‘æ§é…ç½®
    LANGSMITH_API_KEY: Optional[str] = None
    LANGSMITH_PROJECT: str = "enterprise-rag"
    PROMETHEUS_PORT: int = 9090
    ENABLE_MONITORING: bool = True

    # å®‰å…¨é…ç½®
    SECRET_KEY: str = "your-secret-key-change-in-production"
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 30

    class Config:
        env_file = ".env"
        case_sensitive = True

@lru_cache()
def get_settings() -> Settings:
    """è·å–é…ç½®(å•ä¾‹æ¨¡å¼)"""
    return Settings()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    settings = get_settings()
    print(f"App Name: {settings.APP_NAME}")
    print(f"Model: {settings.OPENAI_MODEL}")
    print(f"Vector Store: {settings.VECTOR_STORE_TYPE}")
```

## äºŒã€æ ¸å¿ƒåŠŸèƒ½å®ç°

### 2.1 æ–‡æ¡£å¤„ç†ç³»ç»Ÿ

```python
# app/core/document_processor.py

from langchain_community.document_loaders import (
    PyPDFLoader,
    Docx2txtLoader,
    TextLoader,
    UnstructuredMarkdownLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from typing import List, Optional
import os
import hashlib
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""

    def __init__(
        self,
        chunk_size: int = 500,
        chunk_overlap: int = 50
    ):
        """
        åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨

        å‚æ•°:
            chunk_size: æ–‡æ¡£å—å¤§å°
            chunk_overlap: é‡å å¤§å°
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

        # æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", "ã€‚", ". ", " ", ""]
        )

        # æ–‡ä»¶ç±»å‹åˆ°åŠ è½½å™¨çš„æ˜ å°„
        self.loader_mapping = {
            ".pdf": PyPDFLoader,
            ".docx": Docx2txtLoader,
            ".txt": TextLoader,
            ".md": UnstructuredMarkdownLoader
        }

        logger.info(f"DocumentProcessor initialized (chunk_size={chunk_size}, overlap={chunk_overlap})")

    def process_file(
        self,
        file_path: str,
        metadata: Optional[dict] = None
    ) -> List[Document]:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶

        å‚æ•°:
            file_path: æ–‡ä»¶è·¯å¾„
            metadata: é™„åŠ å…ƒæ•°æ®

        è¿”å›:
            æ–‡æ¡£å—åˆ—è¡¨
        """
        logger.info(f"Processing file: {file_path}")

        # 1. éªŒè¯æ–‡ä»¶
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"File not found: {file_path}")

        file_ext = Path(file_path).suffix.lower()
        if file_ext not in self.loader_mapping:
            raise ValueError(f"Unsupported file type: {file_ext}")

        try:
            # 2. åŠ è½½æ–‡æ¡£
            loader_class = self.loader_mapping[file_ext]
            loader = loader_class(file_path)
            documents = loader.load()

            logger.info(f"Loaded {len(documents)} document(s)")

            # 3. æ·»åŠ å…ƒæ•°æ®
            file_hash = self._calculate_file_hash(file_path)
            file_size = os.path.getsize(file_path)

            base_metadata = {
                "source": file_path,
                "file_name": os.path.basename(file_path),
                "file_type": file_ext,
                "file_hash": file_hash,
                "file_size": file_size
            }

            if metadata:
                base_metadata.update(metadata)

            for doc in documents:
                doc.metadata.update(base_metadata)

            # 4. æ¸…æ´—æ–‡æ¡£
            cleaned_docs = self._clean_documents(documents)

            # 5. åˆ†å‰²æ–‡æ¡£
            chunks = self.text_splitter.split_documents(cleaned_docs)

            # 6. æ·»åŠ å—ç´¢å¼•
            for i, chunk in enumerate(chunks):
                chunk.metadata["chunk_index"] = i
                chunk.metadata["chunk_total"] = len(chunks)

            logger.info(f"Split into {len(chunks)} chunks")

            return chunks

        except Exception as e:
            logger.error(f"Error processing file {file_path}: {e}")
            raise

    def process_directory(
        self,
        directory_path: str,
        recursive: bool = True,
        metadata: Optional[dict] = None
    ) -> List[Document]:
        """
        å¤„ç†æ•´ä¸ªç›®å½•

        å‚æ•°:
            directory_path: ç›®å½•è·¯å¾„
            recursive: æ˜¯å¦é€’å½’å¤„ç†å­ç›®å½•
            metadata: é™„åŠ å…ƒæ•°æ®

        è¿”å›:
            æ‰€æœ‰æ–‡æ¡£å—åˆ—è¡¨
        """
        logger.info(f"Processing directory: {directory_path}")

        all_chunks = []
        file_count = 0
        error_count = 0

        # éå†ç›®å½•
        pattern = "**/*" if recursive else "*"
        for file_path in Path(directory_path).glob(pattern):
            if file_path.is_file() and file_path.suffix.lower() in self.loader_mapping:
                try:
                    chunks = self.process_file(str(file_path), metadata)
                    all_chunks.extend(chunks)
                    file_count += 1
                except Exception as e:
                    logger.error(f"Failed to process {file_path}: {e}")
                    error_count += 1

        logger.info(f"Processed {file_count} files, {error_count} errors, {len(all_chunks)} total chunks")

        return all_chunks

    def _clean_documents(self, documents: List[Document]) -> List[Document]:
        """æ¸…æ´—æ–‡æ¡£"""
        cleaned = []

        for doc in documents:
            # ç§»é™¤å¤šä½™ç©ºç™½
            content = " ".join(doc.page_content.split())

            # è¿‡æ»¤å¤ªçŸ­çš„æ–‡æ¡£
            if len(content) < 10:
                continue

            # ç§»é™¤ç‰¹æ®Šå­—ç¬¦(å¯é€‰)
            # content = re.sub(r'[^\w\s\u4e00-\u9fff,.!?;:""''\(\)]', '', content)

            doc.page_content = content
            cleaned.append(doc)

        return cleaned

    def _calculate_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶ hash"""
        hasher = hashlib.md5()

        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(8192), b''):
                hasher.update(chunk)

        return hasher.hexdigest()

    def update_chunk_size(self, chunk_size: int, chunk_overlap: int):
        """æ›´æ–°åˆ†å—å‚æ•°"""
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", "ã€‚", ". ", " ", ""]
        )

        logger.info(f"Updated chunk_size={chunk_size}, overlap={chunk_overlap}")

# ===== ä½¿ç”¨ç¤ºä¾‹ =====
if __name__ == "__main__":
    import logging
    logging.basicConfig(level=logging.INFO)

    # åˆ›å»ºæ–‡æ¡£å¤„ç†å™¨
    processor = DocumentProcessor(chunk_size=500, chunk_overlap=50)

    # å¤„ç†å•ä¸ªæ–‡ä»¶
    chunks = processor.process_file(
        "example.pdf",
        metadata={"category": "æŠ€æœ¯æ–‡æ¡£", "author": "å¼ ä¸‰"}
    )

    print(f"å¤„ç†ç»“æœ: {len(chunks)} ä¸ªæ–‡æ¡£å—")
    print(f"\nç¬¬ä¸€ä¸ªå—:")
    print(f"å†…å®¹: {chunks[0].page_content[:200]}...")
    print(f"å…ƒæ•°æ®: {chunks[0].metadata}")

    # å¤„ç†ç›®å½•
    all_chunks = processor.process_directory(
        "./documents",
        recursive=True,
        metadata={"project": "ä¼ä¸šçŸ¥è¯†åº“"}
    )

    print(f"\nç›®å½•å¤„ç†ç»“æœ: {len(all_chunks)} ä¸ªæ–‡æ¡£å—")
```

### 2.2 æ£€ç´¢ç³»ç»Ÿ

```python
# app/core/retrieval_system.py

from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain_openai import ChatOpenAI
from langchain.schema import Document
from typing import List, Optional
import os
import logging

logger = logging.getLogger(__name__)

class RetrievalSystem:
    """æ£€ç´¢ç³»ç»Ÿ"""

    def __init__(
        self,
        vector_store_path: str = "./data/vectorstore",
        collection_name: str = "documents",
        embedding_model: str = "text-embedding-3-small",
        top_k: int = 5,
        use_hybrid: bool = True,
        use_rerank: bool = True
    ):
        """
        åˆå§‹åŒ–æ£€ç´¢ç³»ç»Ÿ

        å‚æ•°:
            vector_store_path: å‘é‡å­˜å‚¨è·¯å¾„
            collection_name: é›†åˆåç§°
            embedding_model: Embedding æ¨¡å‹
            top_k: è¿”å›æ–‡æ¡£æ•°é‡
            use_hybrid: æ˜¯å¦ä½¿ç”¨æ··åˆæ£€ç´¢
            use_rerank: æ˜¯å¦ä½¿ç”¨é‡æ’åº
        """
        self.vector_store_path = vector_store_path
        self.collection_name = collection_name
        self.top_k = top_k
        self.use_hybrid = use_hybrid
        self.use_rerank = use_rerank

        # åˆ›å»º Embeddings
        self.embeddings = OpenAIEmbeddings(model=embedding_model)

        # åˆ›å»ºæˆ–åŠ è½½å‘é‡å­˜å‚¨
        self.vectorstore = self._init_vectorstore()

        # BM25 æ£€ç´¢å™¨(ç”¨äºæ··åˆæ£€ç´¢)
        self.bm25_retriever = None

        # é‡æ’åºå™¨
        self.reranker = None
        if use_rerank:
            self.reranker = LLMChainExtractor.from_llm(
                ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
            )

        logger.info(f"RetrievalSystem initialized (top_k={top_k}, hybrid={use_hybrid}, rerank={use_rerank})")

    def _init_vectorstore(self):
        """åˆå§‹åŒ–å‘é‡å­˜å‚¨"""
        if os.path.exists(self.vector_store_path):
            vectorstore = Chroma(
                persist_directory=self.vector_store_path,
                embedding_function=self.embeddings,
                collection_name=self.collection_name
            )
            count = vectorstore._collection.count()
            logger.info(f"Loaded existing vectorstore ({count} documents)")
        else:
            vectorstore = Chroma(
                persist_directory=self.vector_store_path,
                embedding_function=self.embeddings,
                collection_name=self.collection_name
            )
            logger.info("Created new vectorstore")

        return vectorstore

    def index_documents(self, documents: List[Document]):
        """
        ç´¢å¼•æ–‡æ¡£

        å‚æ•°:
            documents: æ–‡æ¡£åˆ—è¡¨
        """
        logger.info(f"Indexing {len(documents)} documents...")

        # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
        self.vectorstore.add_documents(documents)

        # æ›´æ–° BM25(å¦‚æœä½¿ç”¨æ··åˆæ£€ç´¢)
        if self.use_hybrid:
            self._update_bm25(documents)

        logger.info(f"âœ“ Indexed {len(documents)} documents")

    def _update_bm25(self, documents: List[Document]):
        """æ›´æ–° BM25 æ£€ç´¢å™¨"""
        # è·å–æ‰€æœ‰æ–‡æ¡£
        all_docs = self._get_all_documents()
        all_docs.extend(documents)

        # åˆ›å»º BM25 æ£€ç´¢å™¨
        self.bm25_retriever = BM25Retriever.from_documents(all_docs)
        self.bm25_retriever.k = self.top_k

        logger.info(f"Updated BM25 with {len(all_docs)} documents")

    def _get_all_documents(self) -> List[Document]:
        """è·å–æ‰€æœ‰æ–‡æ¡£(ç®€åŒ–å®ç°)"""
        # æ³¨æ„: è¿™æ˜¯ç®€åŒ–å®ç°,ç”Ÿäº§ç¯å¢ƒåº”è¯¥ä»æ•°æ®åº“è·å–
        try:
            results = self.vectorstore.similarity_search("", k=10000)
            return results
        except:
            return []

    def search(
        self,
        query: str,
        k: Optional[int] = None,
        filter_dict: Optional[dict] = None
    ) -> List[Document]:
        """
        æ£€ç´¢æ–‡æ¡£

        å‚æ•°:
            query: æŸ¥è¯¢
            k: è¿”å›æ–‡æ¡£æ•°é‡
            filter_dict: å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶

        è¿”å›:
            ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        """
        k = k or self.top_k

        logger.info(f"Searching for: {query}")

        # åˆ›å»ºæ£€ç´¢å™¨
        retriever = self._create_retriever(k, filter_dict)

        # æ‰§è¡Œæ£€ç´¢
        results = retriever.get_relevant_documents(query)

        logger.info(f"Found {len(results)} documents")

        return results

    def _create_retriever(self, k: int, filter_dict: Optional[dict]):
        """åˆ›å»ºæ£€ç´¢å™¨"""
        if self.use_hybrid and self.bm25_retriever:
            # æ··åˆæ£€ç´¢
            vector_retriever = self.vectorstore.as_retriever(
                search_kwargs={"k": k, "filter": filter_dict} if filter_dict else {"k": k}
            )

            ensemble_retriever = EnsembleRetriever(
                retrievers=[self.bm25_retriever, vector_retriever],
                weights=[0.5, 0.5]
            )

            base_retriever = ensemble_retriever
        else:
            # çº¯å‘é‡æ£€ç´¢
            base_retriever = self.vectorstore.as_retriever(
                search_kwargs={"k": k, "filter": filter_dict} if filter_dict else {"k": k}
            )

        # æ·»åŠ é‡æ’åº
        if self.use_rerank and self.reranker:
            retriever = ContextualCompressionRetriever(
                base_compressor=self.reranker,
                base_retriever=base_retriever
            )
        else:
            retriever = base_retriever

        return retriever

    def search_with_scores(
        self,
        query: str,
        k: Optional[int] = None
    ) -> List[tuple]:
        """
        æ£€ç´¢æ–‡æ¡£å¹¶è¿”å›ç›¸ä¼¼åº¦åˆ†æ•°

        å‚æ•°:
            query: æŸ¥è¯¢
            k: è¿”å›æ–‡æ¡£æ•°é‡

        è¿”å›:
            (æ–‡æ¡£, åˆ†æ•°) å…ƒç»„åˆ—è¡¨
        """
        k = k or self.top_k

        results = self.vectorstore.similarity_search_with_relevance_scores(
            query,
            k=k
        )

        return results

    def delete_documents(self, filter_dict: dict):
        """
        åˆ é™¤æ–‡æ¡£

        å‚æ•°:
            filter_dict: è¿‡æ»¤æ¡ä»¶
        """
        # Chroma çš„åˆ é™¤
        results = self.vectorstore.get(where=filter_dict)

        if results and results["ids"]:
            self.vectorstore.delete(ids=results["ids"])
            logger.info(f"Deleted {len(results['ids'])} documents")
        else:
            logger.info("No documents to delete")

    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_documents": self.vectorstore._collection.count(),
            "collection_name": self.collection_name,
            "vector_store_path": self.vector_store_path,
            "use_hybrid": self.use_hybrid,
            "use_rerank": self.use_rerank
        }

# ===== ä½¿ç”¨ç¤ºä¾‹ =====
if __name__ == "__main__":
    import logging
    logging.basicConfig(level=logging.INFO)

    from langchain.schema import Document

    # åˆ›å»ºæ£€ç´¢ç³»ç»Ÿ
    retrieval_system = RetrievalSystem(
        top_k=5,
        use_hybrid=True,
        use_rerank=True
    )

    # ç´¢å¼•æ–‡æ¡£
    documents = [
        Document(
            page_content="LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ LLM åº”ç”¨çš„æ¡†æ¶",
            metadata={"source": "doc1.txt", "category": "æ¡†æ¶"}
        ),
        Document(
            page_content="RAG ç»“åˆäº†æ£€ç´¢å’Œç”Ÿæˆä¸¤ä¸ªæ­¥éª¤",
            metadata={"source": "doc2.txt", "category": "æŠ€æœ¯"}
        ),
        Document(
            page_content="å‘é‡æ•°æ®åº“ç”¨äºå­˜å‚¨ Embeddings",
            metadata={"source": "doc3.txt", "category": "æ•°æ®åº“"}
        )
    ]

    retrieval_system.index_documents(documents)

    # æ£€ç´¢
    results = retrieval_system.search("ä»€ä¹ˆæ˜¯ RAG?", k=2)

    print("\næ£€ç´¢ç»“æœ:")
    for i, doc in enumerate(results, 1):
        print(f"\n{i}. {doc.page_content}")
        print(f"   æ¥æº: {doc.metadata.get('source')}")

    # ç»Ÿè®¡ä¿¡æ¯
    stats = retrieval_system.get_stats()
    print(f"\nç»Ÿè®¡ä¿¡æ¯:")
    for key, value in stats.items():
        print(f"  {key}: {value}")
```

ç”±äºå†…å®¹è¾ƒé•¿,æˆ‘å°†åœ¨ä¸‹ä¸€æ¡æ¶ˆæ¯ç»§ç»­åˆ›å»ºå‰©ä½™éƒ¨åˆ†...
### 2.3 ç”Ÿæˆç³»ç»Ÿ

```python
# app/core/generation_system.py

from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import Document
from typing import List, Dict, Optional
import logging

logger = logging.getLogger(__name__)

class GenerationSystem:
    """ç”Ÿæˆç³»ç»Ÿ"""

    def __init__(
        self,
        model_name: str = "gpt-3.5-turbo",
        temperature: float = 0.0
    ):
        """
        åˆå§‹åŒ–ç”Ÿæˆç³»ç»Ÿ

        å‚æ•°:
            model_name: LLM æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
        """
        self.model_name = model_name
        self.temperature = temperature

        # åˆ›å»º LLM
        self.llm = ChatOpenAI(
            model=model_name,
            temperature=temperature
        )

        # Prompt æ¨¡æ¿
        self.prompt_template = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ™ºèƒ½åŠ©æ‰‹,åŸºäºæä¾›çš„å‚è€ƒæ–‡æ¡£å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

å›ç­”è¦æ±‚:
1. å‡†ç¡®:ä¸¥æ ¼åŸºäºå‚è€ƒæ–‡æ¡£å†…å®¹
2. å®Œæ•´:å…¨é¢å›ç­”ç”¨æˆ·é—®é¢˜
3. ç®€æ´:é¿å…å†—ä½™ä¿¡æ¯
4. æ¥æº:å¦‚æœå¯èƒ½,å¼•ç”¨æ–‡æ¡£æ¥æº
5. è¯šå®:å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯,æ˜ç¡®å‘ŠçŸ¥

å‚è€ƒæ–‡æ¡£:
{context}"""),
            ("human", "{question}")
        ])

        logger.info(f"GenerationSystem initialized (model={model_name}, temp={temperature})")

    def generate(
        self,
        query: str,
        documents: List[Document],
        conversation_history: Optional[List[Dict]] = None
    ) -> Dict:
        """
        ç”Ÿæˆå›ç­”

        å‚æ•°:
            query: ç”¨æˆ·é—®é¢˜
            documents: æ£€ç´¢åˆ°çš„æ–‡æ¡£
            conversation_history: å¯¹è¯å†å²

        è¿”å›:
            åŒ…å«ç­”æ¡ˆå’Œå…ƒæ•°æ®çš„å­—å…¸
        """
        logger.info(f"Generating answer for: {query}")

        # 1. æ„å»ºä¸Šä¸‹æ–‡
        context = self._build_context(documents)

        # 2. æ·»åŠ å¯¹è¯å†å²(å¦‚æœæœ‰)
        if conversation_history:
            history_text = self._format_history(conversation_history)
            context = f"{history_text}\n\n{context}"

        # 3. ç”Ÿæˆå›ç­”
        chain = self.prompt_template | self.llm

        try:
            response = chain.invoke({
                "context": context,
                "question": query
            })

            answer = response.content

            # 4. æ„å»ºå“åº”
            result = {
                "answer": answer,
                "sources": [doc.metadata.get("source", "Unknown") for doc in documents],
                "source_documents": documents,
                "model": self.model_name,
                "success": True
            }

            logger.info("âœ“ Answer generated successfully")

            return result

        except Exception as e:
            logger.error(f"Error generating answer: {e}")
            return {
                "answer": f"æŠ±æ­‰,ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}",
                "sources": [],
                "source_documents": [],
                "model": self.model_name,
                "success": False,
                "error": str(e)
            }

    def _build_context(self, documents: List[Document]) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡"""
        if not documents:
            return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚"

        context_parts = []

        for i, doc in enumerate(documents, 1):
            source = doc.metadata.get("source", "Unknown")
            content = doc.page_content

            context_parts.append(f"æ–‡æ¡£ {i} (æ¥æº: {source}):\n{content}")

        return "\n\n".join(context_parts)

    def _format_history(self, history: List[Dict]) -> str:
        """æ ¼å¼åŒ–å¯¹è¯å†å²"""
        if not history:
            return ""

        history_parts = ["å¯¹è¯å†å²:"]

        for item in history[-5:]:  # åªä¿ç•™æœ€è¿‘5è½®
            role = item.get("role", "user")
            content = item.get("content", "")

            if role == "user":
                history_parts.append(f"ç”¨æˆ·: {content}")
            elif role == "assistant":
                history_parts.append(f"åŠ©æ‰‹: {content}")

        return "\n".join(history_parts)

    def generate_streaming(
        self,
        query: str,
        documents: List[Document]
    ):
        """
        æµå¼ç”Ÿæˆå›ç­”

        å‚æ•°:
            query: ç”¨æˆ·é—®é¢˜
            documents: æ£€ç´¢åˆ°çš„æ–‡æ¡£

        è¿”å›:
            ç”Ÿæˆå™¨,é€ä¸ªè¿”å› token
        """
        context = self._build_context(documents)

        chain = self.prompt_template | self.llm

        for chunk in chain.stream({
            "context": context,
            "question": query
        }):
            if hasattr(chunk, 'content'):
                yield chunk.content
            else:
                yield str(chunk)

# ===== ä½¿ç”¨ç¤ºä¾‹ =====
if __name__ == "__main__":
    import logging
    logging.basicConfig(level=logging.INFO)

    from langchain.schema import Document

    # åˆ›å»ºç”Ÿæˆç³»ç»Ÿ
    gen_system = GenerationSystem()

    # å‡†å¤‡æ–‡æ¡£
    documents = [
        Document(
            page_content="LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ LLM åº”ç”¨çš„ Python æ¡†æ¶,ç”± Harrison Chase äº 2022 å¹´åˆ›å»ºã€‚",
            metadata={"source": "langchain_intro.md"}
        ),
        Document(
            page_content="LangChain çš„æ ¸å¿ƒç»„ä»¶åŒ…æ‹¬: Modelsã€Promptsã€Chainsã€Memoryã€Agents ç­‰ã€‚",
            metadata={"source": "langchain_components.md"}
        )
    ]

    # ç”Ÿæˆå›ç­”
    result = gen_system.generate(
        query="LangChain æ˜¯è°åˆ›å»ºçš„?",
        documents=documents
    )

    print("\né—®é¢˜:", "LangChain æ˜¯è°åˆ›å»ºçš„?")
    print("\nå›ç­”:", result["answer"])
    print("\næ¥æº:", result["sources"])

    # æµå¼ç”Ÿæˆ
    print("\n\næµå¼ç”Ÿæˆ:")
    for token in gen_system.generate_streaming(
        query="LangChain æœ‰å“ªäº›æ ¸å¿ƒç»„ä»¶?",
        documents=documents
    ):
        print(token, end="", flush=True)
```

### 2.4 å¯¹è¯ç®¡ç†ç³»ç»Ÿ

```python
# app/core/conversation_manager.py

from typing import List, Dict, Optional
from datetime import datetime
import json
import logging

logger = logging.getLogger(__name__)

class ConversationManager:
    """å¯¹è¯ç®¡ç†å™¨"""

    def __init__(
        self,
        max_history_length: int = 10
    ):
        """
        åˆå§‹åŒ–å¯¹è¯ç®¡ç†å™¨

        å‚æ•°:
            max_history_length: æœ€å¤§å†å²é•¿åº¦
        """
        self.max_history_length = max_history_length

        # ä¼šè¯å­˜å‚¨ {session_id: session_data}
        self.sessions = {}

        logger.info(f"ConversationManager initialized (max_history={max_history_length})")

    def create_session(self, session_id: str, metadata: Optional[Dict] = None) -> Dict:
        """
        åˆ›å»ºæ–°ä¼šè¯

        å‚æ•°:
            session_id: ä¼šè¯ID
            metadata: ä¼šè¯å…ƒæ•°æ®

        è¿”å›:
            ä¼šè¯æ•°æ®
        """
        session_data = {
            "session_id": session_id,
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "history": [],
            "metadata": metadata or {}
        }

        self.sessions[session_id] = session_data

        logger.info(f"Session created: {session_id}")

        return session_data

    def get_session(self, session_id: str) -> Optional[Dict]:
        """è·å–ä¼šè¯"""
        return self.sessions.get(session_id)

    def add_message(
        self,
        session_id: str,
        role: str,
        content: str,
        metadata: Optional[Dict] = None
    ):
        """
        æ·»åŠ æ¶ˆæ¯åˆ°ä¼šè¯

        å‚æ•°:
            session_id: ä¼šè¯ID
            role: è§’è‰²(user/assistant)
            content: æ¶ˆæ¯å†…å®¹
            metadata: æ¶ˆæ¯å…ƒæ•°æ®
        """
        if session_id not in self.sessions:
            self.create_session(session_id)

        message = {
            "role": role,
            "content": content,
            "timestamp": datetime.now().isoformat(),
            "metadata": metadata or {}
        }

        self.sessions[session_id]["history"].append(message)
        self.sessions[session_id]["updated_at"] = datetime.now().isoformat()

        # é™åˆ¶å†å²é•¿åº¦
        if len(self.sessions[session_id]["history"]) > self.max_history_length:
            self.sessions[session_id]["history"] = \
                self.sessions[session_id]["history"][-self.max_history_length:]

        logger.debug(f"Message added to session {session_id}")

    def get_history(self, session_id: str, limit: Optional[int] = None) -> List[Dict]:
        """
        è·å–å¯¹è¯å†å²

        å‚æ•°:
            session_id: ä¼šè¯ID
            limit: é™åˆ¶è¿”å›æ•°é‡

        è¿”å›:
            æ¶ˆæ¯åˆ—è¡¨
        """
        if session_id not in self.sessions:
            return []

        history = self.sessions[session_id]["history"]

        if limit:
            return history[-limit:]

        return history

    def clear_history(self, session_id: str):
        """æ¸…ç©ºä¼šè¯å†å²"""
        if session_id in self.sessions:
            self.sessions[session_id]["history"] = []
            self.sessions[session_id]["updated_at"] = datetime.now().isoformat()
            logger.info(f"History cleared for session {session_id}")

    def delete_session(self, session_id: str):
        """åˆ é™¤ä¼šè¯"""
        if session_id in self.sessions:
            del self.sessions[session_id]
            logger.info(f"Session deleted: {session_id}")

    def get_all_sessions(self) -> List[str]:
        """è·å–æ‰€æœ‰ä¼šè¯ID"""
        return list(self.sessions.keys())

    def export_session(self, session_id: str) -> Optional[str]:
        """
        å¯¼å‡ºä¼šè¯ä¸º JSON

        å‚æ•°:
            session_id: ä¼šè¯ID

        è¿”å›:
            JSON å­—ç¬¦ä¸²
        """
        if session_id not in self.sessions:
            return None

        return json.dumps(self.sessions[session_id], indent=2, ensure_ascii=False)

    def import_session(self, session_json: str) -> str:
        """
        å¯¼å…¥ä¼šè¯

        å‚æ•°:
            session_json: ä¼šè¯ JSON å­—ç¬¦ä¸²

        è¿”å›:
            ä¼šè¯ID
        """
        session_data = json.loads(session_json)
        session_id = session_data["session_id"]

        self.sessions[session_id] = session_data

        logger.info(f"Session imported: {session_id}")

        return session_id

# ===== ä½¿ç”¨ç¤ºä¾‹ =====
if __name__ == "__main__":
    import logging
    logging.basicConfig(level=logging.INFO)

    # åˆ›å»ºå¯¹è¯ç®¡ç†å™¨
    conv_manager = ConversationManager(max_history_length=10)

    # åˆ›å»ºä¼šè¯
    session_id = "user_123_session_001"
    conv_manager.create_session(
        session_id,
        metadata={"user_id": "user_123", "platform": "web"}
    )

    # æ·»åŠ æ¶ˆæ¯
    conv_manager.add_message(session_id, "user", "ä»€ä¹ˆæ˜¯ LangChain?")
    conv_manager.add_message(
        session_id,
        "assistant",
        "LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ LLM åº”ç”¨çš„æ¡†æ¶ã€‚",
        metadata={"sources": ["doc1.md"]}
    )
    conv_manager.add_message(session_id, "user", "å®ƒæœ‰å“ªäº›æ ¸å¿ƒç»„ä»¶?")
    conv_manager.add_message(
        session_id,
        "assistant",
        "LangChain çš„æ ¸å¿ƒç»„ä»¶åŒ…æ‹¬: Modelsã€Promptsã€Chainsã€Memoryã€Agents ç­‰ã€‚"
    )

    # è·å–å†å²
    history = conv_manager.get_history(session_id)

    print(f"\nä¼šè¯å†å² ({len(history)} æ¡æ¶ˆæ¯):")
    for msg in history:
        print(f"{msg['role']}: {msg['content']}")

    # å¯¼å‡ºä¼šè¯
    session_json = conv_manager.export_session(session_id)
    print(f"\nå¯¼å‡ºçš„ä¼šè¯:\n{session_json}")
```

## ä¸‰ã€FastAPI åç«¯å®ç°

### 3.1 ä¸»åº”ç”¨

```python
# app/main.py

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import logging

from app.config import get_settings
from app.api import documents, chat, admin, monitoring
from app.core.document_processor import DocumentProcessor
from app.core.retrieval_system import RetrievalSystem
from app.core.generation_system import GenerationSystem
from app.core.conversation_manager import ConversationManager

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

# å…¨å±€å®ä¾‹
settings = get_settings()
document_processor = None
retrieval_system = None
generation_system = None
conversation_manager = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    global document_processor, retrieval_system, generation_system, conversation_manager

    logger.info("Initializing components...")

    document_processor = DocumentProcessor(
        chunk_size=settings.CHUNK_SIZE,
        chunk_overlap=settings.CHUNK_OVERLAP
    )

    retrieval_system = RetrievalSystem(
        vector_store_path=settings.VECTOR_STORE_PATH,
        collection_name=settings.COLLECTION_NAME,
        embedding_model=settings.EMBEDDING_MODEL,
        top_k=settings.TOP_K,
        use_hybrid=settings.USE_HYBRID_SEARCH
    )

    generation_system = GenerationSystem(
        model_name=settings.OPENAI_MODEL,
        temperature=settings.OPENAI_TEMPERATURE
    )

    conversation_manager = ConversationManager()

    logger.info("âœ“ All components initialized")

    yield

    # å…³é—­æ—¶æ¸…ç†
    logger.info("Shutting down...")

# åˆ›å»ºåº”ç”¨
app = FastAPI(
    title=settings.APP_NAME,
    version=settings.APP_VERSION,
    lifespan=lifespan
)

# CORS ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒåº”è¯¥é™åˆ¶
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ³¨å†Œè·¯ç”±
app.include_router(documents.router, prefix="/api/documents", tags=["Documents"])
app.include_router(chat.router, prefix="/api/chat", tags=["Chat"])
app.include_router(admin.router, prefix="/api/admin", tags=["Admin"])
app.include_router(monitoring.router, prefix="/api/monitoring", tags=["Monitoring"])

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "name": settings.APP_NAME,
        "version": settings.APP_VERSION,
        "status": "running"
    }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "components": {
            "document_processor": document_processor is not None,
            "retrieval_system": retrieval_system is not None,
            "generation_system": generation_system is not None,
            "conversation_manager": conversation_manager is not None
        }
    }

# è¿è¡Œåº”ç”¨
if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "app.main:app",
        host=settings.HOST,
        port=settings.PORT,
        reload=settings.DEBUG
    )
```

### 3.2 å¯¹è¯ API

```python
# app/api/chat.py

from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel, Field
from typing import List, Dict, Optional
from sse_starlette.sse import EventSourceResponse
import asyncio
import logging

logger = logging.getLogger(__name__)

router = APIRouter()

# ===== è¯·æ±‚/å“åº”æ¨¡å‹ =====

class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚"""
    question: str = Field(..., description="ç”¨æˆ·é—®é¢˜")
    session_id: Optional[str] = Field(None, description="ä¼šè¯ID")
    k: Optional[int] = Field(5, description="æ£€ç´¢æ–‡æ¡£æ•°é‡")
    stream: Optional[bool] = Field(False, description="æ˜¯å¦æµå¼è¾“å‡º")

class ChatResponse(BaseModel):
    """èŠå¤©å“åº”"""
    answer: str = Field(..., description="å›ç­”")
    sources: List[str] = Field(default_factory=list, description="æ¥æºåˆ—è¡¨")
    session_id: str = Field(..., description="ä¼šè¯ID")
    success: bool = Field(..., description="æ˜¯å¦æˆåŠŸ")

class SessionCreate(BaseModel):
    """åˆ›å»ºä¼šè¯è¯·æ±‚"""
    session_id: Optional[str] = Field(None, description="ä¼šè¯ID(å¯é€‰)")
    metadata: Optional[Dict] = Field(None, description="å…ƒæ•°æ®")

class SessionResponse(BaseModel):
    """ä¼šè¯å“åº”"""
    session_id: str
    created_at: str
    message_count: int

# ===== API ç«¯ç‚¹ =====

@router.post("/message", response_model=ChatResponse)
async def send_message(request: ChatRequest):
    """
    å‘é€æ¶ˆæ¯

    å‚æ•°:
        request: èŠå¤©è¯·æ±‚

    è¿”å›:
        èŠå¤©å“åº”
    """
    from app.main import retrieval_system, generation_system, conversation_manager
    import uuid

    try:
        # 1. ç”Ÿæˆæˆ–ä½¿ç”¨ä¼šè¯ID
        session_id = request.session_id or str(uuid.uuid4())

        # 2. ç¡®ä¿ä¼šè¯å­˜åœ¨
        if not conversation_manager.get_session(session_id):
            conversation_manager.create_session(session_id)

        # 3. æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        conversation_manager.add_message(session_id, "user", request.question)

        # 4. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        logger.info(f"Retrieving documents for: {request.question}")
        documents = retrieval_system.search(request.question, k=request.k)

        # 5. è·å–å¯¹è¯å†å²
        history = conversation_manager.get_history(session_id, limit=5)

        # 6. ç”Ÿæˆå›ç­”
        logger.info("Generating answer...")
        result = generation_system.generate(
            query=request.question,
            documents=documents,
            conversation_history=history
        )

        # 7. æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯
        conversation_manager.add_message(
            session_id,
            "assistant",
            result["answer"],
            metadata={"sources": result["sources"]}
        )

        # 8. è¿”å›å“åº”
        return ChatResponse(
            answer=result["answer"],
            sources=result["sources"],
            session_id=session_id,
            success=result["success"]
        )

    except Exception as e:
        logger.error(f"Error in send_message: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/message/stream")
async def send_message_stream(request: ChatRequest):
    """
    å‘é€æ¶ˆæ¯(æµå¼)

    å‚æ•°:
        request: èŠå¤©è¯·æ±‚

    è¿”å›:
        SSE äº‹ä»¶æµ
    """
    from app.main import retrieval_system, generation_system, conversation_manager
    import uuid

    async def event_generator():
        try:
            # 1. ä¼šè¯ç®¡ç†
            session_id = request.session_id or str(uuid.uuid4())

            if not conversation_manager.get_session(session_id):
                conversation_manager.create_session(session_id)

            conversation_manager.add_message(session_id, "user", request.question)

            # 2. æ£€ç´¢æ–‡æ¡£
            yield {"event": "status", "data": "æ£€ç´¢ç›¸å…³æ–‡æ¡£..."}

            documents = retrieval_system.search(request.question, k=request.k)

            yield {"event": "sources", "data": [doc.metadata.get("source") for doc in documents]}

            # 3. æµå¼ç”Ÿæˆ
            yield {"event": "status", "data": "ç”Ÿæˆå›ç­”..."}

            full_answer = ""

            for token in generation_system.generate_streaming(request.question, documents):
                full_answer += token
                yield {"event": "token", "data": token}

            # 4. ä¿å­˜åŠ©æ‰‹æ¶ˆæ¯
            sources = [doc.metadata.get("source") for doc in documents]
            conversation_manager.add_message(
                session_id,
                "assistant",
                full_answer,
                metadata={"sources": sources}
            )

            # 5. å®Œæˆ
            yield {"event": "done", "data": {"session_id": session_id}}

        except Exception as e:
            logger.error(f"Error in stream: {e}")
            yield {"event": "error", "data": str(e)}

    return EventSourceResponse(event_generator())

@router.post("/session", response_model=SessionResponse)
async def create_session(request: SessionCreate):
    """
    åˆ›å»ºæ–°ä¼šè¯

    å‚æ•°:
        request: åˆ›å»ºä¼šè¯è¯·æ±‚

    è¿”å›:
        ä¼šè¯ä¿¡æ¯
    """
    from app.main import conversation_manager
    import uuid

    session_id = request.session_id or str(uuid.uuid4())

    session = conversation_manager.create_session(session_id, request.metadata)

    return SessionResponse(
        session_id=session["session_id"],
        created_at=session["created_at"],
        message_count=len(session["history"])
    )

@router.get("/session/{session_id}/history")
async def get_session_history(
    session_id: str,
    limit: Optional[int] = None
):
    """
    è·å–ä¼šè¯å†å²

    å‚æ•°:
        session_id: ä¼šè¯ID
        limit: é™åˆ¶æ•°é‡

    è¿”å›:
        æ¶ˆæ¯åˆ—è¡¨
    """
    from app.main import conversation_manager

    history = conversation_manager.get_history(session_id, limit)

    if history is None:
        raise HTTPException(status_code=404, detail="Session not found")

    return {"session_id": session_id, "history": history}

@router.delete("/session/{session_id}")
async def delete_session(session_id: str):
    """
    åˆ é™¤ä¼šè¯

    å‚æ•°:
        session_id: ä¼šè¯ID

    è¿”å›:
        åˆ é™¤ç»“æœ
    """
    from app.main import conversation_manager

    conversation_manager.delete_session(session_id)

    return {"message": "Session deleted", "session_id": session_id}
```

### 3.3 æ–‡æ¡£ç®¡ç† API

```python
# app/api/documents.py

from fastapi import APIRouter, UploadFile, File, HTTPException
from pydantic import BaseModel, Field
from typing import List, Optional
import os
import tempfile
import logging

logger = logging.getLogger(__name__)

router = APIRouter()

# ===== è¯·æ±‚/å“åº”æ¨¡å‹ =====

class IndexResponse(BaseModel):
    """ç´¢å¼•å“åº”"""
    success: bool
    message: str
    chunks_count: int
    file_name: str

class SearchRequest(BaseModel):
    """æœç´¢è¯·æ±‚"""
    query: str = Field(..., description="æœç´¢æŸ¥è¯¢")
    k: Optional[int] = Field(5, description="è¿”å›æ•°é‡")

class DocumentMetadata(BaseModel):
    """æ–‡æ¡£å…ƒæ•°æ®"""
    source: str
    file_name: str
    chunk_index: int
    chunk_total: int

class SearchResult(BaseModel):
    """æœç´¢ç»“æœ"""
    content: str
    metadata: DocumentMetadata
    score: Optional[float] = None

class SearchResponse(BaseModel):
    """æœç´¢å“åº”"""
    query: str
    results: List[SearchResult]
    total: int

# ===== API ç«¯ç‚¹ =====

@router.post("/upload", response_model=IndexResponse)
async def upload_document(
    file: UploadFile = File(...),
    metadata: Optional[str] = None
):
    """
    ä¸Šä¼ å¹¶ç´¢å¼•æ–‡æ¡£

    å‚æ•°:
        file: ä¸Šä¼ çš„æ–‡ä»¶
        metadata: å…ƒæ•°æ® JSON å­—ç¬¦ä¸²

    è¿”å›:
        ç´¢å¼•ç»“æœ
    """
    from app.main import document_processor, retrieval_system
    import json

    try:
        # 1. éªŒè¯æ–‡ä»¶ç±»å‹
        file_ext = os.path.splitext(file.filename)[1].lower()
        from app.config import get_settings
        settings = get_settings()

        if file_ext not in settings.ALLOWED_EXTENSIONS:
            raise HTTPException(
                status_code=400,
                detail=f"Unsupported file type: {file_ext}"
            )

        # 2. ä¿å­˜æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as tmp_file:
            content = await file.read()
            tmp_file.write(content)
            tmp_file_path = tmp_file.name

        # 3. è§£æå…ƒæ•°æ®
        doc_metadata = json.loads(metadata) if metadata else {}
        doc_metadata["original_filename"] = file.filename

        # 4. å¤„ç†æ–‡æ¡£
        logger.info(f"Processing file: {file.filename}")
        chunks = document_processor.process_file(tmp_file_path, doc_metadata)

        # 5. ç´¢å¼•åˆ°å‘é‡æ•°æ®åº“
        logger.info(f"Indexing {len(chunks)} chunks...")
        retrieval_system.index_documents(chunks)

        # 6. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.remove(tmp_file_path)

        return IndexResponse(
            success=True,
            message="Document indexed successfully",
            chunks_count=len(chunks),
            file_name=file.filename
        )

    except Exception as e:
        logger.error(f"Error uploading document: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/search", response_model=SearchResponse)
async def search_documents(request: SearchRequest):
    """
    æœç´¢æ–‡æ¡£

    å‚æ•°:
        request: æœç´¢è¯·æ±‚

    è¿”å›:
        æœç´¢ç»“æœ
    """
    from app.main import retrieval_system

    try:
        # æ‰§è¡Œæœç´¢
        documents = retrieval_system.search(request.query, k=request.k)

        # æ„å»ºå“åº”
        results = [
            SearchResult(
                content=doc.page_content,
                metadata=DocumentMetadata(
                    source=doc.metadata.get("source", "Unknown"),
                    file_name=doc.metadata.get("file_name", "Unknown"),
                    chunk_index=doc.metadata.get("chunk_index", 0),
                    chunk_total=doc.metadata.get("chunk_total", 0)
                )
            )
            for doc in documents
        ]

        return SearchResponse(
            query=request.query,
            results=results,
            total=len(results)
        )

    except Exception as e:
        logger.error(f"Error searching documents: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/stats")
async def get_stats():
    """
    è·å–ç»Ÿè®¡ä¿¡æ¯

    è¿”å›:
        ç»Ÿè®¡æ•°æ®
    """
    from app.main import retrieval_system

    try:
        stats = retrieval_system.get_stats()
        return stats

    except Exception as e:
        logger.error(f"Error getting stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

## å››ã€éƒ¨ç½²æ–¹æ¡ˆ

### 4.1 Docker éƒ¨ç½²

```dockerfile
# Dockerfile

FROM python:3.10-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£… Python ä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY app/ ./app/

# åˆ›å»ºæ•°æ®ç›®å½•
RUN mkdir -p /app/data/documents \
    /app/data/processed \
    /app/data/vectorstore \
    /app/logs

# æš´éœ²ç«¯å£
EXPOSE 8000 9090

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# å¯åŠ¨åº”ç”¨
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# docker-compose.yml

version: '3.8'

services:
  # RAG åº”ç”¨
  rag-app:
    build: .
    ports:
      - "8000:8000"
      - "9090:9090"
    env_file:
      - .env
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    depends_on:
      - postgres
      - redis
      - chroma
    restart: unless-stopped

  # PostgreSQL æ•°æ®åº“
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: rag_db
      POSTGRES_USER: rag_user
      POSTGRES_PASSWORD: rag_password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    restart: unless-stopped

  # Redis ç¼“å­˜
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped

  # Chroma å‘é‡æ•°æ®åº“
  chroma:
    image: ghcr.io/chroma-core/chroma:latest
    ports:
      - "8001:8000"
    volumes:
      - chroma_data:/chroma/chroma
    environment:
      - IS_PERSISTENT=TRUE
      - PERSIST_DIRECTORY=/chroma/chroma
    restart: unless-stopped

  # Prometheus ç›‘æ§
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9091:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
    restart: unless-stopped

  # Grafana å¯è§†åŒ–
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
    depends_on:
      - prometheus
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
  chroma_data:
  prometheus_data:
  grafana_data:
```

### 4.2 Kubernetes éƒ¨ç½²(å¯é€‰)

```yaml
# k8s/deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: rag-app
  labels:
    app: rag-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: rag-app
  template:
    metadata:
      labels:
        app: rag-app
    spec:
      containers:
      - name: rag-app
        image: your-registry/rag-app:latest
        ports:
        - containerPort: 8000
        - containerPort: 9090
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: rag-secrets
              key: openai-api-key
        - name: DATABASE_URL
          valueFrom:
            configMapKeyRef:
              name: rag-config
              key: database-url
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "2000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5

**æ­å–œ!** ğŸ‰ ä½ å·²ç»å®Œæˆäº†ä¸€ä¸ªä¼ä¸šçº§ RAG ç³»ç»Ÿçš„å¼€å‘!

**ä¸‹ä¸€æ­¥**:
1. å…‹éš†ä»£ç ä»“åº“
2. é…ç½®ç¯å¢ƒå˜é‡
3. å¯åŠ¨ Docker Compose
4. ä¸Šä¼ æ–‡æ¡£æµ‹è¯•
5. éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ

**æºç åœ°å€**: https://github.com/your-repo/enterprise-rag-system
