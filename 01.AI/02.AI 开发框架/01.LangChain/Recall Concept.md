---
title: Recall Concept
date: 2025-11-18
permalink: /ai/langchain/recall-concept.html
categories:
  - AI
---

# å¬å›(Recall)æ¦‚å¿µè¯¦è§£

::: tip å­¦ä¹ ç›®æ ‡
- ğŸ¯ ç†è§£å¬å›çš„å®šä¹‰å’Œé‡è¦æ€§
- ğŸ“Š æŒæ¡å¬å›ç‡å’Œç²¾ç¡®ç‡çš„åŒºåˆ«
- ğŸ” å­¦ä¹ å¤šç§å¬å›ç­–ç•¥
- ğŸš€ ä¼˜åŒ– RAG ç³»ç»Ÿçš„å¬å›æ•ˆæœ
- ğŸ’¡ å®ç°é«˜è´¨é‡çš„æ£€ç´¢ç³»ç»Ÿ
:::

## ä¸€ã€å¬å›çš„å®šä¹‰

### 1.1 ä»€ä¹ˆæ˜¯å¬å›ï¼Ÿ

**å¬å›ï¼ˆRecallï¼‰** æ˜¯æŒ‡åœ¨æ£€ç´¢ç³»ç»Ÿä¸­ï¼Œ**æ‰¾å›æ‰€æœ‰ç›¸å…³ç»“æœçš„èƒ½åŠ›**ã€‚

ç®€å•æ¥è¯´ï¼š
- ğŸ“š **å¬å›** = åœ¨æ‰€æœ‰ç›¸å…³æ–‡æ¡£ä¸­ï¼Œæ£€ç´¢ç³»ç»Ÿæ‰¾åˆ°äº†å¤šå°‘
- ğŸ¯ å…³æ³¨çš„æ˜¯"**æŸ¥å…¨ç‡**"ï¼Œå³ä¸è¦é—æ¼ç›¸å…³å†…å®¹

```mermaid
graph TB
    A[æ–‡æ¡£åº“<br/>10000ç¯‡æ–‡æ¡£] --> B{æ£€ç´¢ç³»ç»Ÿ}
    B --> C[å¬å›ç»“æœ<br/>100ç¯‡]

    D[çœŸæ­£ç›¸å…³çš„æ–‡æ¡£<br/>150ç¯‡] --> E{å¬å›ç‡è®¡ç®—}
    C --> E

    E --> F[å¬å›ç‡ = 100 / 150 = 66.7%]

    style A fill:#E3F2FD
    style C fill:#FFE082
    style D fill:#C8E6C9
    style F fill:#81C784
```

### 1.2 å½¢è±¡ç†è§£

**ç”Ÿæ´»ä¸­çš„ä¾‹å­ï¼š**

```
åœºæ™¯ï¼šåœ¨è¶…å¸‚æ‰¾"æ‰€æœ‰çš„è‹¹æœ"

æƒ…å†µ1ï¼šè¶…å¸‚æœ‰100ä¸ªè‹¹æœï¼Œä½ æ‰¾åˆ°äº†80ä¸ª
â†’ å¬å›ç‡ = 80/100 = 80%

æƒ…å†µ2ï¼šè¶…å¸‚æœ‰100ä¸ªè‹¹æœï¼Œä½ æ‰¾åˆ°äº†95ä¸ª
â†’ å¬å›ç‡ = 95/100 = 95%ï¼ˆæ›´å¥½çš„å¬å›ï¼‰
```

**å…³é”®ç‚¹ï¼š**
- âœ… å¬å›ç‡é«˜ = æ‰¾åˆ°äº†å¤§éƒ¨åˆ†ç›¸å…³å†…å®¹
- âŒ å¬å›ç‡ä½ = é—æ¼äº†å¾ˆå¤šç›¸å…³å†…å®¹


## ä¸‰ã€RAG ç³»ç»Ÿä¸­çš„å¬å›

### 3.1 å¬å›åœ¨ RAG ä¸­çš„ä½ç½®

**RAGï¼ˆRetrieval-Augmented Generationï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰** ç³»ç»Ÿçš„æ ¸å¿ƒæµç¨‹ï¼š

```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·æŸ¥è¯¢
    participant R as å¬å›æ¨¡å—
    participant V as å‘é‡æ•°æ®åº“
    participant Re as é‡æ’åº
    participant L as LLMç”Ÿæˆ

    U->>R: "LangChainçš„æ ¸å¿ƒç»„ä»¶æ˜¯ä»€ä¹ˆï¼Ÿ"
    Note over R: ç¬¬1æ­¥ï¼šå¬å›é˜¶æ®µ<br/>å®½æ³›æ£€ç´¢ï¼Œé«˜å¬å›
    R->>V: å‘é‡ç›¸ä¼¼åº¦æœç´¢
    V->>R: è¿”å› Top-50 å€™é€‰

    Note over Re: ç¬¬2æ­¥ï¼šé‡æ’åºé˜¶æ®µ<br/>ç²¾ç»†æ’åºï¼Œé«˜ç²¾ç¡®ç‡
    R->>Re: 50ä¸ªå€™é€‰æ–‡æ¡£
    Re->>Re: é‡æ–°è®¡ç®—ç›¸å…³æ€§
    Re->>L: Top-5 ç²¾é€‰æ–‡æ¡£

    Note over L: ç¬¬3æ­¥ï¼šç”Ÿæˆé˜¶æ®µ<br/>åŸºäºå¬å›å†…å®¹ç”Ÿæˆç­”æ¡ˆ
    L->>L: é˜…è¯»æ–‡æ¡£ + ç”Ÿæˆç­”æ¡ˆ
    L->>U: æœ€ç»ˆç­”æ¡ˆ
```

### 3.2 å¬å›é˜¶æ®µçš„ä»»åŠ¡

å¬å›æ¨¡å—è´Ÿè´£ä»å¤§é‡æ–‡æ¡£ä¸­å¿«é€Ÿæ‰¾åˆ°**å¯èƒ½ç›¸å…³**çš„å€™é€‰é›†ï¼š

**æ ¸å¿ƒä»»åŠ¡ï¼š**

1. **å‘é‡åŒ–æŸ¥è¯¢**
   ```python
   # å°†ç”¨æˆ·é—®é¢˜è½¬ä¸ºå‘é‡
   query_embedding = embeddings.embed_query("ä»€ä¹ˆæ˜¯LangChainï¼Ÿ")
   ```

2. **ç›¸ä¼¼åº¦æœç´¢**
   ```python
   # åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢
   candidates = vectorstore.similarity_search(
       query,
       k=50  # å¬å›50ä¸ªå€™é€‰
   )
   ```

3. **è¿”å›å€™é€‰é›†**
   ```python
   # è¿”å›æœ€ç›¸ä¼¼çš„ Top-K æ–‡æ¡£
   return candidates[:k]
   ```

### 3.3 RAG å¬å›ç¤ºä¾‹

```python
"""
RAG ç³»ç»Ÿä¸­çš„å¬å›å®ç°
"""
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader

class RAGWithRecall:
    """å¸¦å¬å›å¯è§†åŒ–çš„RAGç³»ç»Ÿ"""

    def __init__(self, documents):
        # åˆå§‹åŒ–
        self.embeddings = OpenAIEmbeddings()
        self.llm = ChatOpenAI(model="gpt-3.5-turbo")

        # æ–‡æ¡£å¤„ç†
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50
        )
        self.chunks = splitter.split_documents(documents)

        # åˆ›å»ºå‘é‡å­˜å‚¨
        self.vectorstore = FAISS.from_documents(
            self.chunks,
            self.embeddings
        )

        print(f"âœ… å·²ç´¢å¼• {len(self.chunks)} ä¸ªæ–‡æ¡£å—")

    def recall_documents(self, query: str, k: int = 10):
        """
        å¬å›ç›¸å…³æ–‡æ¡£

        å‚æ•°:
            query: ç”¨æˆ·æŸ¥è¯¢
            k: å¬å›æ•°é‡

        è¿”å›:
            å¬å›çš„æ–‡æ¡£åˆ—è¡¨
        """
        print(f"\nğŸ” æ­£åœ¨å¬å›ç›¸å…³æ–‡æ¡£...")
        print(f"   æŸ¥è¯¢: {query}")
        print(f"   å¬å›æ•°é‡: {k}")

        # æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢
        docs = self.vectorstore.similarity_search(query, k=k)

        print(f"âœ… æˆåŠŸå¬å› {len(docs)} ä¸ªæ–‡æ¡£")

        # æ˜¾ç¤ºå¬å›ç»“æœ
        print("\nå¬å›çš„æ–‡æ¡£ç‰‡æ®µ:")
        for i, doc in enumerate(docs, 1):
            preview = doc.page_content[:100].replace('\n', ' ')
            print(f"  {i}. {preview}...")

        return docs

    def recall_with_scores(self, query: str, k: int = 10):
        """
        å¬å›æ–‡æ¡£å¹¶è¿”å›ç›¸ä¼¼åº¦åˆ†æ•°
        """
        docs_with_scores = self.vectorstore.similarity_search_with_score(
            query, k=k
        )

        print(f"\nå¬å›ç»“æœï¼ˆå¸¦ç›¸ä¼¼åº¦åˆ†æ•°ï¼‰:")
        for i, (doc, score) in enumerate(docs_with_scores, 1):
            preview = doc.page_content[:80].replace('\n', ' ')
            print(f"  {i}. ç›¸ä¼¼åº¦: {score:.4f} | {preview}...")

        return docs_with_scores

    def analyze_recall_quality(self, query: str, ground_truth_ids: list):
        """
        åˆ†æå¬å›è´¨é‡

        å‚æ•°:
            query: æŸ¥è¯¢
            ground_truth_ids: çœŸæ­£ç›¸å…³çš„æ–‡æ¡£IDåˆ—è¡¨
        """
        # å¬å›æ–‡æ¡£
        recalled_docs = self.recall_documents(query, k=20)
        recalled_ids = [doc.metadata.get('id') for doc in recalled_docs]

        # è®¡ç®—å¬å›ç‡
        recalled_relevant = len(set(recalled_ids) & set(ground_truth_ids))
        recall_rate = recalled_relevant / len(ground_truth_ids)

        # è®¡ç®—ç²¾ç¡®ç‡
        precision = recalled_relevant / len(recalled_docs)

        print(f"\nğŸ“Š å¬å›è´¨é‡åˆ†æ:")
        print(f"   çœŸæ­£ç›¸å…³çš„æ–‡æ¡£: {len(ground_truth_ids)} ä¸ª")
        print(f"   å¬å›çš„æ–‡æ¡£: {len(recalled_docs)} ä¸ª")
        print(f"   å¬å›çš„ç›¸å…³æ–‡æ¡£: {recalled_relevant} ä¸ª")
        print(f"   å¬å›ç‡: {recall_rate:.1%}")
        print(f"   ç²¾ç¡®ç‡: {precision:.1%}")

        return {
            'recall': recall_rate,
            'precision': precision
        }

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åŠ è½½æ–‡æ¡£
    loader = TextLoader("langchain_docs.txt", encoding="utf-8")
    documents = loader.load()

    # åˆ›å»ºRAGç³»ç»Ÿ
    rag = RAGWithRecall(documents)

    # æµ‹è¯•å¬å›
    query = "LangChain çš„æ ¸å¿ƒç»„ä»¶æœ‰å“ªäº›ï¼Ÿ"
    docs = rag.recall_documents(query, k=5)

    # æŸ¥çœ‹å¸¦åˆ†æ•°çš„å¬å›
    docs_with_scores = rag.recall_with_scores(query, k=5)
```


## äº”ã€å¬å›ä¼˜åŒ–æŠ€å·§

### 5.1 å¢åŠ å¬å›æ•°é‡ï¼ˆè°ƒæ•´ k å€¼ï¼‰

æœ€ç®€å•çš„ä¼˜åŒ–æ–¹æ³•ï¼šå¬å›æ›´å¤šå€™é€‰æ–‡æ¡£ã€‚

```python
"""
è°ƒæ•´å¬å›æ•°é‡
"""
# é»˜è®¤å¬å›
retriever_default = vectorstore.as_retriever(
    search_kwargs={"k": 4}  # é»˜è®¤4ä¸ª
)

# å¢åŠ å¬å›
retriever_more = vectorstore.as_retriever(
    search_kwargs={"k": 20}  # å¢åŠ åˆ°20ä¸ª
)

# å¯¹æ¯”æ•ˆæœ
query = "LangChain çš„æ ¸å¿ƒç»„ä»¶"

docs_default = retriever_default.get_relevant_documents(query)
docs_more = retriever_more.get_relevant_documents(query)

print(f"é»˜è®¤å¬å›: {len(docs_default)} ä¸ªæ–‡æ¡£")
print(f"å¢åŠ å¬å›: {len(docs_more)} ä¸ªæ–‡æ¡£")

# é€šå¸¸ï¼šå¬å›æ•°é‡ â†‘ â†’ å¬å›ç‡ â†‘ï¼Œä½†ç²¾ç¡®ç‡å¯èƒ½ â†“
```

**æ¨èç­–ç•¥ï¼š**

```python
"""
ä¸¤é˜¶æ®µå¬å›
"""
# ç¬¬1é˜¶æ®µï¼šå®½æ³›å¬å›ï¼ˆé«˜å¬å›ç‡ï¼‰
candidates = vectorstore.similarity_search(query, k=50)

# ç¬¬2é˜¶æ®µï¼šé‡æ’åºï¼ˆé«˜ç²¾ç¡®ç‡ï¼‰
top_results = rerank(candidates, query, k=5)
```

### 5.2 é‡æ’åºï¼ˆRe-rankingï¼‰

å¬å›å¤§é‡å€™é€‰åï¼Œç”¨æ›´ç²¾ç¡®çš„æ¨¡å‹é‡æ–°æ’åºã€‚

#### 5.2.1 åŸºäº LLM çš„é‡æ’åº

```python
"""
ä½¿ç”¨ LLM é‡æ’åº
"""
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain_openai import ChatOpenAI

class RerankerRetrieval:
    """å¸¦é‡æ’åºçš„æ£€ç´¢å™¨"""

    def __init__(self, documents):
        embeddings = OpenAIEmbeddings()
        vectorstore = FAISS.from_documents(documents, embeddings)

        # åŸºç¡€æ£€ç´¢å™¨ï¼ˆå®½æ³›å¬å›ï¼‰
        base_retriever = vectorstore.as_retriever(
            search_kwargs={"k": 20}  # å¬å›20ä¸ªå€™é€‰
        )

        # LLM é‡æ’åºå™¨
        llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
        compressor = LLMChainExtractor.from_llm(llm)

        # å‹ç¼©æ£€ç´¢å™¨ï¼ˆä¼šé‡æ’åºï¼‰
        self.retriever = ContextualCompressionRetriever(
            base_compressor=compressor,
            base_retriever=base_retriever
        )

    def search(self, query: str):
        """
        ä¸¤é˜¶æ®µæ£€ç´¢ï¼š
        1. å‘é‡å¬å› 20 ä¸ªå€™é€‰
        2. LLM é‡æ’åºï¼Œè¿”å›æœ€ç›¸å…³çš„
        """
        results = self.retriever.get_relevant_documents(query)
        return results

# ä½¿ç”¨
reranker = RerankerRetrieval(documents)
results = reranker.search("LangChain çš„æ ¸å¿ƒç»„ä»¶æ˜¯ä»€ä¹ˆï¼Ÿ")

print(f"é‡æ’åºåè¿”å› {len(results)} ä¸ªæ–‡æ¡£")
```

#### 5.2.2 åŸºäº Embedding çš„é‡æ’åº

```python
"""
ä½¿ç”¨ Embedding ç›¸ä¼¼åº¦é‡æ’åº
"""
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import EmbeddingsFilter

class EmbeddingReranker:
    """åŸºäº Embedding çš„é‡æ’åº"""

    def __init__(self, documents, similarity_threshold=0.7):
        embeddings = OpenAIEmbeddings()
        vectorstore = FAISS.from_documents(documents, embeddings)

        # åŸºç¡€æ£€ç´¢å™¨
        base_retriever = vectorstore.as_retriever(
            search_kwargs={"k": 30}  # å¬å›30ä¸ª
        )

        # Embedding è¿‡æ»¤å™¨ï¼ˆé‡æ’åºï¼‰
        embeddings_filter = EmbeddingsFilter(
            embeddings=embeddings,
            similarity_threshold=similarity_threshold  # ç›¸ä¼¼åº¦é˜ˆå€¼
        )

        # ç»„åˆ
        self.retriever = ContextualCompressionRetriever(
            base_compressor=embeddings_filter,
            base_retriever=base_retriever
        )

    def search(self, query: str):
        """
        æµç¨‹ï¼š
        1. å¬å›30ä¸ªå€™é€‰
        2. è®¡ç®—æ›´ç²¾ç¡®çš„ç›¸ä¼¼åº¦
        3. è¿‡æ»¤ä½äºé˜ˆå€¼çš„æ–‡æ¡£
        4. è¿”å›é«˜è´¨é‡ç»“æœ
        """
        return self.retriever.get_relevant_documents(query)

# ä½¿ç”¨
reranker = EmbeddingReranker(documents, similarity_threshold=0.75)
results = reranker.search("ä»€ä¹ˆæ˜¯ RAGï¼Ÿ")
```

### 5.3 å¤šè·¯å¬å›

ä»ä¸åŒæ¥æºå¬å›ï¼Œç„¶ååˆå¹¶å»é‡ã€‚

```python
"""
å¤šè·¯å¬å›ç­–ç•¥
"""
class MultiSourceRetrieval:
    """å¤šè·¯å¬å›æ£€ç´¢å™¨"""

    def __init__(self, vector_store, bm25_retriever, database_retriever=None):
        self.vector_retriever = vector_store.as_retriever(search_kwargs={"k": 10})
        self.bm25_retriever = bm25_retriever
        self.database_retriever = database_retriever

    def search(self, query: str):
        """
        å¤šè·¯å¬å›ç­–ç•¥
        """
        all_docs = []

        # è·¯å¾„1ï¼šå‘é‡æ£€ç´¢
        print("è·¯å¾„1ï¼šå‘é‡è¯­ä¹‰æ£€ç´¢...")
        vector_docs = self.vector_retriever.get_relevant_documents(query)
        all_docs.extend(vector_docs)
        print(f"  å¬å› {len(vector_docs)} ä¸ªæ–‡æ¡£")

        # è·¯å¾„2ï¼šå…³é”®è¯æ£€ç´¢
        print("è·¯å¾„2ï¼šå…³é”®è¯æ£€ç´¢...")
        keyword_docs = self.bm25_retriever.get_relevant_documents(query)
        all_docs.extend(keyword_docs)
        print(f"  å¬å› {len(keyword_docs)} ä¸ªæ–‡æ¡£")

        # è·¯å¾„3ï¼šæ•°æ®åº“æŸ¥è¯¢ï¼ˆå¯é€‰ï¼‰
        if self.database_retriever:
            print("è·¯å¾„3ï¼šæ•°æ®åº“æŸ¥è¯¢...")
            db_docs = self.database_retriever.get_relevant_documents(query)
            all_docs.extend(db_docs)
            print(f"  å¬å› {len(db_docs)} ä¸ªæ–‡æ¡£")

        # å»é‡ï¼ˆåŸºäºå†…å®¹hashï¼‰
        unique_docs = self._deduplicate(all_docs)

        print(f"\næ€»è®¡å¬å› {len(all_docs)} ä¸ªæ–‡æ¡£")
        print(f"å»é‡åå‰©ä½™ {len(unique_docs)} ä¸ªæ–‡æ¡£")

        return unique_docs

    def _deduplicate(self, documents):
        """å»é‡"""
        seen = set()
        unique = []

        for doc in documents:
            # ä½¿ç”¨å†…å®¹hashå»é‡
            content_hash = hash(doc.page_content)
            if content_hash not in seen:
                seen.add(content_hash)
                unique.append(doc)

        return unique

# ä½¿ç”¨
multi_retriever = MultiSourceRetrieval(
    vector_store=vectorstore,
    bm25_retriever=bm25_retriever
)

results = multi_retriever.search("LangChain æ•™ç¨‹")
```

**è¾“å‡ºç¤ºä¾‹ï¼š**
```
è·¯å¾„1ï¼šå‘é‡è¯­ä¹‰æ£€ç´¢...
  å¬å› 10 ä¸ªæ–‡æ¡£
è·¯å¾„2ï¼šå…³é”®è¯æ£€ç´¢...
  å¬å› 10 ä¸ªæ–‡æ¡£
è·¯å¾„3ï¼šæ•°æ®åº“æŸ¥è¯¢...
  å¬å› 5 ä¸ªæ–‡æ¡£

æ€»è®¡å¬å› 25 ä¸ªæ–‡æ¡£
å»é‡åå‰©ä½™ 18 ä¸ªæ–‡æ¡£
```

### 5.4 æŸ¥è¯¢æ‰©å±•

æ‰©å±•ç”¨æˆ·æŸ¥è¯¢ï¼Œæé«˜å¬å›æ•ˆæœã€‚

```python
"""
æŸ¥è¯¢æ‰©å±•æŠ€æœ¯
"""
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

class QueryExpansion:
    """æŸ¥è¯¢æ‰©å±•"""

    def __init__(self, retriever):
        self.retriever = retriever
        self.llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

    def expand_query(self, query: str):
        """
        ä½¿ç”¨ LLM ç”ŸæˆæŸ¥è¯¢å˜ä½“
        """
        prompt = f"""åŸå§‹æŸ¥è¯¢: {query}

è¯·ç”Ÿæˆ3ä¸ªæ„æ€ç›¸è¿‘ä½†è¡¨è¿°ä¸åŒçš„æŸ¥è¯¢å˜ä½“ï¼š
1.
2.
3.
"""
        response = self.llm.invoke([HumanMessage(content=prompt)])

        # è§£æå˜ä½“ï¼ˆç®€åŒ–ç‰ˆï¼‰
        lines = response.content.strip().split('\n')
        variants = [line.split('. ', 1)[1] for line in lines if '. ' in line]

        return [query] + variants

    def search_with_expansion(self, query: str, k: int = 5):
        """
        ä½¿ç”¨æŸ¥è¯¢æ‰©å±•è¿›è¡Œæ£€ç´¢
        """
        # ç”ŸæˆæŸ¥è¯¢å˜ä½“
        queries = self.expand_query(query)
        print(f"åŸå§‹æŸ¥è¯¢: {query}")
        print(f"æ‰©å±•æŸ¥è¯¢: {queries[1:]}\n")

        # å¯¹æ¯ä¸ªæŸ¥è¯¢è¿›è¡Œæ£€ç´¢
        all_docs = []
        for q in queries:
            docs = self.retriever.get_relevant_documents(q)
            all_docs.extend(docs)

        # å»é‡å¹¶è¿”å›
        unique_docs = self._deduplicate(all_docs)
        return unique_docs[:k]

    def _deduplicate(self, documents):
        """å»é‡"""
        seen = set()
        unique = []
        for doc in documents:
            if doc.page_content not in seen:
                seen.add(doc.page_content)
                unique.append(doc)
        return unique

# ä½¿ç”¨
expander = QueryExpansion(retriever)
results = expander.search_with_expansion("LangChainæ€ä¹ˆç”¨ï¼Ÿ", k=5)
```

### 5.5 è¿‡æ»¤å’Œåå¤„ç†

å¬å›åè¿›è¡Œè´¨é‡è¿‡æ»¤ã€‚

```python
"""
å¬å›ç»“æœåå¤„ç†
"""
class RecallPostProcessor:
    """å¬å›åå¤„ç†å™¨"""

    def __init__(self, retriever):
        self.retriever = retriever

    def search_with_filters(
        self,
        query: str,
        min_length: int = 50,
        max_length: int = 2000,
        exclude_keywords: list = None
    ):
        """
        å¸¦è¿‡æ»¤çš„æ£€ç´¢

        å‚æ•°:
            query: æŸ¥è¯¢
            min_length: æœ€å°æ–‡æ¡£é•¿åº¦
            max_length: æœ€å¤§æ–‡æ¡£é•¿åº¦
            exclude_keywords: æ’é™¤åŒ…å«è¿™äº›å…³é”®è¯çš„æ–‡æ¡£
        """
        # å¬å›
        docs = self.retriever.get_relevant_documents(query)

        # è¿‡æ»¤
        filtered = []
        for doc in docs:
            content = doc.page_content

            # é•¿åº¦è¿‡æ»¤
            if len(content) < min_length or len(content) > max_length:
                continue

            # å…³é”®è¯è¿‡æ»¤
            if exclude_keywords:
                if any(kw in content for kw in exclude_keywords):
                    continue

            filtered.append(doc)

        print(f"å¬å› {len(docs)} ä¸ªæ–‡æ¡£")
        print(f"è¿‡æ»¤åå‰©ä½™ {len(filtered)} ä¸ªæ–‡æ¡£")

        return filtered

# ä½¿ç”¨
processor = RecallPostProcessor(retriever)

results = processor.search_with_filters(
    query="Python æ•™ç¨‹",
    min_length=100,           # è‡³å°‘100å­—ç¬¦
    max_length=1000,          # æœ€å¤š1000å­—ç¬¦
    exclude_keywords=["å¹¿å‘Š", "æ¨å¹¿"]  # æ’é™¤å¹¿å‘Š
)
```


## ä¸ƒã€å®æˆ˜æ¡ˆä¾‹

### 7.1 æ„å»ºé«˜è´¨é‡å¬å›ç³»ç»Ÿ

```python
"""
å®Œæ•´çš„é«˜è´¨é‡å¬å›ç³»ç»Ÿ
"""
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever
from langchain.retrievers.document_compressors import EmbeddingsFilter
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from typing import List, Dict
import time

class ProductionRecallSystem:
    """ç”Ÿäº§çº§å¬å›ç³»ç»Ÿ"""

    def __init__(
        self,
        documents_path: str,
        chunk_size: int = 500,
        chunk_overlap: int = 50
    ):
        """
        åˆå§‹åŒ–å¬å›ç³»ç»Ÿ

        å‚æ•°:
            documents_path: æ–‡æ¡£ç›®å½•è·¯å¾„
            chunk_size: æ–‡æ¡£åˆ†å—å¤§å°
            chunk_overlap: åˆ†å—é‡å å¤§å°
        """
        print("ğŸš€ åˆå§‹åŒ–å¬å›ç³»ç»Ÿ...")

        # 1. åŠ è½½æ–‡æ¡£
        print("  [1/5] åŠ è½½æ–‡æ¡£...")
        loader = DirectoryLoader(
            documents_path,
            glob="**/*.txt",
            loader_cls=TextLoader,
            loader_kwargs={'encoding': 'utf-8'}
        )
        documents = loader.load()
        print(f"    åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")

        # 2. æ–‡æ¡£åˆ†å‰²
        print("  [2/5] åˆ†å‰²æ–‡æ¡£...")
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        self.chunks = splitter.split_documents(documents)
        print(f"    åˆ†å‰²æˆ {len(self.chunks)} ä¸ªå—")

        # 3. åˆ›å»ºå‘é‡å­˜å‚¨
        print("  [3/5] åˆ›å»ºå‘é‡ç´¢å¼•...")
        self.embeddings = OpenAIEmbeddings()
        self.vectorstore = FAISS.from_documents(
            self.chunks,
            self.embeddings
        )
        print("    âœ… å‘é‡ç´¢å¼•åˆ›å»ºå®Œæˆ")

        # 4. åˆ›å»ºå¤šå±‚æ£€ç´¢å™¨
        print("  [4/5] é…ç½®æ£€ç´¢ç­–ç•¥...")
        self._setup_retrievers()
        print("    âœ… æ£€ç´¢å™¨é…ç½®å®Œæˆ")

        # 5. æ€§èƒ½ç›‘æ§
        print("  [5/5] å¯åŠ¨ç›‘æ§...")
        self.stats = {
            'total_queries': 0,
            'total_time': 0,
            'recalls': []
        }
        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\n")

    def _setup_retrievers(self):
        """é…ç½®å¤šå±‚æ£€ç´¢ç­–ç•¥"""

        # ç¬¬1å±‚ï¼šå®½æ³›å¬å›ï¼ˆé«˜å¬å›ç‡ï¼‰
        self.broad_retriever = self.vectorstore.as_retriever(
            search_kwargs={"k": 50}  # å¬å›50ä¸ªå€™é€‰
        )

        # ç¬¬2å±‚ï¼šç²¾ç¡®è¿‡æ»¤ï¼ˆé«˜ç²¾ç¡®ç‡ï¼‰
        embeddings_filter = EmbeddingsFilter(
            embeddings=self.embeddings,
            similarity_threshold=0.7
        )

        self.precise_retriever = ContextualCompressionRetriever(
            base_compressor=embeddings_filter,
            base_retriever=self.broad_retriever
        )

    def recall(
        self,
        query: str,
        k: int = 5,
        strategy: str = "precise"
    ) -> List:
        """
        å¬å›æ–‡æ¡£

        å‚æ•°:
            query: æŸ¥è¯¢
            k: è¿”å›æ•°é‡
            strategy: å¬å›ç­–ç•¥
                - "broad": å®½æ³›å¬å›ï¼ˆé«˜å¬å›ç‡ï¼‰
                - "precise": ç²¾ç¡®å¬å›ï¼ˆé«˜ç²¾ç¡®ç‡ï¼‰

        è¿”å›:
            å¬å›çš„æ–‡æ¡£åˆ—è¡¨
        """
        start_time = time.time()

        # é€‰æ‹©ç­–ç•¥
        if strategy == "broad":
            docs = self.broad_retriever.get_relevant_documents(query)
        else:  # precise
            docs = self.precise_retriever.get_relevant_documents(query)

        # é™åˆ¶è¿”å›æ•°é‡
        results = docs[:k]

        # è®°å½•ç»Ÿè®¡
        elapsed = time.time() - start_time
        self.stats['total_queries'] += 1
        self.stats['total_time'] += elapsed

        return results

    def recall_with_details(
        self,
        query: str,
        k: int = 5
    ) -> Dict:
        """
        å¬å›å¹¶è¿”å›è¯¦ç»†ä¿¡æ¯
        """
        start_time = time.time()

        # å¬å›
        docs = self.precise_retriever.get_relevant_documents(query)
        results = docs[:k]

        # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°
        docs_with_scores = self.vectorstore.similarity_search_with_score(
            query, k=k
        )

        elapsed = time.time() - start_time

        return {
            'query': query,
            'documents': results,
            'scores': [score for _, score in docs_with_scores],
            'count': len(results),
            'elapsed_time': elapsed
        }

    def get_stats(self) -> Dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        avg_time = (
            self.stats['total_time'] / self.stats['total_queries']
            if self.stats['total_queries'] > 0 else 0
        )

        return {
            'total_queries': self.stats['total_queries'],
            'total_time': self.stats['total_time'],
            'average_time': avg_time
        }

    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_stats()

        print("=" * 60)
        print("å¬å›ç³»ç»Ÿæ€§èƒ½ç»Ÿè®¡")
        print("=" * 60)
        print(f"æ€»æŸ¥è¯¢æ•°: {stats['total_queries']}")
        print(f"æ€»è€—æ—¶: {stats['total_time']:.2f}ç§’")
        print(f"å¹³å‡è€—æ—¶: {stats['average_time']:.3f}ç§’/æŸ¥è¯¢")
        print("=" * 60)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆå§‹åŒ–ç³»ç»Ÿ
    recall_system = ProductionRecallSystem(
        documents_path="./docs",
        chunk_size=500,
        chunk_overlap=50
    )

    # æµ‹è¯•å¬å›
    queries = [
        "LangChain çš„æ ¸å¿ƒç»„ä»¶æœ‰å“ªäº›ï¼Ÿ",
        "å¦‚ä½•å®ç° RAG ç³»ç»Ÿï¼Ÿ",
        "Agents çš„å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Ÿ"
    ]

    for query in queries:
        print(f"\næŸ¥è¯¢: {query}")
        print("-" * 60)

        # å¬å›
        result = recall_system.recall_with_details(query, k=3)

        print(f"å¬å›æ•°é‡: {result['count']}")
        print(f"è€—æ—¶: {result['elapsed_time']:.3f}ç§’")
        print("\nå¬å›ç»“æœ:")

        for i, (doc, score) in enumerate(zip(result['documents'], result['scores']), 1):
            preview = doc.page_content[:100].replace('\n', ' ')
            print(f"  {i}. [ç›¸ä¼¼åº¦: {score:.4f}] {preview}...")

    # æ‰“å°ç»Ÿè®¡
    print("\n")
    recall_system.print_stats()
```


## ä¹ã€å¸¸è§é—®é¢˜ FAQ

### Q1: å¬å›ç‡å’Œç²¾ç¡®ç‡å“ªä¸ªæ›´é‡è¦ï¼Ÿ

**ç­”ï¼š** å–å†³äºåº”ç”¨åœºæ™¯ã€‚

- **æœç´¢å¼•æ“**ï¼šå¬å›ç‡æ›´é‡è¦
  - ç”¨æˆ·æœŸæœ›çœ‹åˆ°æ‰€æœ‰ç›¸å…³ç»“æœ
  - å¯ä»¥é€šè¿‡æ’åºæå‡å‰æ’è´¨é‡

- **é—®ç­”ç³»ç»Ÿ**ï¼šç²¾ç¡®ç‡æ›´é‡è¦
  - é”™è¯¯ç­”æ¡ˆä¼šé™ä½ä¿¡ä»»åº¦
  - å®å¯è¯´"ä¸çŸ¥é“"ï¼Œä¸è¦ç»™é”™è¯¯ä¿¡æ¯

- **æ¨èç³»ç»Ÿ**ï¼šéœ€è¦å¹³è¡¡
  - æ—¢è¦è¦†ç›–ç”¨æˆ·å…´è¶£ï¼Œåˆè¦ä¿è¯æ¨èè´¨é‡

### Q2: å¦‚ä½•æé«˜å¬å›ç‡ï¼Ÿ

**æ–¹æ³•ï¼š**
1. å¢åŠ å¬å›æ•°é‡ï¼ˆkå€¼ï¼‰
2. é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼
3. ä½¿ç”¨æŸ¥è¯¢æ‰©å±•
4. å¤šè·¯å¬å›åˆå¹¶
5. ä¼˜åŒ–æ–‡æ¡£åˆ†å‰²ç­–ç•¥

### Q3: å¬å›å¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ

**ä¼˜åŒ–æ–¹æ¡ˆï¼š**
1. ä½¿ç”¨æ›´å¿«çš„å‘é‡æ•°æ®åº“ï¼ˆå¦‚ FAISS + HNSWï¼‰
2. å‡å° embedding ç»´åº¦
3. ç¼“å­˜çƒ­é—¨æŸ¥è¯¢
4. å¼‚æ­¥å¬å›
5. é¢„å¬å› + ç¼“å­˜

### Q4: å¦‚ä½•è¯„ä¼°å¬å›æ•ˆæœï¼Ÿ

**è¯„ä¼°æ–¹æ³•ï¼š**
1. å‡†å¤‡æµ‹è¯•é›†ï¼ˆæŸ¥è¯¢ + ç›¸å…³æ–‡æ¡£IDï¼‰
2. è®¡ç®—å¬å›ç‡ã€ç²¾ç¡®ç‡ã€F1
3. è¿›è¡Œ A/B æµ‹è¯•
4. ç”¨æˆ·åé¦ˆæ”¶é›†

### Q5: æ··åˆæ£€ç´¢çš„æƒé‡å¦‚ä½•è°ƒæ•´ï¼Ÿ

**è°ƒæ•´ç­–ç•¥ï¼š**
```python
# æ ¹æ®æŸ¥è¯¢ç±»å‹åŠ¨æ€è°ƒæ•´
if is_concept_query(query):
    weights = [0.7, 0.3]  # è¯­ä¹‰ä¸ºä¸»
elif is_keyword_query(query):
    weights = [0.3, 0.7]  # å…³é”®è¯ä¸ºä¸»
else:
    weights = [0.5, 0.5]  # å‡è¡¡
```


## æ€»ç»“

**å¬å›çš„æœ¬è´¨ï¼š**
- ğŸ“š æ‰¾åˆ°æ‰€æœ‰å¯èƒ½ç›¸å…³çš„ä¿¡æ¯ï¼ˆé«˜å¬å›ç‡ï¼‰
- ğŸ¯ ç”¨é‡æ’åºæå‡ç»“æœè´¨é‡ï¼ˆé«˜ç²¾ç¡®ç‡ï¼‰
- âš–ï¸ åœ¨å¬å›å’Œç²¾ç¡®ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ç‚¹

**å…³é”®è¦ç‚¹ï¼š**
1. å¬å›æ˜¯ RAG ç³»ç»Ÿçš„åŸºç¡€ï¼Œå†³å®šäº†ç­”æ¡ˆçš„ä¸Šé™
2. æ¨èä½¿ç”¨ä¸¤é˜¶æ®µç­–ç•¥ï¼šå®½æ³›å¬å› + ç²¾ç¡®é‡æ’
3. æ··åˆæ£€ç´¢é€šå¸¸ä¼˜äºå•ä¸€ç­–ç•¥
4. éœ€è¦æ ¹æ®åœºæ™¯è°ƒæ•´å‚æ•°å’Œç­–ç•¥
5. æŒç»­ç›‘æ§å’Œä¼˜åŒ–å¬å›æ•ˆæœ

**ä¸‹ä¸€æ­¥å­¦ä¹ ï¼š**
- æ·±å…¥å­¦ä¹ å‘é‡æ•°æ®åº“ä¼˜åŒ–
- ç ”ç©¶é«˜çº§é‡æ’åºæŠ€æœ¯
- æ¢ç´¢å¤šæ¨¡æ€å¬å›ï¼ˆæ–‡æœ¬+å›¾ç‰‡ï¼‰
- å®è·µå¤§è§„æ¨¡æ£€ç´¢ç³»ç»Ÿ

---

**ç¥å­¦ä¹ é¡ºåˆ©ï¼** ğŸ¯

å¦‚æœ‰é—®é¢˜ï¼Œæ¬¢è¿åœ¨ç¤¾åŒºè®¨è®ºï¼
