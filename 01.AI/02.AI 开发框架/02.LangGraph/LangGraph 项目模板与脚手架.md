---
title: LangGraph é¡¹ç›®æ¨¡æ¿ä¸è„šæ‰‹æ¶
date: 2025-09-30
permalink: /ai/langgraph/project-template.html
categories:
  - AI
  - LangGraph
---

# LangGraph é¡¹ç›®æ¨¡æ¿ä¸è„šæ‰‹æ¶

## æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›å®Œæ•´çš„ LangGraph é¡¹ç›®æ¨¡æ¿å’Œè„šæ‰‹æ¶å·¥å…·ï¼Œå¸®åŠ©å¿«é€Ÿå¯åŠ¨æ–°é¡¹ç›®ã€‚

```mermaid
graph TB
    subgraph "é¡¹ç›®ç»“æ„"
        A[é¡¹ç›®æ ¹ç›®å½•] --> B[src/]
        A --> C[tests/]
        A --> D[configs/]
        A --> E[docs/]
        A --> F[scripts/]

        B --> G[workflows/]
        B --> H[nodes/]
        B --> I[utils/]
        B --> J[api/]

        C --> K[unit/]
        C --> L[integration/]
        C --> M[e2e/]
    end

    style A fill:#f9f,stroke:#333,stroke-width:4px
    style B fill:#9cf,stroke:#333,stroke-width:2px
    style G fill:#fc9,stroke:#333,stroke-width:2px
```

## 1. åŸºç¡€é¡¹ç›®æ¨¡æ¿

### 1.1 é¡¹ç›®ç›®å½•ç»“æ„

```bash
langgraph-project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ workflows/           # å·¥ä½œæµå®šä¹‰
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py         # åŸºç¡€å·¥ä½œæµç±»
â”‚   â”‚   â”œâ”€â”€ main_workflow.py
â”‚   â”‚   â””â”€â”€ sub_workflows/
â”‚   â”œâ”€â”€ nodes/              # èŠ‚ç‚¹å®ç°
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ llm_nodes.py
â”‚   â”‚   â”œâ”€â”€ tool_nodes.py
â”‚   â”‚   â”œâ”€â”€ validation_nodes.py
â”‚   â”‚   â””â”€â”€ custom_nodes.py
â”‚   â”œâ”€â”€ states/             # çŠ¶æ€å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_state.py
â”‚   â”‚   â””â”€â”€ workflow_states.py
â”‚   â”œâ”€â”€ utils/              # å·¥å…·å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚   â”‚   â”œâ”€â”€ validators.py
â”‚   â”‚   â””â”€â”€ helpers.py
â”‚   â”œâ”€â”€ api/                # APIæ¥å£
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ app.py
â”‚   â”‚   â”œâ”€â”€ routes.py
â”‚   â”‚   â””â”€â”€ models.py
â”‚   â””â”€â”€ config/             # é…ç½®ç®¡ç†
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ settings.py
â”‚       â””â”€â”€ constants.py
â”œâ”€â”€ tests/                   # æµ‹è¯•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ fixtures/
â”œâ”€â”€ configs/                 # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ development.yaml
â”‚   â”œâ”€â”€ production.yaml
â”‚   â””â”€â”€ test.yaml
â”œâ”€â”€ docs/                    # æ–‡æ¡£
â”‚   â”œâ”€â”€ API.md
â”‚   â”œâ”€â”€ ARCHITECTURE.md
â”‚   â””â”€â”€ DEPLOYMENT.md
â”œâ”€â”€ scripts/                 # è„šæœ¬
â”‚   â”œâ”€â”€ setup.sh
â”‚   â”œâ”€â”€ deploy.sh
â”‚   â””â”€â”€ test.sh
â”œâ”€â”€ docker/                  # Dockerç›¸å…³
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example            # ç¯å¢ƒå˜é‡ç¤ºä¾‹
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements-dev.txt
â”œâ”€â”€ pytest.ini
â”œâ”€â”€ Makefile
â””â”€â”€ README.md
```

### 1.2 åŸºç¡€å·¥ä½œæµç±»

```python
# src/workflows/base.py
from abc import ABC, abstractmethod
from typing import TypedDict, Any, Dict, Optional
from langgraph.graph import StateGraph, END
from langgraph.checkpoint import BaseCheckpointSaver
import logging

logger = logging.getLogger(__name__)

class BaseWorkflow(ABC):
    """åŸºç¡€å·¥ä½œæµæŠ½è±¡ç±»"""

    def __init__(
        self,
        name: str,
        state_class: type[TypedDict],
        checkpointer: Optional[BaseCheckpointSaver] = None
    ):
        self.name = name
        self.state_class = state_class
        self.checkpointer = checkpointer
        self.workflow = StateGraph(state_class)
        self.app = None
        self._setup_workflow()

    @abstractmethod
    def _setup_workflow(self):
        """è®¾ç½®å·¥ä½œæµç»“æ„"""
        pass

    @abstractmethod
    def _add_nodes(self):
        """æ·»åŠ èŠ‚ç‚¹"""
        pass

    @abstractmethod
    def _add_edges(self):
        """æ·»åŠ è¾¹"""
        pass

    def compile(self) -> Any:
        """ç¼–è¯‘å·¥ä½œæµ"""
        if self.app is None:
            self.app = self.workflow.compile(checkpointer=self.checkpointer)
            logger.info(f"Workflow '{self.name}' compiled successfully")
        return self.app

    def invoke(self, input_state: Dict, config: Optional[Dict] = None) -> Dict:
        """æ‰§è¡Œå·¥ä½œæµ"""
        if self.app is None:
            self.compile()

        logger.info(f"Invoking workflow '{self.name}'")
        try:
            result = self.app.invoke(input_state, config)
            logger.info(f"Workflow '{self.name}' completed successfully")
            return result
        except Exception as e:
            logger.error(f"Workflow '{self.name}' failed: {e}")
            raise

    async def ainvoke(self, input_state: Dict, config: Optional[Dict] = None) -> Dict:
        """å¼‚æ­¥æ‰§è¡Œå·¥ä½œæµ"""
        if self.app is None:
            self.compile()

        logger.info(f"Async invoking workflow '{self.name}'")
        try:
            result = await self.app.ainvoke(input_state, config)
            logger.info(f"Workflow '{self.name}' completed successfully")
            return result
        except Exception as e:
            logger.error(f"Workflow '{self.name}' failed: {e}")
            raise

    def get_graph_image(self) -> bytes:
        """è·å–å·¥ä½œæµå›¾åƒ"""
        if self.app is None:
            self.compile()
        return self.app.get_graph().draw_mermaid_png()
```

### 1.3 çŠ¶æ€ç®¡ç†æ¨¡æ¿

```python
# src/states/base_state.py
from typing import TypedDict, List, Dict, Optional, Annotated, Any
from operator import add
from datetime import datetime

class BaseState(TypedDict):
    """åŸºç¡€çŠ¶æ€ç±»"""
    # åŸºç¡€å­—æ®µ
    id: str
    created_at: datetime
    updated_at: datetime

    # æ‰§è¡ŒçŠ¶æ€
    status: str  # pending, running, completed, failed
    error: Optional[str]

    # è¿½è¸ªä¿¡æ¯
    execution_path: Annotated[List[str], add]
    metrics: Dict[str, Any]

class WorkflowState(BaseState):
    """å·¥ä½œæµçŠ¶æ€"""
    # è¾“å…¥è¾“å‡º
    input_data: Dict
    output_data: Dict

    # ä¸­é—´ç»“æœ
    intermediate_results: Annotated[List[Dict], add]

    # ä¸Šä¸‹æ–‡
    context: Dict
    metadata: Dict

class ConversationState(BaseState):
    """å¯¹è¯çŠ¶æ€"""
    messages: Annotated[List[Dict], add]
    user_info: Dict
    session_id: str
    turn_count: int
    conversation_summary: Optional[str]

class ProcessingState(BaseState):
    """å¤„ç†çŠ¶æ€"""
    documents: List[Dict]
    processed_chunks: Annotated[List[Dict], add]
    embeddings: Optional[List[List[float]]]
    search_results: List[Dict]
    final_result: Optional[str]
```

### 1.4 é…ç½®ç®¡ç†

```python
# src/config/settings.py
from pydantic_settings import BaseSettings
from pydantic import Field, validator
from typing import Optional, List, Dict
import os
from pathlib import Path

class Settings(BaseSettings):
    """åº”ç”¨é…ç½®"""

    # åŸºç¡€é…ç½®
    app_name: str = Field(default="LangGraph App", env="APP_NAME")
    environment: str = Field(default="development", env="ENVIRONMENT")
    debug: bool = Field(default=False, env="DEBUG")
    log_level: str = Field(default="INFO", env="LOG_LEVEL")

    # APIé…ç½®
    api_host: str = Field(default="0.0.0.0", env="API_HOST")
    api_port: int = Field(default=8000, env="API_PORT")
    api_prefix: str = Field(default="/api/v1", env="API_PREFIX")
    cors_origins: List[str] = Field(default=["*"], env="CORS_ORIGINS")

    # LLMé…ç½®
    llm_provider: str = Field(default="openai", env="LLM_PROVIDER")
    llm_model: str = Field(default="gpt-3.5-turbo", env="LLM_MODEL")
    llm_temperature: float = Field(default=0.7, env="LLM_TEMPERATURE")
    llm_max_tokens: int = Field(default=2000, env="LLM_MAX_TOKENS")
    openai_api_key: str = Field(..., env="OPENAI_API_KEY")

    # æ•°æ®åº“é…ç½®
    database_url: str = Field(
        default="sqlite:///./app.db",
        env="DATABASE_URL"
    )
    redis_url: Optional[str] = Field(default=None, env="REDIS_URL")

    # å‘é‡æ•°æ®åº“é…ç½®
    vector_db_type: str = Field(default="chroma", env="VECTOR_DB_TYPE")
    vector_db_url: Optional[str] = Field(default=None, env="VECTOR_DB_URL")
    embedding_model: str = Field(
        default="text-embedding-ada-002",
        env="EMBEDDING_MODEL"
    )

    # æ€§èƒ½é…ç½®
    max_concurrent_workflows: int = Field(default=10, env="MAX_CONCURRENT_WORKFLOWS")
    workflow_timeout: int = Field(default=300, env="WORKFLOW_TIMEOUT")
    cache_ttl: int = Field(default=3600, env="CACHE_TTL")

    # å®‰å…¨é…ç½®
    secret_key: str = Field(..., env="SECRET_KEY")
    api_keys: List[str] = Field(default=[], env="API_KEYS")
    enable_auth: bool = Field(default=True, env="ENABLE_AUTH")

    @validator("cors_origins", pre=True)
    def parse_cors_origins(cls, v):
        if isinstance(v, str):
            return [origin.strip() for origin in v.split(",")]
        return v

    @validator("api_keys", pre=True)
    def parse_api_keys(cls, v):
        if isinstance(v, str):
            return [key.strip() for key in v.split(",")]
        return v

    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
        case_sensitive = False

# å•ä¾‹æ¨¡å¼
_settings = None

def get_settings() -> Settings:
    global _settings
    if _settings is None:
        _settings = Settings()
    return _settings
```

## 2. CLI è„šæ‰‹æ¶å·¥å…·

### 2.1 è„šæ‰‹æ¶ä¸»ç¨‹åº

```python
# scripts/langgraph_cli.py
#!/usr/bin/env python3
"""
LangGraph CLI - é¡¹ç›®è„šæ‰‹æ¶å·¥å…·
"""

import click
import os
import shutil
from pathlib import Path
import subprocess
import yaml
import json

TEMPLATE_DIR = Path(__file__).parent / "templates"

@click.group()
def cli():
    """LangGraph é¡¹ç›®è„šæ‰‹æ¶å·¥å…·"""
    pass

@cli.command()
@click.argument("project_name")
@click.option("--template", default="basic", help="é¡¹ç›®æ¨¡æ¿ç±»å‹")
@click.option("--llm", default="openai", help="LLMæä¾›å•†")
@click.option("--with-docker", is_flag=True, help="åŒ…å«Dockeré…ç½®")
@click.option("--with-tests", is_flag=True, help="åŒ…å«æµ‹è¯•æ¡†æ¶")
def new(project_name, template, llm, with_docker, with_tests):
    """åˆ›å»ºæ–°çš„LangGraphé¡¹ç›®"""
    click.echo(f"Creating new LangGraph project: {project_name}")

    # åˆ›å»ºé¡¹ç›®ç›®å½•
    project_path = Path(project_name)
    if project_path.exists():
        click.echo(f"Error: Directory {project_name} already exists", err=True)
        return

    project_path.mkdir()

    # å¤åˆ¶æ¨¡æ¿æ–‡ä»¶
    template_path = TEMPLATE_DIR / template
    if not template_path.exists():
        click.echo(f"Error: Template {template} not found", err=True)
        return

    # å¤åˆ¶åŸºç¡€ç»“æ„
    for item in template_path.iterdir():
        if item.is_dir():
            shutil.copytree(item, project_path / item.name)
        else:
            shutil.copy2(item, project_path)

    # ç”Ÿæˆé…ç½®æ–‡ä»¶
    generate_config(project_path, project_name, llm)

    # æ·»åŠ Dockeræ”¯æŒ
    if with_docker:
        add_docker_support(project_path, project_name)

    # æ·»åŠ æµ‹è¯•æ¡†æ¶
    if with_tests:
        add_test_framework(project_path)

    # åˆå§‹åŒ–gitä»“åº“
    subprocess.run(["git", "init"], cwd=project_path, check=True)

    # å®‰è£…ä¾èµ–
    click.echo("Installing dependencies...")
    subprocess.run(["pip", "install", "-r", "requirements.txt"], cwd=project_path)

    click.echo(f"""
âœ… Project {project_name} created successfully!

Next steps:
1. cd {project_name}
2. cp .env.example .env
3. Edit .env with your configuration
4. python -m src.api.app (to start the server)

Happy coding! ğŸš€
    """)

@cli.command()
@click.option("--name", prompt="Workflow name", help="å·¥ä½œæµåç§°")
@click.option("--type", default="basic", help="å·¥ä½œæµç±»å‹")
def add_workflow(name, type):
    """æ·»åŠ æ–°çš„å·¥ä½œæµ"""
    click.echo(f"Adding workflow: {name}")

    workflow_file = f"src/workflows/{name.lower()}_workflow.py"
    state_file = f"src/states/{name.lower()}_state.py"

    # ç”Ÿæˆå·¥ä½œæµä»£ç 
    workflow_code = generate_workflow_code(name, type)
    with open(workflow_file, 'w') as f:
        f.write(workflow_code)

    # ç”ŸæˆçŠ¶æ€ä»£ç 
    state_code = generate_state_code(name)
    with open(state_file, 'w') as f:
        f.write(state_code)

    click.echo(f"âœ… Workflow {name} added successfully!")

@cli.command()
@click.option("--name", prompt="Node name", help="èŠ‚ç‚¹åç§°")
@click.option("--type", default="process", help="èŠ‚ç‚¹ç±»å‹")
def add_node(name, type):
    """æ·»åŠ æ–°çš„èŠ‚ç‚¹"""
    click.echo(f"Adding node: {name}")

    node_code = generate_node_code(name, type)

    # æ·»åŠ åˆ°ç›¸åº”çš„èŠ‚ç‚¹æ–‡ä»¶
    node_file = f"src/nodes/{type}_nodes.py"

    with open(node_file, 'a') as f:
        f.write(f"\n\n{node_code}")

    click.echo(f"âœ… Node {name} added successfully!")

def generate_config(project_path: Path, project_name: str, llm: str):
    """ç”Ÿæˆé…ç½®æ–‡ä»¶"""

    # .env.example
    env_content = f"""
# Application
APP_NAME={project_name}
ENVIRONMENT=development
DEBUG=True
LOG_LEVEL=INFO

# API
API_HOST=0.0.0.0
API_PORT=8000
API_PREFIX=/api/v1
CORS_ORIGINS=*

# LLM
LLM_PROVIDER={llm}
LLM_MODEL=gpt-3.5-turbo
LLM_TEMPERATURE=0.7
OPENAI_API_KEY=your_api_key_here

# Database
DATABASE_URL=sqlite:///./app.db
REDIS_URL=redis://localhost:6379

# Security
SECRET_KEY=your_secret_key_here
API_KEYS=
ENABLE_AUTH=False
"""

    with open(project_path / ".env.example", 'w') as f:
        f.write(env_content.strip())

    # requirements.txt
    requirements = """
langgraph>=0.0.20
langchain>=0.1.0
langchain-openai>=0.0.5
pydantic>=2.0.0
pydantic-settings>=2.0.0
fastapi>=0.104.0
uvicorn>=0.24.0
python-dotenv>=1.0.0
redis>=5.0.0
sqlalchemy>=2.0.0
chromadb>=0.4.0
pytest>=7.4.0
pytest-asyncio>=0.21.0
pytest-cov>=4.1.0
black>=23.0.0
"""

    with open(project_path / "requirements.txt", 'w') as f:
        f.write(requirements.strip())

def generate_workflow_code(name: str, workflow_type: str) -> str:
    """ç”Ÿæˆå·¥ä½œæµä»£ç """

    return f"""
from typing import Dict, Optional
from langgraph.graph import END
from src.workflows.base import BaseWorkflow
from src.states.{name.lower()}_state import {name}State
import logging

logger = logging.getLogger(__name__)

class {name}Workflow(BaseWorkflow):
    \"\"\"
    {name} å·¥ä½œæµå®ç°
    \"\"\"

    def __init__(self, checkpointer=None):
        super().__init__(
            name="{name}",
            state_class={name}State,
            checkpointer=checkpointer
        )

    def _setup_workflow(self):
        \"\"\"è®¾ç½®å·¥ä½œæµ\"\"\"
        self._add_nodes()
        self._add_edges()

    def _add_nodes(self):
        \"\"\"æ·»åŠ èŠ‚ç‚¹\"\"\"
        self.workflow.add_node("initialize", self._initialize_node)
        self.workflow.add_node("process", self._process_node)
        self.workflow.add_node("finalize", self._finalize_node)

    def _add_edges(self):
        \"\"\"æ·»åŠ è¾¹\"\"\"
        self.workflow.set_entry_point("initialize")
        self.workflow.add_edge("initialize", "process")
        self.workflow.add_edge("process", "finalize")
        self.workflow.add_edge("finalize", END)

    def _initialize_node(self, state: {name}State) -> Dict:
        \"\"\"åˆå§‹åŒ–èŠ‚ç‚¹\"\"\"
        logger.info("Initializing {name} workflow")
        return {{
            "status": "initialized",
            "execution_path": ["initialize"]
        }}

    def _process_node(self, state: {name}State) -> Dict:
        \"\"\"å¤„ç†èŠ‚ç‚¹\"\"\"
        logger.info("Processing in {name} workflow")
        # å®ç°å¤„ç†é€»è¾‘
        return {{
            "status": "processing",
            "execution_path": ["process"]
        }}

    def _finalize_node(self, state: {name}State) -> Dict:
        \"\"\"å®ŒæˆèŠ‚ç‚¹\"\"\"
        logger.info("Finalizing {name} workflow")
        return {{
            "status": "completed",
            "execution_path": ["finalize"],
            "output_data": {{"result": "success"}}
        }}
"""

def generate_state_code(name: str) -> str:
    """ç”ŸæˆçŠ¶æ€ä»£ç """

    return f"""
from typing import TypedDict, List, Dict, Optional, Annotated
from operator import add
from src.states.base_state import BaseState

class {name}State(BaseState):
    \"\"\"
    {name} å·¥ä½œæµçŠ¶æ€
    \"\"\"

    # æ·»åŠ è‡ªå®šä¹‰çŠ¶æ€å­—æ®µ
    custom_field: str
    processing_results: Annotated[List[Dict], add]

    # å¯é€‰å­—æ®µ
    optional_data: Optional[Dict]
"""

def generate_node_code(name: str, node_type: str) -> str:
    """ç”ŸæˆèŠ‚ç‚¹ä»£ç """

    return f"""
def {name.lower()}_node(state: Dict) -> Dict:
    \"\"\"
    {name} èŠ‚ç‚¹

    Args:
        state: å½“å‰çŠ¶æ€

    Returns:
        çŠ¶æ€æ›´æ–°
    \"\"\"
    import logging
    logger = logging.getLogger(__name__)

    logger.info("Executing {name} node")

    # å®ç°èŠ‚ç‚¹é€»è¾‘
    try:
        # TODO: å®ç°å…·ä½“é€»è¾‘
        result = {{"processed": True}}

        return {{
            "execution_path": ["{name.lower()}"],
            "{name.lower()}_result": result
        }}
    except Exception as e:
        logger.error(f"Error in {name} node: {{e}}")
        return {{"error": str(e)}}
"""
```

## 3. Docker æ¨¡æ¿

### 3.1 Dockerfile

```dockerfile
# docker/Dockerfile
FROM python:3.11-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºérootç”¨æˆ·
RUN useradd -m appuser && chown -R appuser:appuser /app
USER appuser

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import requests; requests.get('http://localhost:8000/health')"

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "src.api.app:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 3.2 Docker Compose

```yaml
# docker/docker-compose.yml
version: '3.8'

services:
  app:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    ports:
      - "8000:8000"
    environment:
      - ENVIRONMENT=development
      - DATABASE_URL=postgresql://user:password@postgres:5432/langgraph
      - REDIS_URL=redis://redis:6379
    volumes:
      - ../src:/app/src
      - ../configs:/app/configs
    depends_on:
      - postgres
      - redis
    networks:
      - langgraph-network

  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: langgraph
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    networks:
      - langgraph-network

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - langgraph-network

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - app
    networks:
      - langgraph-network

volumes:
  postgres_data:
  redis_data:

networks:
  langgraph-network:
    driver: bridge
```

## 4. API æ¨¡æ¿

### 4.1 FastAPI åº”ç”¨

```python
# src/api/app.py
from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import logging
from typing import Dict, Any, List

from src.config.settings import get_settings
from src.api.routes import workflow_router, health_router
from src.api.models import WorkflowRequest, WorkflowResponse
from src.utils.logger import setup_logging

# è®¾ç½®æ—¥å¿—
setup_logging()
logger = logging.getLogger(__name__)

# è·å–é…ç½®
settings = get_settings()

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    logger.info("Starting LangGraph application...")
    # åˆå§‹åŒ–èµ„æº
    yield
    # æ¸…ç†èµ„æº
    logger.info("Shutting down LangGraph application...")

# åˆ›å»ºåº”ç”¨
app = FastAPI(
    title=settings.app_name,
    version="1.0.0",
    lifespan=lifespan
)

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.cors_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ³¨å†Œè·¯ç”±
app.include_router(health_router, tags=["health"])
app.include_router(
    workflow_router,
    prefix=settings.api_prefix,
    tags=["workflows"]
)

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "name": settings.app_name,
        "environment": settings.environment,
        "version": "1.0.0"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "src.api.app:app",
        host=settings.api_host,
        port=settings.api_port,
        reload=settings.debug
    )
```

### 4.2 è·¯ç”±å®šä¹‰

```python
# src/api/routes.py
from fastapi import APIRouter, HTTPException, BackgroundTasks
from typing import Dict, Any, List
import logging

from src.api.models import (
    WorkflowRequest,
    WorkflowResponse,
    HealthResponse
)
from src.workflows.registry import WorkflowRegistry

logger = logging.getLogger(__name__)

# å¥åº·æ£€æŸ¥è·¯ç”±
health_router = APIRouter()

@health_router.get("/health", response_model=HealthResponse)
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return HealthResponse(
        status="healthy",
        services={
            "api": "up",
            "workflows": "up"
        }
    )

# å·¥ä½œæµè·¯ç”±
workflow_router = APIRouter()
registry = WorkflowRegistry()

@workflow_router.post("/workflows/{workflow_name}/invoke", response_model=WorkflowResponse)
async def invoke_workflow(
    workflow_name: str,
    request: WorkflowRequest,
    background_tasks: BackgroundTasks
):
    """æ‰§è¡Œå·¥ä½œæµ"""
    try:
        workflow = registry.get_workflow(workflow_name)
        if not workflow:
            raise HTTPException(status_code=404, detail=f"Workflow {workflow_name} not found")

        result = await workflow.ainvoke(request.input_data, request.config)

        return WorkflowResponse(
            workflow_name=workflow_name,
            status="completed",
            result=result
        )
    except Exception as e:
        logger.error(f"Workflow execution failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@workflow_router.get("/workflows", response_model=List[str])
async def list_workflows():
    """åˆ—å‡ºæ‰€æœ‰å·¥ä½œæµ"""
    return registry.list_workflows()

@workflow_router.get("/workflows/{workflow_name}/graph")
async def get_workflow_graph(workflow_name: str):
    """è·å–å·¥ä½œæµå›¾"""
    workflow = registry.get_workflow(workflow_name)
    if not workflow:
        raise HTTPException(status_code=404, detail=f"Workflow {workflow_name} not found")

    image = workflow.get_graph_image()
    return {"graph": image.decode() if isinstance(image, bytes) else image}
```

## 5. æµ‹è¯•æ¨¡æ¿

### 5.1 æµ‹è¯•é…ç½®

```python
# tests/conftest.py
import pytest
import asyncio
from typing import Generator, AsyncGenerator
from unittest.mock import Mock, AsyncMock

from src.config.settings import Settings
from src.workflows.base import BaseWorkflow

@pytest.fixture(scope="session")
def event_loop():
    """åˆ›å»ºäº‹ä»¶å¾ªç¯"""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

@pytest.fixture
def test_settings():
    """æµ‹è¯•é…ç½®"""
    return Settings(
        environment="test",
        debug=True,
        openai_api_key="test_key",
        secret_key="test_secret"
    )

@pytest.fixture
def mock_workflow():
    """æ¨¡æ‹Ÿå·¥ä½œæµ"""
    workflow = Mock(spec=BaseWorkflow)
    workflow.invoke.return_value = {"result": "success"}
    workflow.ainvoke = AsyncMock(return_value={"result": "success"})
    return workflow

@pytest.fixture
def sample_state():
    """ç¤ºä¾‹çŠ¶æ€"""
    return {
        "id": "test_id",
        "input_data": {"test": "data"},
        "status": "pending"
    }
```

### 5.2 å·¥ä½œæµæµ‹è¯•

```python
# tests/unit/test_workflows.py
import pytest
from src.workflows.base import BaseWorkflow
from src.states.base_state import WorkflowState

class TestBaseWorkflow:
    """åŸºç¡€å·¥ä½œæµæµ‹è¯•"""

    def test_workflow_creation(self):
        """æµ‹è¯•å·¥ä½œæµåˆ›å»º"""

        class TestWorkflow(BaseWorkflow):
            def _setup_workflow(self):
                self._add_nodes()
                self._add_edges()

            def _add_nodes(self):
                self.workflow.add_node("test", lambda s: {"result": "test"})

            def _add_edges(self):
                self.workflow.set_entry_point("test")
                self.workflow.add_edge("test", "__end__")

        workflow = TestWorkflow("test", WorkflowState)
        assert workflow.name == "test"
        assert workflow.state_class == WorkflowState

    @pytest.mark.asyncio
    async def test_async_invoke(self, mock_workflow, sample_state):
        """æµ‹è¯•å¼‚æ­¥æ‰§è¡Œ"""
        result = await mock_workflow.ainvoke(sample_state)
        assert result["result"] == "success"
        mock_workflow.ainvoke.assert_called_once_with(sample_state, None)
```

## 6. Makefile

```makefile
# Makefile
.PHONY: help install test lint format run docker-build docker-up docker-down clean

help:
	@echo "Available commands:"
	@echo "  install      Install dependencies"
	@echo "  test         Run tests"
	@echo "  lint         Run linters"
	@echo "  format       Format code"
	@echo "  run          Run the application"
	@echo "  docker-build Build Docker image"
	@echo "  docker-up    Start Docker services"
	@echo "  docker-down  Stop Docker services"
	@echo "  clean        Clean up temporary files"

install:
	pip install -r requirements.txt
	pip install -r requirements-dev.txt

test:
	pytest tests/ -v --cov=src --cov-report=html

lint:
	pylint src/
	mypy src/

format:
	black src/ tests/
	isort src/ tests/

run:
	python -m src.api.app

docker-build:
	docker-compose -f docker/docker-compose.yml build

docker-up:
	docker-compose -f docker/docker-compose.yml up -d

docker-down:
	docker-compose -f docker/docker-compose.yml down

clean:
	find . -type d -name "__pycache__" -exec rm -rf {} +
	find . -type f -name "*.pyc" -delete
	rm -rf .pytest_cache
	rm -rf htmlcov
	rm -rf .coverage
```

## 7. é¡¹ç›®åˆå§‹åŒ–è„šæœ¬

```bash
#!/bin/bash
# scripts/init_project.sh

echo "ğŸš€ Initializing LangGraph Project..."

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
echo "Creating virtual environment..."
python -m venv venv
source venv/bin/activate

# å®‰è£…ä¾èµ–
echo "Installing dependencies..."
pip install --upgrade pip
pip install -r requirements.txt
pip install -r requirements-dev.txt

# å¤åˆ¶ç¯å¢ƒå˜é‡
echo "Setting up environment..."
cp .env.example .env

# åˆå§‹åŒ–æ•°æ®åº“
echo "Initializing database..."
python scripts/init_db.py

# è¿è¡Œæµ‹è¯•
echo "Running tests..."
pytest tests/

# æ ¼å¼åŒ–ä»£ç 
echo "Formatting code..."
black src/ tests/

echo "âœ… Project initialization complete!"
echo ""
echo "Next steps:"
echo "1. Edit .env file with your configuration"
echo "2. Run 'make run' to start the application"
echo "3. Visit http://localhost:8000/docs for API documentation"
```

## æ€»ç»“

è¿™ä¸ªé¡¹ç›®æ¨¡æ¿å’Œè„šæ‰‹æ¶æä¾›äº†ï¼š

1. **å®Œæ•´çš„é¡¹ç›®ç»“æ„**ï¼šæ¸…æ™°çš„ç›®å½•ç»„ç»‡
2. **åŸºç¡€ç±»å’ŒæŠ½è±¡**ï¼šå¯å¤ç”¨çš„å·¥ä½œæµåŸºç±»
3. **é…ç½®ç®¡ç†**ï¼šçµæ´»çš„é…ç½®ç³»ç»Ÿ
4. **CLIå·¥å…·**ï¼šå¿«é€Ÿåˆ›å»ºé¡¹ç›®å’Œç»„ä»¶
5. **Dockeræ”¯æŒ**ï¼šå®¹å™¨åŒ–éƒ¨ç½²
6. **APIæ¨¡æ¿**ï¼šFastAPIé›†æˆ
7. **æµ‹è¯•æ¡†æ¶**ï¼šå®Œæ•´çš„æµ‹è¯•æ”¯æŒ
8. **å¼€å‘å·¥å…·**ï¼šMakefileå’Œåˆå§‹åŒ–è„šæœ¬

é€šè¿‡è¿™ä¸ªè„šæ‰‹æ¶ï¼Œå¯ä»¥å¿«é€Ÿå¯åŠ¨ä¸€ä¸ªç”Ÿäº§çº§çš„ LangGraph é¡¹ç›®ï¼Œä¸“æ³¨äºä¸šåŠ¡é€»è¾‘çš„å®ç°ã€‚