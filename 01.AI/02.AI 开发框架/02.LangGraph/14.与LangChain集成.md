---
title: LangGraphå®æˆ˜æ•™ç¨‹(ç¬¬14ç« ):ä¸LangChainé›†æˆ
date: 2025-02-07
permalink: /ai/langgraph/14.ä¸langchainé›†æˆ.html
categories:
  - LangGraph
tags:
  - LangGraph
---

# ä¸LangChainé›†æˆ

**ä¸ºä»€ä¹ˆéœ€è¦é›†æˆ LangChainï¼Ÿ**

LangGraph å’Œ LangChain æ˜¯äº’è¡¥çš„æ¡†æ¶:
- **LangGraph**: æä¾›çµæ´»çš„å›¾ç»“æ„å’ŒçŠ¶æ€ç®¡ç†,æ“…é•¿å¤æ‚çš„å·¥ä½œæµç¼–æ’
- **LangChain**: æä¾›ä¸°å¯Œçš„ç»„ä»¶ç”Ÿæ€(LLMã€å·¥å…·ã€è®°å¿†ã€é“¾ç­‰)

å°†äºŒè€…é›†æˆå¯ä»¥:
1. **å€ŸåŠ›ç”Ÿæ€**: ä½¿ç”¨ LangChain ä¸°å¯Œçš„å·¥å…·å’Œç»„ä»¶åº“
2. **æœ€ä½³ç»„åˆ**: LangGraph çš„ç¼–æ’èƒ½åŠ› + LangChain çš„ç»„ä»¶èƒ½åŠ›
3. **å¿«é€Ÿå¼€å‘**: é¿å…é‡å¤é€ è½®å­,ä¸“æ³¨ä¸šåŠ¡é€»è¾‘
4. **çµæ´»æ‰©å±•**: è½»æ¾æ·»åŠ æ–°çš„ LLMã€å·¥å…·æˆ–è®°å¿†ç³»ç»Ÿ

## ä¸€ã€é›†æˆæ¶æ„

**æ•´ä½“æ¶æ„æ¦‚è§ˆ**

LangGraph ä½œä¸ºç¼–æ’å¼•æ“,å¯ä»¥æ— ç¼é›†æˆ LangChain çš„å„ç§ç»„ä»¶:

```mermaid
graph TD
    A[LangGraph å›¾] --> B[LangChain LLM]
    A --> C[LangChain Tools]
    A --> D[LangChain Memory]
    A --> E[LangChain Chains]
    A --> F[LangChain Agents]

    B --> B1[ChatOpenAI]
    B --> B2[ChatAnthropic]
    B --> B3[è‡ªå®šä¹‰ LLM]

    C --> C1[æœç´¢å·¥å…·]
    C --> C2[æ•°æ®åº“å·¥å…·]
    C --> C3[API å·¥å…·]

    D --> D1[BufferMemory]
    D --> D2[VectorStoreMemory]
    D --> D3[ConversationMemory]

    E --> E1[LLMChain]
    E --> E2[SequentialChain]
    E --> E3[RouterChain]

    style A fill:#e1f5fe
    style B fill:#b3e5fc
    style C fill:#81d4fa
    style D fill:#4fc3f7
```

**é›†æˆå±‚æ¬¡**

| é›†æˆç±»å‹ | å¤æ‚åº¦ | çµæ´»æ€§ | é€‚ç”¨åœºæ™¯ |
|---------|-------|-------|---------|
| **LLM é›†æˆ** | ä½ | é«˜ | éœ€è¦è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ |
| **Tools é›†æˆ** | ä½ | é«˜ | éœ€è¦å¤–éƒ¨å·¥å…·è°ƒç”¨ |
| **Memory é›†æˆ** | ä¸­ | ä¸­ | éœ€è¦å¯¹è¯è®°å¿† |
| **Chains é›†æˆ** | ä¸­ | ä¸­ | å¤ç”¨ç°æœ‰é“¾é€»è¾‘ |
| **Agents é›†æˆ** | é«˜ | ä½ | éœ€è¦è‡ªä¸»å†³ç­–èƒ½åŠ› |

**é›†æˆåŸåˆ™**

1. **çŠ¶æ€ä¼˜å…ˆ**: LangGraph çš„çŠ¶æ€ç®¡ç†æ˜¯æ ¸å¿ƒ,LangChain ç»„ä»¶ä½œä¸ºå·¥å…·ä½¿ç”¨
2. **æ˜ç¡®è¾¹ç•Œ**: æ¸…æ™°å®šä¹‰æ¯ä¸ªç»„ä»¶çš„è¾“å…¥è¾“å‡º
3. **é”™è¯¯å¤„ç†**: LangChain ç»„ä»¶å¯èƒ½æŠ›å‡ºå¼‚å¸¸,éœ€è¦å¦¥å–„å¤„ç†
4. **æˆæœ¬æ§åˆ¶**: LLM è°ƒç”¨æœ‰æˆæœ¬,éœ€è¦åˆç†è®¾è®¡è°ƒç”¨ç­–ç•¥

## äºŒã€é›†æˆ LLM

**ä»€ä¹ˆæ˜¯ LLM é›†æˆï¼Ÿ**

LLM(Large Language Model)é›†æˆæ˜¯æŒ‡åœ¨ LangGraph å›¾ä¸­ä½¿ç”¨ LangChain æä¾›çš„å„ç§å¤§è¯­è¨€æ¨¡å‹ã€‚LangChain æ”¯æŒ OpenAIã€Anthropicã€Googleã€æœ¬åœ°æ¨¡å‹ç­‰å¤šç§ LLMã€‚

**ä¸ºä»€ä¹ˆè¦é›†æˆ LLMï¼Ÿ**

1. **ç»Ÿä¸€æ¥å£**: LangChain ä¸ºä¸åŒ LLM æä¾›ç»Ÿä¸€çš„è°ƒç”¨æ¥å£
2. **æ˜“äºåˆ‡æ¢**: å¯ä»¥è½»æ¾åˆ‡æ¢ä¸åŒçš„ LLM æä¾›å•†
3. **ä¸°å¯ŒåŠŸèƒ½**: æ”¯æŒæµå¼è¾“å‡ºã€å‡½æ•°è°ƒç”¨ã€ç»“æ„åŒ–è¾“å‡ºç­‰
4. **æˆæœ¬ä¼˜åŒ–**: æ ¹æ®ä»»åŠ¡é€‰æ‹©åˆé€‚ä»·æ ¼çš„æ¨¡å‹

**LLM é›†æˆæ¨¡å¼**

```mermaid
graph LR
    A[ç”¨æˆ·è¾“å…¥] --> B[LangGraph èŠ‚ç‚¹]
    B --> C[LangChain LLM]
    C --> D[æ¨¡å‹å“åº”]
    D --> E[çŠ¶æ€æ›´æ–°]
    E --> F[ä¸‹ä¸€ä¸ªèŠ‚ç‚¹]

    style A fill:#e1f5fe
    style B fill:#b3e5fc
    style C fill:#81d4fa
    style D fill:#4fc3f7
    style E fill:#c8e6c9
```

### 2.1 åŸºç¡€ LLM é›†æˆ

**ä½¿ç”¨åœºæ™¯**

- é—®ç­”ç³»ç»Ÿ
- æ–‡æœ¬ç”Ÿæˆ
- å¯¹è¯æœºå™¨äºº
- å†…å®¹æ‘˜è¦
- ç¿»è¯‘æœåŠ¡

**å®Œæ•´å®ç°ç¤ºä¾‹**

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from typing import TypedDict, Annotated
import operator
from langgraph.graph import StateGraph, END

# å®šä¹‰èŠå¤©çŠ¶æ€
class ChatState(TypedDict):
    messages: Annotated[list, operator.add]  # æ¶ˆæ¯å†å²(ä½¿ç”¨ Reducer ç´¯ç§¯)
    llm_response: str                         # LLM å“åº”

def create_llm_graph():
    """
    é›†æˆ LangChain LLM çš„å›¾

    ç‰¹ç‚¹:
    1. ä½¿ç”¨ LangChain çš„ ChatOpenAI
    2. æ”¯æŒæ¶ˆæ¯å†å²
    3. è‡ªåŠ¨çŠ¶æ€ç®¡ç†
    """
    # åˆå§‹åŒ– LLM
    llm = ChatOpenAI(
        model="gpt-4",
        temperature=0.7  # æ§åˆ¶è¾“å‡ºéšæœºæ€§
    )

    def chat_node(state: ChatState) -> dict:
        """
        ä½¿ç”¨ LangChain LLM çš„èŠ‚ç‚¹

        åŠŸèƒ½:
        1. ä»çŠ¶æ€è·å–æ¶ˆæ¯å†å²
        2. è°ƒç”¨ LLM ç”Ÿæˆå“åº”
        3. æ›´æ–°çŠ¶æ€
        """
        # è°ƒç”¨ LLM
        response = llm.invoke(state["messages"])

        print(f"[LLM] æ”¶åˆ° {len(state['messages'])} æ¡æ¶ˆæ¯")
        print(f"[LLM] å“åº”: {response.content[:100]}...")

        return {
            "messages": [AIMessage(content=response.content)],
            "llm_response": response.content
        }

    # æ„å»ºå›¾
    graph = StateGraph(ChatState)
    graph.add_node("chat", chat_node)
    graph.set_entry_point("chat")
    graph.add_edge("chat", END)

    return graph.compile()

# ä½¿ç”¨ç¤ºä¾‹
def test_llm_integration():
    """
    æµ‹è¯• LLM é›†æˆ

    æ¼”ç¤º:
    1. ä½¿ç”¨ç³»ç»Ÿæ¶ˆæ¯è®¾ç½®è§’è‰²
    2. å‘é€ç”¨æˆ·æ¶ˆæ¯
    3. è·å– AI å“åº”
    """
    print("=== åŸºç¡€ LLM é›†æˆæµ‹è¯• ===\n")

    app = create_llm_graph()

    result = app.invoke({
        "messages": [
            SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„ AI åŠ©æ‰‹,ä¸“é—¨è§£é‡ŠæŠ€æœ¯æ¦‚å¿µ"),
            HumanMessage(content="ä»€ä¹ˆæ˜¯ LangGraph? è¯·ç”¨ç®€å•çš„è¯­è¨€è§£é‡Š")
        ],
        "llm_response": ""
    })

    print(f"\nâœ… LLM å›å¤:\n{result['llm_response']}")
    print(f"\næ¶ˆæ¯æ€»æ•°: {len(result['messages'])}")
```

**å…³é”®æŠ€æœ¯ç‚¹**

1. **æ¶ˆæ¯ç±»å‹**
```python
from langchain_core.messages import (
    SystemMessage,   # ç³»ç»Ÿæç¤º
    HumanMessage,    # ç”¨æˆ·æ¶ˆæ¯
    AIMessage,       # AI å“åº”
    FunctionMessage  # å‡½æ•°è°ƒç”¨ç»“æœ
)
```

2. **LLM é…ç½®**
```python
llm = ChatOpenAI(
    model="gpt-4",           # æ¨¡å‹é€‰æ‹©
    temperature=0.7,         # éšæœºæ€§æ§åˆ¶
    max_tokens=1000,         # æœ€å¤§tokenæ•°
    streaming=True,          # æµå¼è¾“å‡º
    api_key="your-key"       # APIå¯†é’¥
)
```

3. **é”™è¯¯å¤„ç†**
```python
from langchain_core.exceptions import OutputParserException

def safe_llm_call(llm, messages):
    """å®‰å…¨çš„ LLM è°ƒç”¨"""
    try:
        return llm.invoke(messages)
    except Exception as e:
        return AIMessage(content=f"é”™è¯¯: {str(e)}")
```

### 2.2 å¤š LLM åä½œ

**ä»€ä¹ˆæ˜¯å¤š LLM åä½œï¼Ÿ**

å¤š LLM åä½œæ˜¯æŒ‡åœ¨åŒä¸€ä¸ªå·¥ä½œæµä¸­ä½¿ç”¨å¤šä¸ªä¸åŒçš„å¤§è¯­è¨€æ¨¡å‹,åˆ©ç”¨å„ä¸ªæ¨¡å‹çš„ä¼˜åŠ¿,é€šè¿‡åä½œäº§ç”Ÿæ›´å¥½çš„ç»“æœã€‚

**ä¸ºä»€ä¹ˆéœ€è¦å¤š LLM åä½œï¼Ÿ**

1. **å–é•¿è¡¥çŸ­**: ä¸åŒæ¨¡å‹æ“…é•¿ä¸åŒä»»åŠ¡
2. **äº¤å‰éªŒè¯**: å¤šä¸ªæ¨¡å‹çš„å›ç­”å¯ä»¥äº’ç›¸éªŒè¯
3. **è´¨é‡æå‡**: ç»¼åˆå¤šä¸ªæ¨¡å‹çš„è¾“å‡ºæé«˜ç­”æ¡ˆè´¨é‡
4. **æˆæœ¬ä¼˜åŒ–**: ç®€å•ä»»åŠ¡ç”¨ä¾¿å®œæ¨¡å‹,å¤æ‚ä»»åŠ¡ç”¨é«˜çº§æ¨¡å‹

**åä½œæ¨¡å¼**

```mermaid
graph TB
    A[ç”¨æˆ·æŸ¥è¯¢] --> B[GPT-4 åˆ†æ]
    A --> C[Claude åˆ†æ]
    B --> D[ç»¼åˆèŠ‚ç‚¹]
    C --> D
    D --> E[æœ€ç»ˆç­”æ¡ˆ]

    style A fill:#e1f5fe
    style B fill:#b3e5fc
    style C fill:#b3e5fc
    style D fill:#81d4fa
    style E fill:#c8e6c9
```

**å®Œæ•´å®ç°ç¤ºä¾‹**

```python
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from typing import TypedDict

class MultiLLMState(TypedDict):
    query: str
    gpt_response: str
    claude_response: str
    final_answer: str

def create_multi_llm_graph():
    """å¤šä¸ª LLM åä½œçš„å›¾"""
    gpt = ChatOpenAI(model="gpt-4")
    claude = ChatAnthropic(model="claude-3-sonnet-20240229")

    def gpt_node(state: MultiLLMState) -> dict:
        """GPT èŠ‚ç‚¹"""
        response = gpt.invoke(state["query"])
        return {"gpt_response": response.content}

    def claude_node(state: MultiLLMState) -> dict:
        """Claude èŠ‚ç‚¹"""
        response = claude.invoke(state["query"])
        return {"claude_response": response.content}

    def synthesize_node(state: MultiLLMState) -> dict:
        """ç»¼åˆä¸¤ä¸ª LLM çš„å›ç­”"""
        synthesis_prompt = f"""
        å¯¹äºé—®é¢˜: {state['query']}

        GPT-4 çš„å›ç­”: {state['gpt_response']}
        Claude çš„å›ç­”: {state['claude_response']}

        è¯·ç»¼åˆè¿™ä¸¤ä¸ªå›ç­”,ç»™å‡ºæœ€ä½³ç­”æ¡ˆã€‚
        """

        final_response = gpt.invoke(synthesis_prompt)
        return {"final_answer": final_response.content}

    graph = StateGraph(MultiLLMState)
    graph.add_node("gpt", gpt_node)
    graph.add_node("claude", claude_node)
    graph.add_node("synthesize", synthesize_node)

    # GPT å’Œ Claude å¹¶è¡Œæ‰§è¡Œ
    graph.set_entry_point("gpt")
    graph.set_entry_point("claude")

    graph.add_edge("gpt", "synthesize")
    graph.add_edge("claude", "synthesize")
    graph.add_edge("synthesize", END)

    return graph.compile()
```

## ä¸‰ã€é›†æˆ Tools

### 3.1 åŸºç¡€å·¥å…·é›†æˆ

```python
from langchain_community.tools import DuckDuckGoSearchRun
from langchain_community.utilities import WikipediaAPIWrapper
from langchain_core.tools import tool
from typing import TypedDict

class ToolState(TypedDict):
    query: str
    search_results: str
    wiki_results: str
    final_answer: str

@tool
def custom_calculator(expression: str) -> str:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼"""
    try:
        result = eval(expression)
        return f"è®¡ç®—ç»“æœ: {result}"
    except Exception as e:
        return f"è®¡ç®—é”™è¯¯: {str(e)}"

def create_tool_graph():
    """é›†æˆå·¥å…·çš„å›¾"""
    # LangChain å·¥å…·
    search = DuckDuckGoSearchRun()
    wikipedia = WikipediaAPIWrapper()

    def search_node(state: ToolState) -> dict:
        """æœç´¢èŠ‚ç‚¹"""
        results = search.run(state["query"])
        return {"search_results": results}

    def wiki_node(state: ToolState) -> dict:
        """ç»´åŸºç™¾ç§‘èŠ‚ç‚¹"""
        results = wikipedia.run(state["query"])
        return {"wiki_results": results}

    def synthesize_node(state: ToolState) -> dict:
        """ç»¼åˆå·¥å…·ç»“æœ"""
        answer = f"""
        åŸºäºæœç´¢ç»“æœå’Œç»´åŸºç™¾ç§‘:

        æœç´¢: {state['search_results'][:200]}...
        ç»´åŸº: {state['wiki_results'][:200]}...

        ç»¼åˆç­”æ¡ˆ: [è¿™é‡Œåº”è¯¥ç”¨ LLM ç”Ÿæˆç»¼åˆç­”æ¡ˆ]
        """
        return {"final_answer": answer}

    graph = StateGraph(ToolState)
    graph.add_node("search", search_node)
    graph.add_node("wiki", wiki_node)
    graph.add_node("synthesize", synthesize_node)

    # å¹¶è¡Œæ‰§è¡Œå·¥å…·
    graph.set_entry_point("search")
    graph.set_entry_point("wiki")

    graph.add_edge("search", "synthesize")
    graph.add_edge("wiki", "synthesize")
    graph.add_edge("synthesize", END)

    return graph.compile()
```

### 3.2 åŠ¨æ€å·¥å…·è°ƒç”¨

```python
from langchain_core.tools import BaseTool
from typing import TypedDict, List

class ToolCallState(TypedDict):
    query: str
    available_tools: List[str]
    selected_tool: str
    tool_result: str

# å®šä¹‰å¤šä¸ªå·¥å…·
@tool
def weather_tool(location: str) -> str:
    """è·å–å¤©æ°”ä¿¡æ¯"""
    return f"{location} çš„å¤©æ°”: æ™´å¤© 25Â°C"

@tool
def news_tool(topic: str) -> str:
    """è·å–æ–°é—»"""
    return f"å…³äº {topic} çš„æ–°é—»: ..."

@tool
def stock_tool(symbol: str) -> str:
    """è·å–è‚¡ç¥¨ä¿¡æ¯"""
    return f"{symbol} è‚¡ç¥¨ä»·æ ¼: $100"

def create_dynamic_tool_graph():
    """åŠ¨æ€å·¥å…·è°ƒç”¨å›¾"""
    llm = ChatOpenAI(model="gpt-4")

    # å·¥å…·æ˜ å°„
    tools = {
        "weather": weather_tool,
        "news": news_tool,
        "stock": stock_tool
    }

    def select_tool(state: ToolCallState) -> dict:
        """LLM é€‰æ‹©åˆé€‚çš„å·¥å…·"""
        prompt = f"""
        å¯¹äºæŸ¥è¯¢: {state['query']}
        å¯ç”¨å·¥å…·: {', '.join(state['available_tools'])}

        é€‰æ‹©æœ€åˆé€‚çš„å·¥å…·åç§°ï¼ˆåªè¿”å›å·¥å…·åï¼‰ã€‚
        """

        response = llm.invoke(prompt)
        return {"selected_tool": response.content.strip()}

    def execute_tool(state: ToolCallState) -> dict:
        """æ‰§è¡Œé€‰å®šçš„å·¥å…·"""
        tool = tools.get(state["selected_tool"])

        if tool:
            # ä»æŸ¥è¯¢ä¸­æå–å‚æ•°ï¼ˆç®€åŒ–ç¤ºä¾‹ï¼‰
            result = tool.invoke(state["query"])
        else:
            result = "æœªæ‰¾åˆ°åˆé€‚çš„å·¥å…·"

        return {"tool_result": result}

    graph = StateGraph(ToolCallState)
    graph.add_node("select", select_tool)
    graph.add_node("execute", execute_tool)

    graph.set_entry_point("select")
    graph.add_edge("select", "execute")
    graph.add_edge("execute", END)

    return graph.compile()
```

## å››ã€é›†æˆ Memory

### 4.1 å¯¹è¯è®°å¿†

```python
from langchain.memory import ConversationBufferMemory
from langchain_core.messages import HumanMessage, AIMessage
from typing import TypedDict, Annotated
import operator

class MemoryState(TypedDict):
    messages: Annotated[list, operator.add]
    memory_summary: str

def create_memory_graph():
    """é›†æˆè®°å¿†çš„å›¾"""
    llm = ChatOpenAI(model="gpt-4")
    memory = ConversationBufferMemory(return_messages=True)

    def chat_with_memory(state: MemoryState) -> dict:
        """å¸¦è®°å¿†çš„èŠå¤©"""
        # ä»çŠ¶æ€è·å–æ¶ˆæ¯
        current_messages = state["messages"]

        # ä¿å­˜åˆ°è®°å¿†
        if len(current_messages) > 0:
            last_message = current_messages[-1]
            if isinstance(last_message, HumanMessage):
                memory.chat_memory.add_user_message(last_message.content)

        # è·å–è®°å¿†ä¸Šä¸‹æ–‡
        memory_context = memory.load_memory_variables({})
        history = memory_context.get("history", [])

        # è°ƒç”¨ LLMï¼ˆåŒ…å«å†å²ï¼‰
        all_messages = history + current_messages
        response = llm.invoke(all_messages)

        # ä¿å­˜ AI å›å¤åˆ°è®°å¿†
        memory.chat_memory.add_ai_message(response.content)

        return {
            "messages": [AIMessage(content=response.content)],
            "memory_summary": f"å¯¹è¯è½®æ¬¡: {len(memory.chat_memory.messages)}"
        }

    graph = StateGraph(MemoryState)
    graph.add_node("chat", chat_with_memory)
    graph.set_entry_point("chat")
    graph.add_edge("chat", END)

    return graph.compile()

# æµ‹è¯•å¤šè½®å¯¹è¯
def test_memory():
    """æµ‹è¯•è®°å¿†åŠŸèƒ½"""
    app = create_memory_graph()

    # ç¬¬ä¸€è½®
    result1 = app.invoke({
        "messages": [HumanMessage(content="æˆ‘å«å¼ ä¸‰")],
        "memory_summary": ""
    })
    print(f"å›å¤1: {result1['messages'][-1].content}")

    # ç¬¬äºŒè½®ï¼ˆLLM åº”è¯¥è®°å¾—åå­—ï¼‰
    result2 = app.invoke({
        "messages": [HumanMessage(content="æˆ‘å«ä»€ä¹ˆåå­—?")],
        "memory_summary": ""
    })
    print(f"å›å¤2: {result2['messages'][-1].content}")
```

### 4.2 å‘é‡è®°å¿†

```python
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain.memory import VectorStoreRetrieverMemory
from typing import TypedDict

class VectorMemoryState(TypedDict):
    query: str
    relevant_history: str
    response: str

def create_vector_memory_graph():
    """ä½¿ç”¨å‘é‡è®°å¿†çš„å›¾"""
    # åˆ›å»ºå‘é‡å­˜å‚¨
    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.from_texts(
        ["åˆå§‹åŒ–å‘é‡å­˜å‚¨"],
        embedding=embeddings
    )

    # åˆ›å»ºè®°å¿†
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    memory = VectorStoreRetrieverMemory(retriever=retriever)

    llm = ChatOpenAI(model="gpt-4")

    def query_with_vector_memory(state: VectorMemoryState) -> dict:
        """å¸¦å‘é‡è®°å¿†çš„æŸ¥è¯¢"""
        # æ£€ç´¢ç›¸å…³å†å²
        relevant = memory.load_memory_variables(
            {"query": state["query"]}
        )

        # æ„å»ºæç¤º
        prompt = f"""
        ç›¸å…³å†å²è®°å½•:
        {relevant.get('history', 'æ— ')}

        å½“å‰é—®é¢˜: {state['query']}

        è¯·å›ç­”é—®é¢˜ã€‚
        """

        response = llm.invoke(prompt)

        # ä¿å­˜åˆ°è®°å¿†
        memory.save_context(
            {"input": state["query"]},
            {"output": response.content}
        )

        return {
            "relevant_history": str(relevant),
            "response": response.content
        }

    graph = StateGraph(VectorMemoryState)
    graph.add_node("query", query_with_vector_memory)
    graph.set_entry_point("query")
    graph.add_edge("query", END)

    return graph.compile()
```

## äº”ã€é›†æˆ Chains

### 5.1 LLMChain é›†æˆ

```python
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from typing import TypedDict

class ChainState(TypedDict):
    topic: str
    outline: str
    content: str

def create_chain_graph():
    """é›†æˆ LangChain Chain çš„å›¾"""
    llm = ChatOpenAI(model="gpt-4")

    # åˆ›å»º Chain
    outline_prompt = PromptTemplate(
        input_variables=["topic"],
        template="ä¸ºä¸»é¢˜ '{topic}' åˆ›å»ºä¸€ä¸ªæ–‡ç« å¤§çº²ã€‚"
    )
    outline_chain = LLMChain(llm=llm, prompt=outline_prompt)

    content_prompt = PromptTemplate(
        input_variables=["topic", "outline"],
        template="""
        ä¸»é¢˜: {topic}
        å¤§çº²: {outline}

        æ ¹æ®ä»¥ä¸Šå¤§çº²,å†™ä¸€ç¯‡è¯¦ç»†çš„æ–‡ç« ã€‚
        """
    )
    content_chain = LLMChain(llm=llm, prompt=content_prompt)

    def outline_node(state: ChainState) -> dict:
        """ç”Ÿæˆå¤§çº²"""
        outline = outline_chain.run(topic=state["topic"])
        return {"outline": outline}

    def content_node(state: ChainState) -> dict:
        """ç”Ÿæˆå†…å®¹"""
        content = content_chain.run(
            topic=state["topic"],
            outline=state["outline"]
        )
        return {"content": content}

    graph = StateGraph(ChainState)
    graph.add_node("outline", outline_node)
    graph.add_node("content", content_node)

    graph.set_entry_point("outline")
    graph.add_edge("outline", "content")
    graph.add_edge("content", END)

    return graph.compile()
```

### 5.2 SequentialChain é›†æˆ

```python
from langchain.chains import SequentialChain
from typing import TypedDict

class SequentialState(TypedDict):
    input: str
    output: str

def create_sequential_chain_graph():
    """é›†æˆ SequentialChain"""
    llm = ChatOpenAI(model="gpt-4")

    # ç¬¬ä¸€ä¸ª chain
    chain1 = LLMChain(
        llm=llm,
        prompt=PromptTemplate(
            input_variables=["text"],
            template="æ€»ç»“ä»¥ä¸‹æ–‡æœ¬: {text}"
        ),
        output_key="summary"
    )

    # ç¬¬äºŒä¸ª chain
    chain2 = LLMChain(
        llm=llm,
        prompt=PromptTemplate(
            input_variables=["summary"],
            template="å°†ä»¥ä¸‹æ‘˜è¦ç¿»è¯‘æˆè‹±æ–‡: {summary}"
        ),
        output_key="translation"
    )

    # ç»„åˆæˆ SequentialChain
    sequential_chain = SequentialChain(
        chains=[chain1, chain2],
        input_variables=["text"],
        output_variables=["translation"]
    )

    def process_node(state: SequentialState) -> dict:
        """ä½¿ç”¨ Sequential Chain å¤„ç†"""
        result = sequential_chain({"text": state["input"]})
        return {"output": result["translation"]}

    graph = StateGraph(SequentialState)
    graph.add_node("process", process_node)
    graph.set_entry_point("process")
    graph.add_edge("process", END)

    return graph.compile()
```

## å…­ã€é›†æˆ Agents

### 6.1 ReAct Agent é›†æˆ

```python
from langchain.agents import create_react_agent, AgentExecutor
from langchain.prompts import PromptTemplate
from langchain.tools import Tool
from typing import TypedDict

class AgentState(TypedDict):
    task: str
    agent_output: str

def create_agent_graph():
    """é›†æˆ LangChain Agent"""
    llm = ChatOpenAI(model="gpt-4", temperature=0)

    # å®šä¹‰å·¥å…·
    tools = [
        Tool(
            name="Calculator",
            func=lambda x: str(eval(x)),
            description="ç”¨äºæ•°å­¦è®¡ç®—"
        ),
        Tool(
            name="Search",
            func=lambda x: f"æœç´¢ç»“æœ: {x}",
            description="ç”¨äºæœç´¢ä¿¡æ¯"
        )
    ]

    # åˆ›å»º agent
    prompt = PromptTemplate.from_template("""
    å›ç­”ä»¥ä¸‹é—®é¢˜,ä½ å¯ä»¥ä½¿ç”¨è¿™äº›å·¥å…·:

    {tools}

    ä½¿ç”¨ä»¥ä¸‹æ ¼å¼:

    Question: éœ€è¦å›ç­”çš„é—®é¢˜
    Thought: æ€è€ƒè¯¥åšä»€ä¹ˆ
    Action: è¦é‡‡å–çš„è¡ŒåŠ¨,åº”è¯¥æ˜¯ [{tool_names}] ä¸­çš„ä¸€ä¸ª
    Action Input: è¡ŒåŠ¨çš„è¾“å…¥
    Observation: è¡ŒåŠ¨çš„ç»“æœ
    ... (è¿™ä¸ª Thought/Action/Action Input/Observation å¯ä»¥é‡å¤ N æ¬¡)
    Thought: æˆ‘ç°åœ¨çŸ¥é“æœ€ç»ˆç­”æ¡ˆäº†
    Final Answer: åŸå§‹é—®é¢˜çš„æœ€ç»ˆç­”æ¡ˆ

    å¼€å§‹!

    Question: {input}
    Thought: {agent_scratchpad}
    """)

    agent = create_react_agent(llm, tools, prompt)
    agent_executor = AgentExecutor(
        agent=agent,
        tools=tools,
        verbose=True,
        max_iterations=3
    )

    def agent_node(state: AgentState) -> dict:
        """Agent èŠ‚ç‚¹"""
        result = agent_executor.invoke({"input": state["task"]})
        return {"agent_output": result["output"]}

    graph = StateGraph(AgentState)
    graph.add_node("agent", agent_node)
    graph.set_entry_point("agent")
    graph.add_edge("agent", END)

    return graph.compile()

# æµ‹è¯• Agent
def test_agent():
    """æµ‹è¯• Agent é›†æˆ"""
    app = create_agent_graph()

    result = app.invoke({
        "task": "è®¡ç®— 25 * 4 ç„¶åæœç´¢è¿™ä¸ªæ•°å­—çš„å«ä¹‰",
        "agent_output": ""
    })

    print(f"Agent è¾“å‡º: {result['agent_output']}")
```

## ä¸ƒã€å®Œæ•´é›†æˆç¤ºä¾‹

### RAG ç³»ç»Ÿ

```python
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from typing import TypedDict, Annotated
import operator

class RAGState(TypedDict):
    query: str
    documents: Annotated[list, operator.add]
    context: str
    answer: str

def create_rag_graph():
    """å®Œæ•´çš„ RAG ç³»ç»Ÿ"""
    # åˆå§‹åŒ–ç»„ä»¶
    embeddings = OpenAIEmbeddings()
    llm = ChatOpenAI(model="gpt-4")

    # å‡è®¾å·²ç»æœ‰å‘é‡å­˜å‚¨
    # vectorstore = FAISS.load_local("./vectorstore")

    def retrieve_node(state: RAGState) -> dict:
        """æ£€ç´¢æ–‡æ¡£"""
        # vectorstore.similarity_search(state["query"], k=3)
        # æ¨¡æ‹Ÿæ£€ç´¢
        docs = [
            {"content": "æ–‡æ¡£1å†…å®¹", "score": 0.9},
            {"content": "æ–‡æ¡£2å†…å®¹", "score": 0.8},
        ]
        return {"documents": docs}

    def rerank_node(state: RAGState) -> dict:
        """é‡æ’åºæ–‡æ¡£"""
        # ä½¿ç”¨ LLM é‡æ’åº
        sorted_docs = sorted(
            state["documents"],
            key=lambda x: x["score"],
            reverse=True
        )
        return {"documents": sorted_docs[:3]}

    def generate_node(state: RAGState) -> dict:
        """ç”Ÿæˆç­”æ¡ˆ"""
        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([
            doc["content"] for doc in state["documents"]
        ])

        prompt = f"""
        åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜:

        ä¸Šä¸‹æ–‡:
        {context}

        é—®é¢˜: {state["query"]}

        ç­”æ¡ˆ:
        """

        response = llm.invoke(prompt)
        return {
            "context": context,
            "answer": response.content
        }

    graph = StateGraph(RAGState)
    graph.add_node("retrieve", retrieve_node)
    graph.add_node("rerank", rerank_node)
    graph.add_node("generate", generate_node)

    graph.set_entry_point("retrieve")
    graph.add_edge("retrieve", "rerank")
    graph.add_edge("rerank", "generate")
    graph.add_edge("generate", END)

    return graph.compile()
```

## å…«ã€æœ€ä½³å®è·µ

### 8.1 é›†æˆæ¸…å•

**è®¾è®¡é˜¶æ®µ**

- âœ… **æ˜ç¡®éœ€æ±‚**: æ¸…æ¥šçŸ¥é“éœ€è¦å“ªäº› LangChain ç»„ä»¶
- âœ… **é€‰æ‹©ç»„ä»¶**: æ ¹æ®åœºæ™¯é€‰æ‹©åˆé€‚çš„ LLMã€å·¥å…·ã€è®°å¿†ç­‰
- âœ… **å®šä¹‰çŠ¶æ€**: è®¾è®¡æ¸…æ™°çš„çŠ¶æ€ç»“æ„,åŒ…å«æ‰€æœ‰å¿…è¦å­—æ®µ
- âœ… **è§„åˆ’æµç¨‹**: ç”»å‡ºå›¾ç»“æ„,æ˜ç¡®æ•°æ®æµå‘

**å¼€å‘é˜¶æ®µ**

- âœ… **çŠ¶æ€è½¬æ¢**: åœ¨èŠ‚ç‚¹ä¸­æ­£ç¡®å¤„ç† LangChain è¾“å‡ºåˆ°çŠ¶æ€çš„è½¬æ¢
- âœ… **å¼‚å¸¸å¤„ç†**: ä¸º LLM è°ƒç”¨ã€å·¥å…·æ‰§è¡Œæ·»åŠ  try-except
- âœ… **ç±»å‹å®‰å…¨**: ä½¿ç”¨ TypedDict å®šä¹‰çŠ¶æ€,é¿å…ç±»å‹é”™è¯¯
- âœ… **æ—¥å¿—è®°å½•**: æ·»åŠ è¯¦ç»†æ—¥å¿—,æ–¹ä¾¿è°ƒè¯•

**ä¼˜åŒ–é˜¶æ®µ**

- âœ… **ç¼“å­˜æœºåˆ¶**: å¯¹é‡å¤çš„ LLM è°ƒç”¨ä½¿ç”¨ç¼“å­˜
- âœ… **å¹¶è¡Œæ‰§è¡Œ**: è¯†åˆ«å¯å¹¶è¡Œçš„ LLM/å·¥å…·è°ƒç”¨
- âœ… **æˆæœ¬æ§åˆ¶**: ç›‘æ§ API è°ƒç”¨æ¬¡æ•°,ä½¿ç”¨æ€§ä»·æ¯”é«˜çš„æ¨¡å‹
- âœ… **æ€§èƒ½ç›‘æ§**: ä½¿ç”¨ LangSmith æˆ–è‡ªå®šä¹‰ç›‘æ§

**æµ‹è¯•é˜¶æ®µ**

- âœ… **å•å…ƒæµ‹è¯•**: æµ‹è¯•æ¯ä¸ªèŠ‚ç‚¹çš„åŠŸèƒ½
- âœ… **é›†æˆæµ‹è¯•**: æµ‹è¯•æ•´ä¸ªå›¾çš„æ‰§è¡Œæµç¨‹
- âœ… **è¾¹ç•Œæµ‹è¯•**: æµ‹è¯•å¼‚å¸¸æƒ…å†µå’Œè¾¹ç•Œæ¡ä»¶
- âœ… **æ€§èƒ½æµ‹è¯•**: æµ‹è¯•å“åº”æ—¶é—´å’Œèµ„æºæ¶ˆè€—

### 8.2 å¸¸è§é—®é¢˜ FAQ

**Q1: å¦‚ä½•å¤„ç† LangChain å’Œ LangGraph çš„çŠ¶æ€å·®å¼‚?**

A: åœ¨èŠ‚ç‚¹ä¸­è¿›è¡ŒçŠ¶æ€è½¬æ¢,æ˜ç¡®è¾“å…¥è¾“å‡ºæ ¼å¼ã€‚

```python
def llm_node(state: MyState) -> dict:
    # ä» LangGraph çŠ¶æ€æå–æ•°æ®
    messages = state["messages"]

    # è°ƒç”¨ LangChain ç»„ä»¶
    response = llm.invoke(messages)

    # è½¬æ¢ä¸º LangGraph çŠ¶æ€æ ¼å¼
    return {
        "messages": [AIMessage(content=response.content)],
        "llm_response": response.content
    }
```

**Q2: å¦‚ä½•ä¼˜åŒ– LLM è°ƒç”¨æˆæœ¬?**

A: ä½¿ç”¨å¤šç§ç­–ç•¥é™ä½æˆæœ¬:

1. **ä½¿ç”¨ç¼“å­˜**
```python
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache

set_llm_cache(InMemoryCache())
```

2. **é€‰æ‹©åˆé€‚æ¨¡å‹**
```python
# ç®€å•ä»»åŠ¡ç”¨ä¾¿å®œæ¨¡å‹
simple_llm = ChatOpenAI(model="gpt-3.5-turbo")

# å¤æ‚ä»»åŠ¡ç”¨é«˜çº§æ¨¡å‹
complex_llm = ChatOpenAI(model="gpt-4")
```

3. **æ‰¹å¤„ç†**
```python
# æ‰¹é‡è°ƒç”¨
responses = llm.batch([msg1, msg2, msg3])
```

**Q3: å¦‚ä½•è°ƒè¯•é›†æˆé—®é¢˜?**

A: ä½¿ç”¨å¤šç§è°ƒè¯•å·¥å…·:

1. **å¯ç”¨ verbose æ¨¡å¼**
```python
llm = ChatOpenAI(model="gpt-4", verbose=True)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
```

2. **æ·»åŠ è¯¦ç»†æ—¥å¿—**
```python
import logging
logging.basicConfig(level=logging.DEBUG)

def debug_node(state):
    logging.debug(f"èŠ‚ç‚¹è¾“å…¥: {state}")
    result = process(state)
    logging.debug(f"èŠ‚ç‚¹è¾“å‡º: {result}")
    return result
```

3. **ä½¿ç”¨ LangSmith**
```python
import os
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "your-key"
```

**Q4: å¦‚ä½•å¤„ç† LLM è°ƒç”¨å¤±è´¥?**

A: å®ç°é‡è¯•å’Œé™çº§æœºåˆ¶:

```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10)
)
def robust_llm_call(llm, messages):
    """å¸¦é‡è¯•çš„ LLM è°ƒç”¨"""
    return llm.invoke(messages)
```

**Q5: å¦‚ä½•åœ¨ LangGraph ä¸­ä½¿ç”¨ LangChain çš„æµå¼è¾“å‡º?**

A: ä½¿ç”¨å¼‚æ­¥æµå¼:

```python
async def streaming_node(state: ChatState):
    """æµå¼ LLM èŠ‚ç‚¹"""
    response_content = ""

    async for chunk in llm.astream(state["messages"]):
        response_content += chunk.content
        print(chunk.content, end="", flush=True)

    return {"llm_response": response_content}
```

**Q6: å¦‚ä½•ç®¡ç†å¤šä¸ª LLM çš„ API å¯†é’¥?**

A: ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶:

```python
import os
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic

# æ–¹å¼1: ç¯å¢ƒå˜é‡
os.environ["OPENAI_API_KEY"] = "your-openai-key"
os.environ["ANTHROPIC_API_KEY"] = "your-anthropic-key"

# æ–¹å¼2: æ˜¾å¼ä¼ é€’
gpt = ChatOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
claude = ChatAnthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
```

**Q7: LangChain ç»„ä»¶åœ¨ LangGraph ä¸­æ˜¯å¦çº¿ç¨‹å®‰å…¨?**

A: å¤§å¤šæ•° LangChain ç»„ä»¶æ˜¯çº¿ç¨‹å®‰å…¨çš„,ä½†éœ€è¦æ³¨æ„:

- Memory ç»„ä»¶å¯èƒ½ä¸æ˜¯çº¿ç¨‹å®‰å…¨çš„,å»ºè®®æ¯ä¸ªä¼šè¯ä½¿ç”¨ç‹¬ç«‹å®ä¾‹
- å‘é‡å­˜å‚¨çš„å¹¶å‘å†™å…¥éœ€è¦åŠ é”
- LLM è°ƒç”¨æœ¬èº«æ˜¯çº¿ç¨‹å®‰å…¨çš„

```python
from threading import Lock

memory_lock = Lock()

def thread_safe_memory_node(state):
    with memory_lock:
        # å®‰å…¨åœ°è®¿é—® memory
        memory.save_context(...)
```

**Q8: å¦‚ä½•ç›‘æ§é›†æˆç³»ç»Ÿçš„æ€§èƒ½?**

A: å®ç°æ€§èƒ½ç›‘æ§:

```python
import time
from collections import defaultdict

class PerformanceMonitor:
    def __init__(self):
        self.metrics = defaultdict(list)

    def track(self, node_name: str, duration: float):
        self.metrics[node_name].append(duration)

    def report(self):
        for node, durations in self.metrics.items():
            avg = sum(durations) / len(durations)
            print(f"{node}: å¹³å‡ {avg:.2f}s, è°ƒç”¨ {len(durations)} æ¬¡")

monitor = PerformanceMonitor()

def monitored_node(state):
    start = time.time()
    result = process(state)
    monitor.track("process_node", time.time() - start)
    return result
```

### 8.3 é›†æˆæ¨¡å¼æ€»ç»“

**æ¨èçš„é›†æˆæ¨¡å¼**

1. **ç®€å•é—®ç­”ç³»ç»Ÿ**
```
LangGraph(çŠ¶æ€ç®¡ç†) + ChatOpenAI(LLM) + BufferMemory(è®°å¿†)
```

2. **RAG ç³»ç»Ÿ**
```
LangGraph(ç¼–æ’) + FAISS(å‘é‡åº“) + OpenAIEmbeddings + ChatOpenAI
```

3. **Agent ç³»ç»Ÿ**
```
LangGraph(æµç¨‹æ§åˆ¶) + Tools(å·¥å…·é›†) + ReActAgent(å†³ç­–) + Memory(è®°å¿†)
```

4. **å¤šæ¨¡æ€ç³»ç»Ÿ**
```
LangGraph(ç¼–æ’) + ChatOpenAI(æ–‡æœ¬) + DALL-E(å›¾åƒ) + Whisper(è¯­éŸ³)
```

**æ€§èƒ½å¯¹æ¯”**

| é›†æˆæ–¹å¼ | å»¶è¿Ÿ | æˆæœ¬ | å¤æ‚åº¦ | é€‚ç”¨åœºæ™¯ |
|---------|------|------|-------|---------|
| å• LLM | ä½ | ä¸­ | ä½ | ç®€å•å¯¹è¯ |
| å¤š LLM åä½œ | ä¸­ | é«˜ | ä¸­ | è´¨é‡è¦æ±‚é«˜ |
| LLM + Tools | ä¸­ | ä¸­ | ä¸­ | éœ€è¦å¤–éƒ¨æ•°æ® |
| Agent | é«˜ | é«˜ | é«˜ | å¤æ‚ä»»åŠ¡ |
| RAG | ä¸­ | ä¸­ | ä¸­ | çŸ¥è¯†åº“é—®ç­” |

### 8.4 ä¸‹ä¸€æ­¥å­¦ä¹ 

**è¿›é˜¶ä¸»é¢˜**

1. **è‡ªå®šä¹‰ LangChain ç»„ä»¶**
   - å®ç°è‡ªå®šä¹‰ LLM
   - åˆ›å»ºè‡ªå®šä¹‰å·¥å…·
   - å¼€å‘è‡ªå®šä¹‰ Memory

2. **é«˜çº§é›†æˆæŠ€å·§**
   - æµå¼é›†æˆ
   - å¹¶è¡Œ LLM è°ƒç”¨
   - åŠ¨æ€å·¥å…·é€‰æ‹©

3. **ç”Ÿäº§éƒ¨ç½²**
   - æ€§èƒ½ä¼˜åŒ–
   - é”™è¯¯å¤„ç†
   - ç›‘æ§å‘Šè­¦

**ç›¸å…³èµ„æº**

- [LangChain å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/)
- [LangGraph å®˜æ–¹æ–‡æ¡£](https://langchain-ai.github.io/langgraph/)
- [LangSmith å¹³å°](https://smith.langchain.com/)
- [LangChain Hub](https://smith.langchain.com/hub)

---

**æ€»ç»“** ğŸ“š

LangGraph ä¸ LangChain çš„é›†æˆä¸ºæ„å»ºå¤æ‚ AI åº”ç”¨æä¾›äº†å¼ºå¤§çš„å·¥å…·ç»„åˆ:

- âœ… **LangGraph** æä¾›çµæ´»çš„æµç¨‹ç¼–æ’å’ŒçŠ¶æ€ç®¡ç†
- âœ… **LangChain** æä¾›ä¸°å¯Œçš„ç»„ä»¶ç”Ÿæ€å’Œå·¥å…·é“¾
- âœ… **ä¸¤è€…ç»“åˆ** å®ç°äº†æœ€ä½³çš„å¼€å‘ä½“éªŒå’ŒåŠŸèƒ½å®Œæ•´æ€§

é€šè¿‡æœ¬ç« å­¦ä¹ ,ä½ å·²ç»æŒæ¡äº†:
1. å¦‚ä½•é›†æˆå„ç§ LLM
2. å¦‚ä½•ä½¿ç”¨ LangChain å·¥å…·
3. å¦‚ä½•é›†æˆè®°å¿†ç³»ç»Ÿ
4. å¦‚ä½•ä½¿ç”¨ Chains å’Œ Agents
5. å¦‚ä½•æ„å»ºå®Œæ•´çš„ RAG ç³»ç»Ÿ

**ä¸‹ä¸€æ­¥:** å­¦ä¹  [12.å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ](./12.å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ.md) æ„å»ºåä½œç³»ç»Ÿ! ğŸš€
